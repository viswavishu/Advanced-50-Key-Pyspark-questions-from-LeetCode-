{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e91c1ad0-5f5c-4966-9937-c566ad7e690f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<p style=\"font-size: 24px; background: -webkit-linear-gradient(left, #FFD700, #FFAA00); -webkit-background-clip: text; -webkit-text-fill-color: transparent;\">\n",
    "  50 Advanced Premium Pyspark SQL Problems\n",
    "</p>\n",
    "\n",
    "![Databricks Logo](files/advancdsql.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f0b8227-ead1-4ee9-bfce-74e527078952",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d4c8169-9b0e-41e4-8a41-853e34740acb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1821. Find Customers With Positive Revenue this Year\n",
    "### Level: Easy\n",
    "```\n",
    "Table: Customers\n",
    "\n",
    "+--------------+------+\n",
    "| Column Name  | Type |\n",
    "+--------------+------+\n",
    "| customer_id  | int  |\n",
    "| year         | int  |\n",
    "| revenue      | int  |\n",
    "+--------------+------+\n",
    "(customer_id, year) is the primary key (combination of columns with unique values) for this table.\n",
    "This table contains the customer ID and the revenue of customers in different years.\n",
    "Note that this revenue can be negative.\n",
    " \n",
    "\n",
    "Write a solution to report the customers with postive revenue in the year 2021.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Customers table:\n",
    "+-------------+------+---------+\n",
    "| customer_id | year | revenue |\n",
    "+-------------+------+---------+\n",
    "| 1           | 2018 | 50      |\n",
    "| 1           | 2021 | 30      |\n",
    "| 1           | 2020 | 70      |\n",
    "| 2           | 2021 | -50     |\n",
    "| 3           | 2018 | 10      |\n",
    "| 3           | 2016 | 50      |\n",
    "| 4           | 2021 | 20      |\n",
    "+-------------+------+---------+\n",
    "Output: \n",
    "+-------------+\n",
    "| customer_id |\n",
    "+-------------+\n",
    "| 1           |\n",
    "| 4           |\n",
    "+-------------+\n",
    "Explanation: \n",
    "Customer 1 has revenue equal to 30 in the year 2021.\n",
    "Customer 2 has revenue equal to -50 in the year 2021.\n",
    "Customer 3 has no revenue in the year 2021.\n",
    "Customer 4 has revenue equal to 20 in the year 2021.\n",
    "Thus only customers 1 and 4 have positive revenue in the year 2021.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "854442ca-5a40-4f38-ad7d-ff4c88d2ae71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-------+\n|customer_id|year|revenue|\n+-----------+----+-------+\n|          1|2018|     50|\n|          1|2021|     30|\n|          1|2020|     70|\n|          2|2021|    -50|\n|          3|2018|     10|\n|          3|2016|     50|\n|          4|2021|     20|\n+-----------+----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Assuming spark session already exists\n",
    "# spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [[1, 2018, 50], [1, 2021, 30], [1, 2020, 70], [2, 2021, -50], [3, 2018, 10], [3, 2016, 50], [4, 2021, 20]]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"revenue\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "customers_df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "customers_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bb99f1b-d932-460f-92e8-4e996218c796",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|customer_id|\n+-----------+\n|          1|\n|          4|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Solution:\n",
    "from pyspark.sql.functions import col\n",
    "customers_df.filter((col('year') == 2021) & (col('revenue') > 0)).select(col('customer_id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50c63c10-d2e8-4509-8946-f5af7aa3b724",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 183. Customers Who Never Order\n",
    "### Level: Easy\n",
    "```\n",
    "Table: Customers\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| id          | int     |\n",
    "| name        | varchar |\n",
    "+-------------+---------+\n",
    "id is the primary key (column with unique values) for this table.\n",
    "Each row of this table indicates the ID and name of a customer.\n",
    " \n",
    "\n",
    "Table: Orders\n",
    "\n",
    "+-------------+------+\n",
    "| Column Name | Type |\n",
    "+-------------+------+\n",
    "| id          | int  |\n",
    "| customerId  | int  |\n",
    "+-------------+------+\n",
    "id is the primary key (column with unique values) for this table.\n",
    "customerId is a foreign key (reference columns) of the ID from the Customers table.\n",
    "Each row of this table indicates the ID of an order and the ID of the customer who ordered it.\n",
    " \n",
    "\n",
    "Write a solution to find all customers who never order anything.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Customers table:\n",
    "+----+-------+\n",
    "| id | name  |\n",
    "+----+-------+\n",
    "| 1  | Joe   |\n",
    "| 2  | Henry |\n",
    "| 3  | Sam   |\n",
    "| 4  | Max   |\n",
    "+----+-------+\n",
    "Orders table:\n",
    "+----+------------+\n",
    "| id | customerId |\n",
    "+----+------------+\n",
    "| 1  | 3          |\n",
    "| 2  | 1          |\n",
    "+----+------------+\n",
    "Output: \n",
    "+-----------+\n",
    "| Customers |\n",
    "+-----------+\n",
    "| Henry     |\n",
    "| Max       |\n",
    "+-----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4e67996-032f-4b91-8fef-6b1a8afd2d3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n| id| name|\n+---+-----+\n|  1|  Joe|\n|  2|Henry|\n|  3|  Sam|\n|  4|  Max|\n+---+-----+\n\n+---+----------+\n| id|customerId|\n+---+----------+\n|  1|         3|\n|  2|         1|\n+---+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Assuming spark session already exists\n",
    "# spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "# Sample data for customers\n",
    "data_customers = [[1, 'Joe'], [2, 'Henry'], [3, 'Sam'], [4, 'Max']]\n",
    "\n",
    "# Define schema for customers\n",
    "schema_customers = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame for customers\n",
    "customers_df = spark.createDataFrame(data_customers, schema=schema_customers)\n",
    "\n",
    "# Show the DataFrame for customers\n",
    "customers_df.show()\n",
    "\n",
    "# Sample data for orders\n",
    "data_orders = [[1, 3], [2, 1]]\n",
    "\n",
    "# Define schema for orders\n",
    "schema_orders = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"customerId\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame for orders\n",
    "orders_df = spark.createDataFrame(data_orders, schema=schema_orders)\n",
    "\n",
    "# Show the DataFrame for orders\n",
    "orders_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aae131b6-e547-4057-b63b-7750f3b8321f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|Customers|\n+---------+\n|    Henry|\n|      Max|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "#solution 1:\n",
    "\n",
    "order_ids = [row.customerId for row in orders_df.select('customerId').distinct().collect()]\n",
    "customers_df.filter(~col('id').isin(order_ids)).select(col('name').alias('Customers')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "691a03d0-d38b-4b45-b2ea-22ffb6adfc06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|Customers|\n+---------+\n|    Henry|\n|      Max|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "# Solution 2:\n",
    "order_ids_df = orders_df.select(col('customerId').alias('id')).distinct()\n",
    "\n",
    "customers_not_in_orders_df = customers_df.join(\n",
    "    order_ids_df,\n",
    "    on='id',\n",
    "    how='left_anti'\n",
    ")\n",
    "\n",
    "customers_not_in_orders_df.select(col('name').alias('Customers')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d078436-3946-4b28-9ed9-d09ed01853cf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1873. Calculate Special Bonus\n",
    "### Level: Easy\n",
    "```\n",
    "Table: Employees\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| employee_id | int     |\n",
    "| name        | varchar |\n",
    "| salary      | int     |\n",
    "+-------------+---------+\n",
    "employee_id is the primary key (column with unique values) for this table.\n",
    "Each row of this table indicates the employee ID, employee name, and salary.\n",
    " \n",
    "\n",
    "Write a solution to calculate the bonus of each employee. The bonus of an employee is 100% of their salary\n",
    " if the ID of the employee is an odd number and the employee's name does not start with the character\n",
    "  'M'. The bonus of an employee is 0 otherwise.\n",
    "\n",
    "Return the result table ordered by employee_id.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Employees table:\n",
    "+-------------+---------+--------+\n",
    "| employee_id | name    | salary |\n",
    "+-------------+---------+--------+\n",
    "| 2           | Meir    | 3000   |\n",
    "| 3           | Michael | 3800   |\n",
    "| 7           | Addilyn | 7400   |\n",
    "| 8           | Juan    | 6100   |\n",
    "| 9           | Kannon  | 7700   |\n",
    "+-------------+---------+--------+\n",
    "Output: \n",
    "+-------------+-------+\n",
    "| employee_id | bonus |\n",
    "+-------------+-------+\n",
    "| 2           | 0     |\n",
    "| 3           | 0     |\n",
    "| 7           | 7400  |\n",
    "| 8           | 0     |\n",
    "| 9           | 7700  |\n",
    "+-------------+-------+\n",
    "Explanation: \n",
    "The employees with IDs 2 and 8 get 0 bonus because they have an even employee_id.\n",
    "The employee with ID 3 gets 0 bonus because their name starts with 'M'.\n",
    "The rest of the employees get a 100% bonus.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28fd08f4-47e4-4121-a697-060e37942510",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------+\n|employee_id|   name|salary|\n+-----------+-------+------+\n|          2|   Meir|  3000|\n|          3|Michael|  3800|\n|          7|Addilyn|  7400|\n|          8|   Juan|  6100|\n|          9| Kannon|  7700|\n+-----------+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"name\", StringType(), nullable=True),\n",
    "    StructField(\"salary\", IntegerType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Create a list of rows from the pandas DataFrame\n",
    "data = [\n",
    "    (2, 'Meir', 3000),\n",
    "    (3, 'Michael', 3800),\n",
    "    (7, 'Addilyn', 7400),\n",
    "    (8, 'Juan', 6100),\n",
    "    (9, 'Kannon', 7700)\n",
    "]\n",
    "\n",
    "# Create a PySpark DataFrame from the data and schema\n",
    "employees_df = spark.createDataFrame(data, schema=schema)\n",
    "employees_df.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93782f44-35cc-46b1-8c9a-2b486780407e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n|employee_id|bonus|\n+-----------+-----+\n|          2|    0|\n|          3|    0|\n|          7| 7400|\n|          8|    0|\n|          9| 7700|\n+-----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Solution:\n",
    "from pyspark.sql.functions import *\n",
    "employees_df.withColumn('bonus',\\\n",
    "                        when(((col('employee_id') % 2 != 0) & (~col('name').like('M%'))), col('salary')).otherwise(0))\\\n",
    "    .select('employee_id','bonus').orderBy(col('employee_id')).show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "840a2add-491a-4a60-a8b5-7420ca5ba520",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium)1398. Customers Who Bought Products A and B but Not C\n",
    "### Level: Medium\n",
    "```\n",
    "Table: Customers\n",
    "\n",
    "+---------------------+---------+\n",
    "| Column Name         | Type    |\n",
    "+---------------------+---------+\n",
    "| customer_id         | int     |\n",
    "| customer_name       | varchar |\n",
    "+---------------------+---------+\n",
    "customer_id is the column with unique values for this table.\n",
    "customer_name is the name of the customer.\n",
    " \n",
    "\n",
    "Table: Orders\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| order_id      | int     |\n",
    "| customer_id   | int     |\n",
    "| product_name  | varchar |\n",
    "+---------------+---------+\n",
    "order_id is the column with unique values for this table.\n",
    "customer_id is the id of the customer who bought the product \"product_name\".\n",
    " \n",
    "\n",
    "Write a solution to report the customer_id and customer_name of customers who bought products \"A\", \"B\" but did not buy the product \"C\" since we want to recommend them to purchase this product.\n",
    "\n",
    "Return the result table ordered by customer_id.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Customers table:\n",
    "+-------------+---------------+\n",
    "| customer_id | customer_name |\n",
    "+-------------+---------------+\n",
    "| 1           | Daniel        |\n",
    "| 2           | Diana         |\n",
    "| 3           | Elizabeth     |\n",
    "| 4           | Jhon          |\n",
    "+-------------+---------------+\n",
    "Orders table:\n",
    "+------------+--------------+---------------+\n",
    "| order_id   | customer_id  | product_name  |\n",
    "+------------+--------------+---------------+\n",
    "| 10         |     1        |     A         |\n",
    "| 20         |     1        |     B         |\n",
    "| 30         |     1        |     D         |\n",
    "| 40         |     1        |     C         |\n",
    "| 50         |     2        |     A         |\n",
    "| 60         |     3        |     A         |\n",
    "| 70         |     3        |     B         |\n",
    "| 80         |     3        |     D         |\n",
    "| 90         |     4        |     C         |\n",
    "+------------+--------------+---------------+\n",
    "Output: \n",
    "+-------------+---------------+\n",
    "| customer_id | customer_name |\n",
    "+-------------+---------------+\n",
    "| 3           | Elizabeth     |\n",
    "+-------------+---------------+\n",
    "Explanation: Only the customer_id with id 3 bought the product A and B but not the product C.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2b33daa-80fd-4da8-a506-499e58fe5656",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n|customer_id|customer_name|\n+-----------+-------------+\n|          1|       Daniel|\n|          2|        Diana|\n|          3|    Elizabeth|\n|          4|         Jhon|\n+-----------+-------------+\n\n+--------+-----------+------------+\n|order_id|customer_id|product_name|\n+--------+-----------+------------+\n|      10|          1|           A|\n|      20|          1|           B|\n|      30|          1|           D|\n|      40|          1|           C|\n|      50|          2|           A|\n|      60|          3|           A|\n|      70|          3|           B|\n|      80|          3|           D|\n|      90|          4|           C|\n+--------+-----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Define the schema for the 'customers' DataFrame\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"customer_name\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Define the schema for the 'orders' DataFrame\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"customer_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"product_name\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Data for 'customers' DataFrame\n",
    "customers_data = [\n",
    "    (1, 'Daniel'),\n",
    "    (2, 'Diana'),\n",
    "    (3, 'Elizabeth'),\n",
    "    (4, 'Jhon')\n",
    "]\n",
    "\n",
    "# Data for 'orders' DataFrame\n",
    "orders_data = [\n",
    "    (10, 1, 'A'),\n",
    "    (20, 1, 'B'),\n",
    "    (30, 1, 'D'),\n",
    "    (40, 1, 'C'),\n",
    "    (50, 2, 'A'),\n",
    "    (60, 3, 'A'),\n",
    "    (70, 3, 'B'),\n",
    "    (80, 3, 'D'),\n",
    "    (90, 4, 'C')\n",
    "]\n",
    "\n",
    "# Create PySpark DataFrames from the data and schemas\n",
    "customers_df = spark.createDataFrame(customers_data, schema=customers_schema)\n",
    "orders_df = spark.createDataFrame(orders_data, schema=orders_schema)\n",
    "customers_df.show()\n",
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7788ec8-3f02-4181-a283-dc9148e709f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n|customer_id|customer_name|\n+-----------+-------------+\n|          3|    Elizabeth|\n+-----------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "orders_df.alias('O').join(customers_df.alias('C'), col('O.customer_id') == col('C.customer_id'),'left')\\\n",
    "    .filter(col('product_name').isin('A','B','C')).groupBy(col('O.customer_id'),col('C.customer_name'))\\\n",
    "    .agg(count(col('product_name')).alias('noOfProductsBrought')).filter(col('noOfProductsBrought') == 2)\\\n",
    "        .drop(col('noOfProductsBrought')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d1d9d04-c2c6-4cc9-959e-5f73cebaaabc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1112. Highest Grade For Each Student\n",
    "### Level: Medium\n",
    "```\n",
    "Table: Enrollments\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| student_id    | int     |\n",
    "| course_id     | int     |\n",
    "| grade         | int     |\n",
    "+---------------+---------+\n",
    "(student_id, course_id) is the primary key (combination of columns with unique values) of this table.\n",
    "grade is never NULL.\n",
    " \n",
    "\n",
    "Write a solution to find the highest grade with its corresponding course for each student. In case of a tie, you should find the course with the smallest course_id.\n",
    "\n",
    "Return the result table ordered by student_id in ascending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Enrollments table:\n",
    "+------------+-------------------+\n",
    "| student_id | course_id | grade |\n",
    "+------------+-----------+-------+\n",
    "| 2          | 2         | 95    |\n",
    "| 2          | 3         | 95    |\n",
    "| 1          | 1         | 90    |\n",
    "| 1          | 2         | 99    |\n",
    "| 3          | 1         | 80    |\n",
    "| 3          | 2         | 75    |\n",
    "| 3          | 3         | 82    |\n",
    "+------------+-----------+-------+\n",
    "Output: \n",
    "+------------+-------------------+\n",
    "| student_id | course_id | grade |\n",
    "+------------+-----------+-------+\n",
    "| 1          | 2         | 99    |\n",
    "| 2          | 2         | 95    |\n",
    "| 3          | 3         | 82    |\n",
    "+------------+-----------+-------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86e3f2fb-6167-43df-90ad-a7fbb0f2e236",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+\n|student_id|course_id|grade|\n+----------+---------+-----+\n|         2|        2|   95|\n|         2|        3|   95|\n|         1|        1|   90|\n|         1|        2|   99|\n|         3|        1|   80|\n|         3|        2|   75|\n|         3|        3|   82|\n+----------+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "# Define the schema for the 'enrollments' DataFrame\n",
    "enrollments_schema = StructType([\n",
    "    StructField(\"student_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"course_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"grade\", IntegerType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Data for 'enrollments' DataFrame\n",
    "enrollments_data = [\n",
    "    (2, 2, 95),\n",
    "    (2, 3, 95),\n",
    "    (1, 1, 90),\n",
    "    (1, 2, 99),\n",
    "    (3, 1, 80),\n",
    "    (3, 2, 75),\n",
    "    (3, 3, 82)\n",
    "]\n",
    "\n",
    "# Create PySpark DataFrame from the data and schema\n",
    "enrollments_df = spark.createDataFrame(enrollments_data, schema=enrollments_schema)\n",
    "enrollments_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dddf9a3-6482-4dce-9a35-13d80696bcd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----+\n|student_id|course_id|grade|\n+----------+---------+-----+\n|         1|        2|   99|\n|         2|        2|   95|\n|         3|        3|   82|\n+----------+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Solution:\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "windowOptions = Window.partitionBy(col('student_id')).orderBy(col('grade').desc(),col('course_id'))\n",
    "\n",
    "enrollments_df.withColumn('Rn',row_number().over(windowOptions)).filter(col('Rn') == 1).drop(col('Rn')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42213dc9-1c2a-4b43-9e32-542b9d42fac5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Basic Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "689abcde-145a-442c-a34f-ab808858ce3d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 175. Combine Two Tables\n",
    "### Level: Easy\n",
    "```\n",
    "Table: Person\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| personId    | int     |\n",
    "| lastName    | varchar |\n",
    "| firstName   | varchar |\n",
    "+-------------+---------+\n",
    "personId is the primary key (column with unique values) for this table.\n",
    "This table contains information about the ID of some persons and their first and last names.\n",
    " \n",
    "\n",
    "Table: Address\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| addressId   | int     |\n",
    "| personId    | int     |\n",
    "| city        | varchar |\n",
    "| state       | varchar |\n",
    "+-------------+---------+\n",
    "addressId is the primary key (column with unique values) for this table.\n",
    "Each row of this table contains information about the city and state of one person with ID = PersonId.\n",
    " \n",
    "\n",
    "Write a solution to report the first name, last name, city, and state of each person in the Person table. If the address of a personId is not present in the Address table, report null instead.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Person table:\n",
    "+----------+----------+-----------+\n",
    "| personId | lastName | firstName |\n",
    "+----------+----------+-----------+\n",
    "| 1        | Wang     | Allen     |\n",
    "| 2        | Alice    | Bob       |\n",
    "+----------+----------+-----------+\n",
    "Address table:\n",
    "+-----------+----------+---------------+------------+\n",
    "| addressId | personId | city          | state      |\n",
    "+-----------+----------+---------------+------------+\n",
    "| 1         | 2        | New York City | New York   |\n",
    "| 2         | 3        | Leetcode      | California |\n",
    "+-----------+----------+---------------+------------+\n",
    "Output: \n",
    "+-----------+----------+---------------+----------+\n",
    "| firstName | lastName | city          | state    |\n",
    "+-----------+----------+---------------+----------+\n",
    "| Allen     | Wang     | Null          | Null     |\n",
    "| Bob       | Alice    | New York City | New York |\n",
    "+-----------+----------+---------------+----------+\n",
    "Explanation: \n",
    "There is no address in the address table for the personId = 1 so we return null in their city and state.\n",
    "addressId = 1 contains information about the address of personId = 2.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e5ed6fc-abea-4d83-9069-46109b0b0ac4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------+\n|personId|lastName|firstName|\n+--------+--------+---------+\n|       1|    Wang|    Allen|\n|       2|   Alice|      Bob|\n+--------+--------+---------+\n\n+---------+--------+-------------+----------+\n|addressId|personId|         city|     state|\n+---------+--------+-------------+----------+\n|        1|       2|New York City|  New York|\n|        2|       3|     Leetcode|California|\n+---------+--------+-------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Define the schema for the 'person' DataFrame\n",
    "person_schema = StructType([\n",
    "    StructField(\"personId\", IntegerType(), nullable=False),\n",
    "    StructField(\"lastName\", StringType(), nullable=True),\n",
    "    StructField(\"firstName\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Data for 'person' DataFrame\n",
    "person_data = [\n",
    "    (1, 'Wang', 'Allen'),\n",
    "    (2, 'Alice', 'Bob')\n",
    "]\n",
    "\n",
    "# Create PySpark DataFrame for 'person'\n",
    "person_df = spark.createDataFrame(person_data, schema=person_schema)\n",
    "\n",
    "# Define the schema for the 'address' DataFrame\n",
    "address_schema = StructType([\n",
    "    StructField(\"addressId\", IntegerType(), nullable=False),\n",
    "    StructField(\"personId\", IntegerType(), nullable=False),\n",
    "    StructField(\"city\", StringType(), nullable=True),\n",
    "    StructField(\"state\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Data for 'address' DataFrame\n",
    "address_data = [\n",
    "    (1, 2, 'New York City', 'New York'),\n",
    "    (2, 3, 'Leetcode', 'California')\n",
    "]\n",
    "\n",
    "# Create PySpark DataFrame for 'address'\n",
    "address_df = spark.createDataFrame(address_data, schema=address_schema)\n",
    "person_df.show()\n",
    "address_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc96a591-fc17-40ef-897c-e2250ecbce1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------+--------+\n|firstName|lastName|         city|   state|\n+---------+--------+-------------+--------+\n|    Allen|    Wang|         null|    null|\n|      Bob|   Alice|New York City|New York|\n+---------+--------+-------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "#Solution: \n",
    "from pyspark.sql.functions import col\n",
    "person_df.alias('P').join(address_df.alias('A'),col('P.personId') == col('A.personId'),'left')\\\n",
    "    .select(col('firstName'),col('lastName'),col('city'),col('state')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30a11da6-9806-450e-a7b8-4268752bd3f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1607. Sellers With No Sales\n",
    "```\n",
    "Table: Customer\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| customer_id   | int     |\n",
    "| customer_name | varchar |\n",
    "+---------------+---------+\n",
    "customer_id is the column with unique values for this table.\n",
    "Each row of this table contains the information of each customer in the WebStore.\n",
    " \n",
    "\n",
    "Table: Orders\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| order_id      | int     |\n",
    "| sale_date     | date    |\n",
    "| order_cost    | int     |\n",
    "| customer_id   | int     |\n",
    "| seller_id     | int     |\n",
    "+---------------+---------+\n",
    "order_id is the column with unique values for this table.\n",
    "Each row of this table contains all orders made in the webstore.\n",
    "sale_date is the date when the transaction was made between the customer (customer_id) and the seller (seller_id).\n",
    " \n",
    "\n",
    "Table: Seller\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| seller_id     | int     |\n",
    "| seller_name   | varchar |\n",
    "+---------------+---------+\n",
    "seller_id is the column with unique values for this table.\n",
    "Each row of this table contains the information of each seller.\n",
    " \n",
    "\n",
    "Write a solution to report the names of all sellers who did not make any sales in 2020.\n",
    "\n",
    "Return the result table ordered by seller_name in ascending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Customer table:\n",
    "+--------------+---------------+\n",
    "| customer_id  | customer_name |\n",
    "+--------------+---------------+\n",
    "| 101          | Alice         |\n",
    "| 102          | Bob           |\n",
    "| 103          | Charlie       |\n",
    "+--------------+---------------+\n",
    "Orders table:\n",
    "+-------------+------------+--------------+-------------+-------------+\n",
    "| order_id    | sale_date  | order_cost   | customer_id | seller_id   |\n",
    "+-------------+------------+--------------+-------------+-------------+\n",
    "| 1           | 2020-03-01 | 1500         | 101         | 1           |\n",
    "| 2           | 2020-05-25 | 2400         | 102         | 2           |\n",
    "| 3           | 2019-05-25 | 800          | 101         | 3           |\n",
    "| 4           | 2020-09-13 | 1000         | 103         | 2           |\n",
    "| 5           | 2019-02-11 | 700          | 101         | 2           |\n",
    "+-------------+------------+--------------+-------------+-------------+\n",
    "Seller table:\n",
    "+-------------+-------------+\n",
    "| seller_id   | seller_name |\n",
    "+-------------+-------------+\n",
    "| 1           | Daniel      |\n",
    "| 2           | Elizabeth   |\n",
    "| 3           | Frank       |\n",
    "+-------------+-------------+\n",
    "Output: \n",
    "+-------------+\n",
    "| seller_name |\n",
    "+-------------+\n",
    "| Frank       |\n",
    "+-------------+\n",
    "Explanation: \n",
    "Daniel made 1 sale in March 2020.\n",
    "Elizabeth made 2 sales in 2020 and 1 sale in 2019.\n",
    "Frank made 1 sale in 2019 but no sales in 2020.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3230f69-d588-4d15-af48-86d0068d1b54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n|customer_id|customer_name|\n+-----------+-------------+\n|        101|        Alice|\n|        102|          Bob|\n|        103|      Charlie|\n+-----------+-------------+\n\n+--------+----------+----------+-----------+---------+\n|order_id| sale_date|order_cost|customer_id|seller_id|\n+--------+----------+----------+-----------+---------+\n|       1|2020-03-01|      1500|        101|        1|\n|       2|2020-05-25|      2400|        102|        2|\n|       3|2019-05-25|       800|        101|        3|\n|       4|2020-09-13|      1000|        103|        2|\n|       5|2019-02-11|       700|        101|        2|\n+--------+----------+----------+-----------+---------+\n\n+---------+-----------+\n|seller_id|seller_name|\n+---------+-----------+\n|        1|     Daniel|\n|        2|  Elizabeth|\n|        3|      Frank|\n+---------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "# Define the schema for the 'customer' DataFrame\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"customer_name\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Data for 'customer' DataFrame\n",
    "customer_data = [\n",
    "    (101, 'Alice'),\n",
    "    (102, 'Bob'),\n",
    "    (103, 'Charlie')\n",
    "]\n",
    "\n",
    "# Create PySpark DataFrame for 'customer'\n",
    "customer_df = spark.createDataFrame(customer_data, schema=customer_schema)\n",
    "\n",
    "# Define the schema for the 'orders' DataFrame\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"sale_date\", StringType(), nullable=True),\n",
    "    StructField(\"order_cost\", IntegerType(), nullable=True),\n",
    "    StructField(\"customer_id\", IntegerType(), nullable=True),\n",
    "    StructField(\"seller_id\", IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Data for 'orders' DataFrame\n",
    "orders_data = [\n",
    "    (1, '2020-03-01', 1500, 101, 1),\n",
    "    (2, '2020-05-25', 2400, 102, 2),\n",
    "    (3, '2019-05-25', 800, 101, 3),\n",
    "    (4, '2020-09-13', 1000, 103, 2),\n",
    "    (5, '2019-02-11', 700, 101, 2)\n",
    "]\n",
    "\n",
    "# Create PySpark DataFrame for 'orders'\n",
    "orders_df = spark.createDataFrame(orders_data, schema=orders_schema)\n",
    "\n",
    "# Define the schema for the 'seller' DataFrame\n",
    "seller_schema = StructType([\n",
    "    StructField(\"seller_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"seller_name\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Data for 'seller' DataFrame\n",
    "seller_data = [\n",
    "    (1, 'Daniel'),\n",
    "    (2, 'Elizabeth'),\n",
    "    (3, 'Frank')\n",
    "]\n",
    "\n",
    "# Create PySpark DataFrame for 'seller'\n",
    "seller_df = spark.createDataFrame(seller_data, schema=seller_schema)\n",
    "\n",
    "customer_df.show()\n",
    "orders_df.show()\n",
    "seller_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b46e9503-2327-423d-80b1-15928f56779f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|seller_name|\n+-----------+\n|      Frank|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df = seller_df.alias('S').join(orders_df.alias('A'), col('S.seller_id') == col('A.seller_id'),'left').filter((year(col('sale_date')) == '2020')).select(col('S.seller_id'))\n",
    "seller_ids_2020 = [row.seller_id for row in df.collect()]\n",
    "seller_df.filter(~col('seller_id').isin(seller_ids_2020)).select(col('seller_name')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63e8533b-54b5-415f-b648-8b28f53ea413",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1407. Top Travellers\n",
    "```\n",
    "Table: Users\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| id            | int     |\n",
    "| name          | varchar |\n",
    "+---------------+---------+\n",
    "id is the column with unique values for this table.\n",
    "name is the name of the user.\n",
    " \n",
    "\n",
    "Table: Rides\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| id            | int     |\n",
    "| user_id       | int     |\n",
    "| distance      | int     |\n",
    "+---------------+---------+\n",
    "id is the column with unique values for this table.\n",
    "user_id is the id of the user who traveled the distance \"distance\".\n",
    " \n",
    "\n",
    "Write a solution to report the distance traveled by each user.\n",
    "\n",
    "Return the result table ordered by travelled_distance in descending order, if two or more users traveled the same distance, order them by their name in ascending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Users table:\n",
    "+------+-----------+\n",
    "| id   | name      |\n",
    "+------+-----------+\n",
    "| 1    | Alice     |\n",
    "| 2    | Bob       |\n",
    "| 3    | Alex      |\n",
    "| 4    | Donald    |\n",
    "| 7    | Lee       |\n",
    "| 13   | Jonathan  |\n",
    "| 19   | Elvis     |\n",
    "+------+-----------+\n",
    "Rides table:\n",
    "+------+----------+----------+\n",
    "| id   | user_id  | distance |\n",
    "+------+----------+----------+\n",
    "| 1    | 1        | 120      |\n",
    "| 2    | 2        | 317      |\n",
    "| 3    | 3        | 222      |\n",
    "| 4    | 7        | 100      |\n",
    "| 5    | 13       | 312      |\n",
    "| 6    | 19       | 50       |\n",
    "| 7    | 7        | 120      |\n",
    "| 8    | 19       | 400      |\n",
    "| 9    | 7        | 230      |\n",
    "+------+----------+----------+\n",
    "Output: \n",
    "+----------+--------------------+\n",
    "| name     | travelled_distance |\n",
    "+----------+--------------------+\n",
    "| Elvis    | 450                |\n",
    "| Lee      | 450                |\n",
    "| Bob      | 317                |\n",
    "| Jonathan | 312                |\n",
    "| Alex     | 222                |\n",
    "| Alice    | 120                |\n",
    "| Donald   | 0                  |\n",
    "+----------+--------------------+\n",
    "Explanation: \n",
    "Elvis and Lee traveled 450 miles, Elvis is the top traveler as his name is alphabetically smaller than Lee.\n",
    "Bob, Jonathan, Alex, and Alice have only one ride and we just order them by the total distances of the ride.\n",
    "Donald did not have any rides, the distance traveled by him is 0.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e26dac2-1fca-4dff-b08c-cfc452478747",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n| id|    name|\n+---+--------+\n|  1|   Alice|\n|  2|     Bob|\n|  3|    Alex|\n|  4|  Donald|\n|  7|     Lee|\n| 13|Jonathan|\n| 19|   Elvis|\n+---+--------+\n\n+---+-------+--------+\n| id|user_id|distance|\n+---+-------+--------+\n|  1|      1|     120|\n|  2|      2|     317|\n|  3|      3|     222|\n|  4|      7|     100|\n|  5|     13|     312|\n|  6|     19|      50|\n|  7|      7|     120|\n|  8|     19|     400|\n|  9|      7|     230|\n+---+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Define data\n",
    "data_users = [(1, 'Alice'), (2, 'Bob'), (3, 'Alex'), (4, 'Donald'), (7, 'Lee'), (13, 'Jonathan'), (19, 'Elvis')]\n",
    "data_rides = [(1, 1, 120), (2, 2, 317), (3, 3, 222), (4, 7, 100), (5, 13, 312), (6, 19, 50), (7, 7, 120), (8, 19, 400), (9, 7, 230)]\n",
    "\n",
    "# Create PySpark DataFrames\n",
    "users_df = spark.createDataFrame(data_users, schema=['id', 'name'])\n",
    "rides_df = spark.createDataFrame(data_rides, schema=['id', 'user_id', 'distance'])\n",
    "\n",
    "# Show DataFrames\n",
    "users_df.show()\n",
    "rides_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf6f3bd0-ad18-4568-b770-364b747ba742",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n|    name|travelled_distance|\n+--------+------------------+\n|   Elvis|               450|\n|     Lee|               450|\n|     Bob|               317|\n|Jonathan|               312|\n|    Alex|               222|\n|   Alice|               120|\n|  Donald|                 0|\n+--------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "rides_df.alias('R').join(users_df.alias('U'), col('R.user_id') == col('U.id'),'right')\\\n",
    "    .groupBy(col('R.user_id'),col('U.name'))\\\n",
    "        .agg(sum(col('distance')).alias('travelled_distance'))\\\n",
    "        .select(col('U.name'),col('travelled_distance')).fillna(0)\\\n",
    "            .orderBy(col('travelled_distance').desc(),col('U.name')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3b240f6-473e-483e-bfbb-81274f5722d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 607. Sales Person\n",
    "```\n",
    "Table: SalesPerson\n",
    "\n",
    "+-----------------+---------+\n",
    "| Column Name     | Type    |\n",
    "+-----------------+---------+\n",
    "| sales_id        | int     |\n",
    "| name            | varchar |\n",
    "| salary          | int     |\n",
    "| commission_rate | int     |\n",
    "| hire_date       | date    |\n",
    "+-----------------+---------+\n",
    "sales_id is the primary key (column with unique values) for this table.\n",
    "Each row of this table indicates the name and the ID of a salesperson alongside their salary, commission rate, and hire date.\n",
    " \n",
    "\n",
    "Table: Company\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| com_id      | int     |\n",
    "| name        | varchar |\n",
    "| city        | varchar |\n",
    "+-------------+---------+\n",
    "com_id is the primary key (column with unique values) for this table.\n",
    "Each row of this table indicates the name and the ID of a company and the city in which the company is located.\n",
    " \n",
    "\n",
    "Table: Orders\n",
    "\n",
    "+-------------+------+\n",
    "| Column Name | Type |\n",
    "+-------------+------+\n",
    "| order_id    | int  |\n",
    "| order_date  | date |\n",
    "| com_id      | int  |\n",
    "| sales_id    | int  |\n",
    "| amount      | int  |\n",
    "+-------------+------+\n",
    "order_id is the primary key (column with unique values) for this table.\n",
    "com_id is a foreign key (reference column) to com_id from the Company table.\n",
    "sales_id is a foreign key (reference column) to sales_id from the SalesPerson table.\n",
    "Each row of this table contains information about one order. This includes the ID of the company, the ID of the salesperson, the date of the order, and the amount paid.\n",
    " \n",
    "\n",
    "Write a solution to find the names of all the salespersons who did not have any orders related to the company with the name \"RED\".\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "SalesPerson table:\n",
    "+----------+------+--------+-----------------+------------+\n",
    "| sales_id | name | salary | commission_rate | hire_date  |\n",
    "+----------+------+--------+-----------------+------------+\n",
    "| 1        | John | 100000 | 6               | 4/1/2006   |\n",
    "| 2        | Amy  | 12000  | 5               | 5/1/2010   |\n",
    "| 3        | Mark | 65000  | 12              | 12/25/2008 |\n",
    "| 4        | Pam  | 25000  | 25              | 1/1/2005   |\n",
    "| 5        | Alex | 5000   | 10              | 2/3/2007   |\n",
    "+----------+------+--------+-----------------+------------+\n",
    "Company table:\n",
    "+--------+--------+----------+\n",
    "| com_id | name   | city     |\n",
    "+--------+--------+----------+\n",
    "| 1      | RED    | Boston   |\n",
    "| 2      | ORANGE | New York |\n",
    "| 3      | YELLOW | Boston   |\n",
    "| 4      | GREEN  | Austin   |\n",
    "+--------+--------+----------+\n",
    "Orders table:\n",
    "+----------+------------+--------+----------+--------+\n",
    "| order_id | order_date | com_id | sales_id | amount |\n",
    "+----------+------------+--------+----------+--------+\n",
    "| 1        | 1/1/2014   | 3      | 4        | 10000  |\n",
    "| 2        | 2/1/2014   | 4      | 5        | 5000   |\n",
    "| 3        | 3/1/2014   | 1      | 1        | 50000  |\n",
    "| 4        | 4/1/2014   | 1      | 4        | 25000  |\n",
    "+----------+------------+--------+----------+--------+\n",
    "Output: \n",
    "+------+\n",
    "| name |\n",
    "+------+\n",
    "| Amy  |\n",
    "| Mark |\n",
    "| Alex |\n",
    "+------+\n",
    "Explanation: \n",
    "According to orders 3 and 4 in the Orders table, it is easy to tell that only salesperson John and Pam have sales to company RED, so we report all the other names in the table salesperson.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81dc0549-3f2d-43d3-a792-6e257d0df866",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+------+---------------+----------+\n|sales_id|name|salary|commission_rate| hire_date|\n+--------+----+------+---------------+----------+\n|       1|John|100000|              6|2006-04-01|\n|       2| Amy| 12000|              5|2010-05-01|\n|       3|Mark| 65000|             12|2008-12-25|\n|       4| Pam| 25000|             25|2005-01-01|\n|       5|Alex|  5000|             10|2007-02-03|\n+--------+----+------+---------------+----------+\n\n+------+------+--------+\n|com_id|  name|    city|\n+------+------+--------+\n|     1|   RED|  Boston|\n|     2|ORANGE|New York|\n|     3|YELLOW|  Boston|\n|     4| GREEN|  Austin|\n+------+------+--------+\n\n+--------+----------+------+--------+------+\n|order_id|order_date|com_id|sales_id|amount|\n+--------+----------+------+--------+------+\n|       1|2014-01-01|     3|       4| 10000|\n|       2|2014-02-01|     4|       5|  5000|\n|       3|2014-03-01|     1|       1| 50000|\n|       4|2014-04-01|     1|       4| 25000|\n+--------+----------+------+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Example\").getOrCreate()\n",
    "\n",
    "# Define the data\n",
    "sales_person_data = [\n",
    "    (1, 'John', 100000, 6, '2006-04-01'),\n",
    "    (2, 'Amy', 12000, 5, '2010-05-01'),\n",
    "    (3, 'Mark', 65000, 12, '2008-12-25'),\n",
    "    (4, 'Pam', 25000, 25, '2005-01-01'),\n",
    "    (5, 'Alex', 5000, 10, '2007-02-03')\n",
    "]\n",
    "\n",
    "company_data = [\n",
    "    (1, 'RED', 'Boston'),\n",
    "    (2, 'ORANGE', 'New York'),\n",
    "    (3, 'YELLOW', 'Boston'),\n",
    "    (4, 'GREEN', 'Austin')\n",
    "]\n",
    "\n",
    "orders_data = [\n",
    "    (1, '2014-01-01', 3, 4, 10000),\n",
    "    (2, '2014-02-01', 4, 5, 5000),\n",
    "    (3, '2014-03-01', 1, 1, 50000),\n",
    "    (4, '2014-04-01', 1, 4, 25000)\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "sales_person_df = spark.createDataFrame(sales_person_data, ['sales_id', 'name', 'salary', 'commission_rate', 'hire_date'])\n",
    "company_df = spark.createDataFrame(company_data, ['com_id', 'name', 'city'])\n",
    "orders_df = spark.createDataFrame(orders_data, ['order_id', 'order_date', 'com_id', 'sales_id', 'amount'])\n",
    "\n",
    "# Convert columns to appropriate types\n",
    "sales_person_df = sales_person_df.withColumn('hire_date', col('hire_date').cast('date'))\n",
    "orders_df = orders_df.withColumn('order_date', col('order_date').cast('date'))\n",
    "\n",
    "# Show DataFrames\n",
    "sales_person_df.show()\n",
    "company_df.show()\n",
    "orders_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7df26442-8a06-4177-89a2-cf771485f77a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n|name|\n+----+\n| Amy|\n|Mark|\n|Alex|\n+----+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to find the names of all the salespersons who did not have any orders related to the company with the name \"RED\".\n",
    "# Return the result table in any order.\n",
    "#Solution:\n",
    "\n",
    "df = sales_person_df.alias('S').join(orders_df.alias('O'), col('O.sales_id') == col('S.sales_id'),'left')\\\n",
    "    .join(company_df.alias('C'), col('C.com_id') == col('O.com_id'),'left').filter(col('C.name') == 'RED').select(col('S.sales_id').alias('Sales'))\n",
    "\n",
    "sales_person_df.filter(~col('sales_id').isin(df.select('Sales').rdd.flatMap(lambda x: x).collect())).select(col('name')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c45853e3-7e84-491f-88a0-2212ec1b4371",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1440. Evaluate Boolean Expression. \n",
    "```\n",
    "Table Variables:\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| name          | varchar |\n",
    "| value         | int     |\n",
    "+---------------+---------+\n",
    "In SQL, name is the primary key for this table.\n",
    "This table contains the stored variables and their values.\n",
    " \n",
    "\n",
    "Table Expressions:\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| left_operand  | varchar |\n",
    "| operator      | enum    |\n",
    "| right_operand | varchar |\n",
    "+---------------+---------+\n",
    "In SQL, (left_operand, operator, right_operand) is the primary key for this table.\n",
    "This table contains a boolean expression that should be evaluated.\n",
    "operator is an enum that takes one of the values ('<', '>', '=')\n",
    "The values of left_operand and right_operand are guaranteed to be in the Variables table.\n",
    " \n",
    "\n",
    "Evaluate the boolean expressions in Expressions table.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Variables table:\n",
    "+------+-------+\n",
    "| name | value |\n",
    "+------+-------+\n",
    "| x    | 66    |\n",
    "| y    | 77    |\n",
    "+------+-------+\n",
    "Expressions table:\n",
    "+--------------+----------+---------------+\n",
    "| left_operand | operator | right_operand |\n",
    "+--------------+----------+---------------+\n",
    "| x            | >        | y             |\n",
    "| x            | <        | y             |\n",
    "| x            | =        | y             |\n",
    "| y            | >        | x             |\n",
    "| y            | <        | x             |\n",
    "| x            | =        | x             |\n",
    "+--------------+----------+---------------+\n",
    "Output: \n",
    "+--------------+----------+---------------+-------+\n",
    "| left_operand | operator | right_operand | value |\n",
    "+--------------+----------+---------------+-------+\n",
    "| x            | >        | y             | false |\n",
    "| x            | <        | y             | true  |\n",
    "| x            | =        | y             | false |\n",
    "| y            | >        | x             | true  |\n",
    "| y            | <        | x             | false |\n",
    "| x            | =        | x             | true  |\n",
    "+--------------+----------+---------------+-------+\n",
    "Explanation: \n",
    "As shown, you need to find the value of each boolean expression in the table using the variables table.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "277fe936-87ad-4510-82c6-fa618cb5a5a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables DataFrame:\n+----+-----+\n|name|value|\n+----+-----+\n|   x|   66|\n|   y|   77|\n+----+-----+\n\nExpressions DataFrame:\n+------------+--------+-------------+\n|left_operand|operator|right_operand|\n+------------+--------+-------------+\n|           x|       >|            y|\n|           x|       <|            y|\n|           x|       =|            y|\n|           y|       >|            x|\n|           y|       <|            x|\n|           x|       =|            x|\n+------------+--------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Define schema for variables DataFrame\n",
    "variables_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"value\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Define data for variables DataFrame\n",
    "variables_data = [['x', 66], ['y', 77]]\n",
    "\n",
    "# Create DataFrame\n",
    "variables_df = spark.createDataFrame(variables_data, schema=variables_schema)\n",
    "\n",
    "# Define schema for expressions DataFrame\n",
    "expressions_schema = StructType([\n",
    "    StructField(\"left_operand\", StringType(), True),\n",
    "    StructField(\"operator\", StringType(), True),\n",
    "    StructField(\"right_operand\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define data for expressions DataFrame\n",
    "expressions_data = [['x', '>', 'y'], ['x', '<', 'y'], ['x', '=', 'y'], ['y', '>', 'x'], ['y', '<', 'x'], ['x', '=', 'x']]\n",
    "\n",
    "# Create DataFrame\n",
    "expressions_df = spark.createDataFrame(expressions_data, schema=expressions_schema)\n",
    "\n",
    "# Show the DataFrames\n",
    "print(\"Variables DataFrame:\")\n",
    "variables_df.show()\n",
    "\n",
    "print(\"Expressions DataFrame:\")\n",
    "expressions_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26389dff-0777-4cfc-8a5f-1d173672d1ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-------------+------+\n|left_operand|operator|right_operand|fValue|\n+------------+--------+-------------+------+\n|           y|       <|            x| false|\n|           y|       >|            x|  true|\n|           x|       =|            x|  true|\n|           x|       =|            y| false|\n|           x|       <|            y|  true|\n|           x|       >|            y| false|\n+------------+--------+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "expressions_df.alias('E').join(variables_df.alias('V1'),col('E.left_operand') == col('V1.name'),'inner')\\\n",
    "        .join(variables_df.alias('V2'),col('E.right_operand') == col('V2.name'),'inner')\\\n",
    "            .withColumn('fValue',when(col('operator') == '<',col('V1.value') < col('V2.value'))\\\n",
    "                .when(col('operator') == '>',col('V1.value') > col('V2.value'))\\\n",
    "                .when(col('operator') == '=',col('V1.value') == col('V2.value'))\n",
    "                ).select(col('left_operand'),col('E.operator'),col('right_operand'),col('fValue')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ade1a34-0515-483e-9632-5a67de3df260",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1212. Team Scores in Football Tournament\n",
    "```\n",
    "Table: Teams\n",
    "\n",
    "+---------------+----------+\n",
    "| Column Name   | Type     |\n",
    "+---------------+----------+\n",
    "| team_id       | int      |\n",
    "| team_name     | varchar  |\n",
    "+---------------+----------+\n",
    "team_id is the column with unique values of this table.\n",
    "Each row of this table represents a single football team.\n",
    " \n",
    "\n",
    "Table: Matches\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| match_id      | int     |\n",
    "| host_team     | int     |\n",
    "| guest_team    | int     | \n",
    "| host_goals    | int     |\n",
    "| guest_goals   | int     |\n",
    "+---------------+---------+\n",
    "match_id is the column of unique values of this table.\n",
    "Each row is a record of a finished match between two different teams. \n",
    "Teams host_team and guest_team are represented by their IDs in the Teams table (team_id), and they scored host_goals and guest_goals goals, respectively.\n",
    " \n",
    "\n",
    "You would like to compute the scores of all teams after all matches. Points are awarded as follows:\n",
    "A team receives three points if they win a match (i.e., Scored more goals than the opponent team).\n",
    "A team receives one point if they draw a match (i.e., Scored the same number of goals as the opponent team).\n",
    "A team receives no points if they lose a match (i.e., Scored fewer goals than the opponent team).\n",
    "Write a solution that selects the team_id, team_name and num_points of each team in the tournament after all described matches.\n",
    "\n",
    "Return the result table ordered by num_points in decreasing order. In case of a tie, order the records by team_id in increasing order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Teams table:\n",
    "+-----------+--------------+\n",
    "| team_id   | team_name    |\n",
    "+-----------+--------------+\n",
    "| 10        | Leetcode FC  |\n",
    "| 20        | NewYork FC   |\n",
    "| 30        | Atlanta FC   |\n",
    "| 40        | Chicago FC   |\n",
    "| 50        | Toronto FC   |\n",
    "+-----------+--------------+\n",
    "Matches table:\n",
    "+------------+--------------+---------------+-------------+--------------+\n",
    "| match_id   | host_team    | guest_team    | host_goals  | guest_goals  |\n",
    "+------------+--------------+---------------+-------------+--------------+\n",
    "| 1          | 10           | 20            | 3           | 0            |\n",
    "| 2          | 30           | 10            | 2           | 2            |\n",
    "| 3          | 10           | 50            | 5           | 1            |\n",
    "| 4          | 20           | 30            | 1           | 0            |\n",
    "| 5          | 50           | 30            | 1           | 0            |\n",
    "+------------+--------------+---------------+-------------+--------------+\n",
    "Output: \n",
    "+------------+--------------+---------------+\n",
    "| team_id    | team_name    | num_points    |\n",
    "+------------+--------------+---------------+\n",
    "| 10         | Leetcode FC  | 7             |\n",
    "| 20         | NewYork FC   | 3             |\n",
    "| 50         | Toronto FC   | 3             |\n",
    "| 30         | Atlanta FC   | 1             |\n",
    "| 40         | Chicago FC   | 0             |\n",
    "+------------+--------------+---------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56ed8128-45bd-4f80-a304-f16940d75a2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n|team_id|  team_name|\n+-------+-----------+\n|     10|Leetcode FC|\n|     20| NewYork FC|\n|     30| Atlanta FC|\n|     40| Chicago FC|\n|     50| Toronto FC|\n+-------+-----------+\n\n+--------+---------+----------+----------+-----------+\n|match_id|host_team|guest_team|host_goals|guest_goals|\n+--------+---------+----------+----------+-----------+\n|       1|       10|        20|         3|          0|\n|       2|       30|        10|         2|          2|\n|       3|       10|        50|         5|          1|\n|       4|       20|        30|         1|          0|\n|       5|       50|        30|         1|          0|\n+--------+---------+----------+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Define schema for teams\n",
    "teams_schema = StructType([\n",
    "    StructField('team_id', IntegerType(), True),\n",
    "    StructField('team_name', StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame for teams\n",
    "teams_data = [\n",
    "    (10, 'Leetcode FC'),\n",
    "    (20, 'NewYork FC'),\n",
    "    (30, 'Atlanta FC'),\n",
    "    (40, 'Chicago FC'),\n",
    "    (50, 'Toronto FC')\n",
    "]\n",
    "teams_df = spark.createDataFrame(teams_data, schema=teams_schema)\n",
    "\n",
    "# Define schema for matches\n",
    "matches_schema = StructType([\n",
    "    StructField('match_id', IntegerType(), True),\n",
    "    StructField('host_team', IntegerType(), True),\n",
    "    StructField('guest_team', IntegerType(), True),\n",
    "    StructField('host_goals', IntegerType(), True),\n",
    "    StructField('guest_goals', IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame for matches\n",
    "matches_data = [\n",
    "    (1, 10, 20, 3, 0),\n",
    "    (2, 30, 10, 2, 2),\n",
    "    (3, 10, 50, 5, 1),\n",
    "    (4, 20, 30, 1, 0),\n",
    "    (5, 50, 30, 1, 0)\n",
    "]\n",
    "matches_df = spark.createDataFrame(matches_data, schema=matches_schema)\n",
    "\n",
    "# Show the DataFrames\n",
    "teams_df.show()\n",
    "matches_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fddab2e2-f8bb-4da0-8aae-e0f89b5d21fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------+\n|team_id|  team_name|num_points|\n+-------+-----------+----------+\n|     10|Leetcode FC|         7|\n|     20| NewYork FC|         3|\n|     50| Toronto FC|         3|\n|      0| Chicago FC|         2|\n|     30| Atlanta FC|         1|\n+-------+-----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = teams_df.alias('T').join(matches_df.alias('M'), col('T.team_id') == col('M.host_team'), 'left')\\\n",
    "    .fillna(0)\\\n",
    "    .withColumn('num_points', when(col('host_goals') > col('guest_goals'), 3)\n",
    "                              .when(col('host_goals') < col('guest_goals'), 0)\n",
    "                              .when(col('host_goals') == col('guest_goals'), 1)\n",
    "                              .otherwise(0))\\\n",
    "    .select(col('host_team').alias('team_id'), col('T.team_name'), col('num_points'))\n",
    "\n",
    "# Compute points for guest teams\n",
    "df2 = teams_df.alias('T').join(matches_df.alias('M'), col('T.team_id') == col('M.guest_team'), 'left')\\\n",
    "    .fillna(0)\\\n",
    "    .withColumn('num_points', when(col('guest_goals') > col('host_goals'), 3)\n",
    "                              .when(col('guest_goals') < col('host_goals'), 0)\n",
    "                              .when(col('guest_goals') == col('host_goals'), 1)\n",
    "                              .otherwise(0))\\\n",
    "    .select(col('guest_team').alias('team_id'), col('T.team_name'), col('num_points'))\n",
    "\n",
    "# Combine host and guest results\n",
    "df3 = df1.union(df2)\n",
    "\n",
    "# Aggregate points by team_id and team_name\n",
    "result_df = df3.groupBy(col('team_id'), col('team_name'))\\\n",
    "    .agg(coalesce(sum(col('num_points')), lit(0)).alias('num_points'))\\\n",
    "    .orderBy(col('num_points').desc(), col('team_id'))\n",
    "\n",
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a760b5df-68be-4c38-9ad6-8fdb8f6af5f9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Basic Aggregate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eba4c331-03a9-4c55-8c51-93c9a67be4fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1890. The Latest Login in 2020\n",
    "```\n",
    "Table: Logins\n",
    "\n",
    "+----------------+----------+\n",
    "| Column Name    | Type     |\n",
    "+----------------+----------+\n",
    "| user_id        | int      |\n",
    "| time_stamp     | datetime |\n",
    "+----------------+----------+\n",
    "(user_id, time_stamp) is the primary key (combination of columns with unique values) for this table.\n",
    "Each row contains information about the login time for the user with ID user_id.\n",
    " \n",
    "\n",
    "Write a solution to report the latest login for all users in the year 2020. Do not include the users who did not login in 2020.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Logins table:\n",
    "+---------+---------------------+\n",
    "| user_id | time_stamp          |\n",
    "+---------+---------------------+\n",
    "| 6       | 2020-06-30 15:06:07 |\n",
    "| 6       | 2021-04-21 14:06:06 |\n",
    "| 6       | 2019-03-07 00:18:15 |\n",
    "| 8       | 2020-02-01 05:10:53 |\n",
    "| 8       | 2020-12-30 00:46:50 |\n",
    "| 2       | 2020-01-16 02:49:50 |\n",
    "| 2       | 2019-08-25 07:59:08 |\n",
    "| 14      | 2019-07-14 09:00:00 |\n",
    "| 14      | 2021-01-06 11:59:59 |\n",
    "+---------+---------------------+\n",
    "Output: \n",
    "+---------+---------------------+\n",
    "| user_id | last_stamp          |\n",
    "+---------+---------------------+\n",
    "| 6       | 2020-06-30 15:06:07 |\n",
    "| 8       | 2020-12-30 00:46:50 |\n",
    "| 2       | 2020-01-16 02:49:50 |\n",
    "+---------+---------------------+\n",
    "Explanation: \n",
    "User 6 logged into their account 3 times but only once in 2020, so we include this login in the result table.\n",
    "User 8 logged into their account 2 times in 2020, once in February and once in December. We include only the latest one (December) in the result table.\n",
    "User 2 logged into their account 2 times but only once in 2020, so we include this login in the result table.\n",
    "User 14 did not login in 2020, so we do not include them in the result table.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05e43d2c-15b4-46bd-b02a-1bf16ef01835",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n|user_id|         time_stamp|\n+-------+-------------------+\n|      6|2020-06-30 15:06:07|\n|      6|2021-04-21 14:06:06|\n|      6|2019-03-07 00:18:15|\n|      8|2020-02-01 05:10:53|\n|      8|2020-12-30 00:46:50|\n|      2|2020-01-16 02:49:50|\n|      2|2019-08-25 07:59:08|\n|     14|2019-07-14 09:00:00|\n|     14|2021-01-06 11:59:59|\n+-------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"LoginsExample\").getOrCreate()\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"time_stamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Create the data\n",
    "data = [\n",
    "    (6, datetime.strptime('2020-06-30 15:06:07', '%Y-%m-%d %H:%M:%S')),\n",
    "    (6, datetime.strptime('2021-04-21 14:06:06', '%Y-%m-%d %H:%M:%S')),\n",
    "    (6, datetime.strptime('2019-03-07 00:18:15', '%Y-%m-%d %H:%M:%S')),\n",
    "    (8, datetime.strptime('2020-02-01 05:10:53', '%Y-%m-%d %H:%M:%S')),\n",
    "    (8, datetime.strptime('2020-12-30 00:46:50', '%Y-%m-%d %H:%M:%S')),\n",
    "    (2, datetime.strptime('2020-01-16 02:49:50', '%Y-%m-%d %H:%M:%S')),\n",
    "    (2, datetime.strptime('2019-08-25 07:59:08', '%Y-%m-%d %H:%M:%S')),\n",
    "    (14, datetime.strptime('2019-07-14 09:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "    (14, datetime.strptime('2021-01-06 11:59:59', '%Y-%m-%d %H:%M:%S'))\n",
    "]\n",
    "\n",
    "# Create the DataFrame\n",
    "logins_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "logins_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebafb857-8d92-42cb-8eb5-98d2f7c2ed54",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n|user_id|         time_stamp|\n+-------+-------------------+\n|      2|2020-01-16 02:49:50|\n|      6|2020-06-30 15:06:07|\n|      8|2020-12-30 00:46:50|\n+-------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to report the latest login for all users in the year 2020. Do not include the users who did not login in 2020.\n",
    "\n",
    "# Return the result table in any order.\n",
    "\n",
    "# The result format is in the following example.\n",
    "\n",
    "\n",
    "# Initial thoughts:\n",
    "#     Filter 2020 data and apply window function row_number partition by user_id  time_stamp order by desc\n",
    "#     and select where row_number is 1\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "windowOptions = Window.partitionBy(col('user_id')).orderBy(col('time_stamp').desc())\n",
    "logins_df.filter(year(col('time_stamp')) == 2020).withColumn('RN',row_number().over(windowOptions))\\\n",
    "    .filter(col('RN') == 1).drop(col('RN')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73d6084c-73b9-4bdd-a4e7-396f674f3681",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 511. Game Play Analysis I\n",
    "```\n",
    "Table: Activity\n",
    "\n",
    "+--------------+---------+\n",
    "| Column Name  | Type    |\n",
    "+--------------+---------+\n",
    "| player_id    | int     |\n",
    "| device_id    | int     |\n",
    "| event_date   | date    |\n",
    "| games_played | int     |\n",
    "+--------------+---------+\n",
    "(player_id, event_date) is the primary key (combination of columns with unique values) of this table.\n",
    "This table shows the activity of players of some games.\n",
    "Each row is a record of a player who logged in and played a number of games (possibly 0) before logging out on someday using some device.\n",
    " \n",
    "\n",
    "Write a solution to find the first login date for each player.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Activity table:\n",
    "+-----------+-----------+------------+--------------+\n",
    "| player_id | device_id | event_date | games_played |\n",
    "+-----------+-----------+------------+--------------+\n",
    "| 1         | 2         | 2016-03-01 | 5            |\n",
    "| 1         | 2         | 2016-05-02 | 6            |\n",
    "| 2         | 3         | 2017-06-25 | 1            |\n",
    "| 3         | 1         | 2016-03-02 | 0            |\n",
    "| 3         | 4         | 2018-07-03 | 5            |\n",
    "+-----------+-----------+------------+--------------+\n",
    "Output: \n",
    "+-----------+-------------+\n",
    "| player_id | first_login |\n",
    "+-----------+-------------+\n",
    "| 1         | 2016-03-01  |\n",
    "| 2         | 2017-06-25  |\n",
    "| 3         | 2016-03-02  |\n",
    "+-----------+-------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c655a893-8acb-4e90-8a61-974bf9a8ed8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+------------+\n|player_id|device_id|event_date|games_played|\n+---------+---------+----------+------------+\n|        1|        2|2016-03-01|           5|\n|        1|        2|2016-05-02|           6|\n|        2|        3|2017-06-25|           1|\n|        3|        1|2016-03-02|           0|\n|        3|        4|2018-07-03|           5|\n+---------+---------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"player_id\", IntegerType(), True),\n",
    "    StructField(\"device_id\", IntegerType(), True),\n",
    "    StructField(\"event_date\", StringType(), True),\n",
    "    StructField(\"games_played\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create data\n",
    "data = [\n",
    "    (1, 2, '2016-03-01', 5),\n",
    "    (1, 2, '2016-05-02', 6),\n",
    "    (2, 3, '2017-06-25', 1),\n",
    "    (3, 1, '2016-03-02', 0),\n",
    "    (3, 4, '2018-07-03', 5)\n",
    "]\n",
    "\n",
    "# Create PySpark DataFrame\n",
    "activity_df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "activity_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "972319b3-3f6d-4981-91f9-8a07e031f3cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n|player_id|firstLogin|\n+---------+----------+\n|        1|2016-03-01|\n|        2|2017-06-25|\n|        3|2016-03-02|\n+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to find the first login date for each player.\n",
    "\n",
    "# Return the result table in any order.\n",
    "\n",
    "# Initial thoughts:\n",
    "# 1.Use window function rowNumber and order by event_date asc as first login\n",
    "# 2.filter based on rowNumber 1\n",
    "\n",
    "windowOptions = Window.partitionBy(col('player_id')).orderBy(col('event_date'))\n",
    "activity_df.withColumn('RN', row_number().over(windowOptions)).filter(col('RN') == 1)\\\n",
    "    .select(col('player_id'),col('event_date').alias('firstLogin')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f282be01-ed75-4d5b-9b2e-984824df26e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1571. Warehouse Manager\n",
    "```\n",
    "Table: Warehouse\n",
    "\n",
    "+--------------+---------+\n",
    "| Column Name  | Type    |\n",
    "+--------------+---------+\n",
    "| name         | varchar |\n",
    "| product_id   | int     |\n",
    "| units        | int     |\n",
    "+--------------+---------+\n",
    "(name, product_id) is the primary key (combination of columns with unique values) for this table.\n",
    "Each row of this table contains the information of the products in each warehouse.\n",
    " \n",
    "\n",
    "Table: Products\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| product_id    | int     |\n",
    "| product_name  | varchar |\n",
    "| Width         | int     |\n",
    "| Length        | int     |\n",
    "| Height        | int     |\n",
    "+---------------+---------+\n",
    "product_id is the primary key (column with unique values) for this table.\n",
    "Each row of this table contains information about the product dimensions (Width, Lenght, and Height) in feets of each product.\n",
    " \n",
    "\n",
    "Write a solution to report the number of cubic feet of volume the inventory occupies in each warehouse.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The query result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Warehouse table:\n",
    "+------------+--------------+-------------+\n",
    "| name       | product_id   | units       |\n",
    "+------------+--------------+-------------+\n",
    "| LCHouse1   | 1            | 1           |\n",
    "| LCHouse1   | 2            | 10          |\n",
    "| LCHouse1   | 3            | 5           |\n",
    "| LCHouse2   | 1            | 2           |\n",
    "| LCHouse2   | 2            | 2           |\n",
    "| LCHouse3   | 4            | 1           |\n",
    "+------------+--------------+-------------+\n",
    "Products table:\n",
    "+------------+--------------+------------+----------+-----------+\n",
    "| product_id | product_name | Width      | Length   | Height    |\n",
    "+------------+--------------+------------+----------+-----------+\n",
    "| 1          | LC-TV        | 5          | 50       | 40        |\n",
    "| 2          | LC-KeyChain  | 5          | 5        | 5         |\n",
    "| 3          | LC-Phone     | 2          | 10       | 10        |\n",
    "| 4          | LC-T-Shirt   | 4          | 10       | 20        |\n",
    "+------------+--------------+------------+----------+-----------+\n",
    "Output: \n",
    "+----------------+------------+\n",
    "| warehouse_name | volume     | \n",
    "+----------------+------------+\n",
    "| LCHouse1       | 12250      | \n",
    "| LCHouse2       | 20250      |\n",
    "| LCHouse3       | 800        |\n",
    "+----------------+------------+\n",
    "Explanation: \n",
    "Volume of product_id = 1 (LC-TV), 5x50x40 = 10000\n",
    "Volume of product_id = 2 (LC-KeyChain), 5x5x5 = 125 \n",
    "Volume of product_id = 3 (LC-Phone), 2x10x10 = 200\n",
    "Volume of product_id = 4 (LC-T-Shirt), 4x10x20 = 800\n",
    "LCHouse1: 1 unit of LC-TV + 10 units of LC-KeyChain + 5 units of LC-Phone.\n",
    "          Total volume: 1*10000 + 10*125  + 5*200 = 12250 cubic feet\n",
    "LCHouse2: 2 units of LC-TV + 2 units of LC-KeyChain.\n",
    "          Total volume: 2*10000 + 2*125 = 20250 cubic feet\n",
    "LCHouse3: 1 unit of LC-T-Shirt.\n",
    "          Total volume: 1*800 = 800 cubic feet.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df8c2b0e-1878-4518-9b1a-f3e57b3cbb34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----+\n|    name|product_id|units|\n+--------+----------+-----+\n|LCHouse1|         1|    1|\n|LCHouse1|         2|   10|\n|LCHouse1|         3|    5|\n|LCHouse2|         1|    2|\n|LCHouse2|         2|    2|\n|LCHouse3|         4|    1|\n+--------+----------+-----+\n\n+----------+------------+-----+------+------+\n|product_id|product_name|Width|Length|Height|\n+----------+------------+-----+------+------+\n|         1|       LC-TV|    5|    50|    40|\n|         2| LC-KeyChain|    5|     5|     5|\n|         3|    LC-Phone|    2|    10|    10|\n|         4|  LC-T-Shirt|    4|    10|    20|\n+----------+------------+-----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Assuming spark session is already created\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Data for warehouse\n",
    "warehouse_data = [\n",
    "    ('LCHouse1', 1, 1), \n",
    "    ('LCHouse1', 2, 10), \n",
    "    ('LCHouse1', 3, 5), \n",
    "    ('LCHouse2', 1, 2), \n",
    "    ('LCHouse2', 2, 2), \n",
    "    ('LCHouse3', 4, 1)\n",
    "]\n",
    "\n",
    "warehouse_schema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('product_id', IntegerType(), True),\n",
    "    StructField('units', IntegerType(), True)\n",
    "])\n",
    "\n",
    "warehouse_df = spark.createDataFrame(warehouse_data, schema=warehouse_schema)\n",
    "\n",
    "# Data for products\n",
    "products_data = [\n",
    "    (1, 'LC-TV', 5, 50, 40),\n",
    "    (2, 'LC-KeyChain', 5, 5, 5),\n",
    "    (3, 'LC-Phone', 2, 10, 10),\n",
    "    (4, 'LC-T-Shirt', 4, 10, 20)\n",
    "]\n",
    "\n",
    "products_schema = StructType([\n",
    "    StructField('product_id', IntegerType(), True),\n",
    "    StructField('product_name', StringType(), True),\n",
    "    StructField('Width', IntegerType(), True),\n",
    "    StructField('Length', IntegerType(), True),\n",
    "    StructField('Height', IntegerType(), True)\n",
    "])\n",
    "\n",
    "products_df = spark.createDataFrame(products_data, schema=products_schema)\n",
    "\n",
    "# Display the DataFrames\n",
    "warehouse_df.show()\n",
    "products_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bca964c0-299f-4657-87f2-27b71c14ca0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n|    name|volume|\n+--------+------+\n|LCHouse1| 12250|\n|LCHouse2| 20250|\n|LCHouse3|   800|\n+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to report the number of cubic feet of volume the inventory occupies in each warehouse.\n",
    "\n",
    "# Return the result table in any order.\n",
    "\n",
    "# 1.Here we ar trying to find the total space will be occupied by the products.\n",
    "# 2.first we left join warehouse with product then multiple the width * length * height\n",
    "# 4. mutiply the toal value with units in warehouse\n",
    "# 5. return warehouse and its total volume. Group by warehouse and sum the final value.\n",
    "# 6.warehouse, volumns as output.\n",
    "\n",
    "warehouse_df.alias('W').join(products_df.alias('P'), col('W.product_id') == col('P.product_id'),'left')\\\n",
    "    .withColumn('volume',( col('Width') * col('Length') * col('Height')) * col('units'))\\\n",
    "        .groupBy(col('name')).agg(sum('volume').alias('volume')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1b89a3f-724c-4a8e-a8de-ea700e53d485",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 586. Customer Placing the Largest Number of Orders\n",
    "```\n",
    "Table: Orders\n",
    "\n",
    "+-----------------+----------+\n",
    "| Column Name     | Type     |\n",
    "+-----------------+----------+\n",
    "| order_number    | int      |\n",
    "| customer_number | int      |\n",
    "+-----------------+----------+\n",
    "order_number is the primary key (column with unique values) for this table.\n",
    "This table contains information about the order ID and the customer ID.\n",
    " \n",
    "\n",
    "Write a solution to find the customer_number for the customer who has placed the largest number of orders.\n",
    "\n",
    "The test cases are generated so that exactly one customer will have placed more orders than any other customer.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Orders table:\n",
    "+--------------+-----------------+\n",
    "| order_number | customer_number |\n",
    "+--------------+-----------------+\n",
    "| 1            | 1               |\n",
    "| 2            | 2               |\n",
    "| 3            | 3               |\n",
    "| 4            | 3               |\n",
    "+--------------+-----------------+\n",
    "Output: \n",
    "+-----------------+\n",
    "| customer_number |\n",
    "+-----------------+\n",
    "| 3               |\n",
    "+-----------------+\n",
    "Explanation: \n",
    "The customer with number 3 has two orders, which is greater than either customer 1 or 2 because each of them only has one order. \n",
    "So the result is customer_number 3.\n",
    " \n",
    "\n",
    "Follow up: What if more than one customer has the largest number of orders, can you find all the customer_number in this case?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "775a3b33-c4fe-431a-9624-cb13d070e1df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n|order_number|customer_number|\n+------------+---------------+\n|           1|              1|\n|           2|              2|\n|           3|              3|\n|           4|              3|\n+------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "# Assuming spark session is already created\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Data for orders\n",
    "orders_data = [\n",
    "    (1, 1),\n",
    "    (2, 2),\n",
    "    (3, 3),\n",
    "    (4, 3)\n",
    "]\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField('order_number', IntegerType(), True),\n",
    "    StructField('customer_number', IntegerType(), True)\n",
    "])\n",
    "\n",
    "orders_df = spark.createDataFrame(orders_data, schema=orders_schema)\n",
    "\n",
    "# Display the DataFrame\n",
    "orders_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c43cbe73-d9d9-4d37-83a8-1f759ca45ea2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n|customer_number|\n+---------------+\n|              3|\n+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to find the customer_number for the customer who has placed the largest number of orders.\n",
    "\n",
    "# The test cases are generated so that exactly one customer will have placed more orders than any other customer.\n",
    "\n",
    "# Initial thought:\n",
    "# 1.group by customer order by count desc limit 1 or max count\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "orders_df.groupBy(col('customer_number')).agg(count(col('order_number')).alias('noOfOrders'))\\\n",
    "    .orderBy(col('noOfOrders').desc()).select(col('customer_number')).limit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75fa52a2-020c-4792-a623-feea4ddbae92",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1741. Find Total Time Spent by Each Employee\n",
    "```\n",
    "Table: Employees\n",
    "\n",
    "+-------------+------+\n",
    "| Column Name | Type |\n",
    "+-------------+------+\n",
    "| emp_id      | int  |\n",
    "| event_day   | date |\n",
    "| in_time     | int  |\n",
    "| out_time    | int  |\n",
    "+-------------+------+\n",
    "(emp_id, event_day, in_time) is the primary key (combinations of columns with unique values) of this table.\n",
    "The table shows the employees' entries and exits in an office.\n",
    "event_day is the day at which this event happened, in_time is the minute at which the employee entered the office, and out_time is the minute at which they left the office.\n",
    "in_time and out_time are between 1 and 1440.\n",
    "It is guaranteed that no two events on the same day intersect in time, and in_time < out_time.\n",
    " \n",
    "\n",
    "Write a solution to calculate the total time in minutes spent by each employee on each day at the office. Note that within one day, an employee can enter and leave more than once. The time spent in the office for a single entry is out_time - in_time.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Employees table:\n",
    "+--------+------------+---------+----------+\n",
    "| emp_id | event_day  | in_time | out_time |\n",
    "+--------+------------+---------+----------+\n",
    "| 1      | 2020-11-28 | 4       | 32       |\n",
    "| 1      | 2020-11-28 | 55      | 200      |\n",
    "| 1      | 2020-12-03 | 1       | 42       |\n",
    "| 2      | 2020-11-28 | 3       | 33       |\n",
    "| 2      | 2020-12-09 | 47      | 74       |\n",
    "+--------+------------+---------+----------+\n",
    "Output: \n",
    "+------------+--------+------------+\n",
    "| day        | emp_id | total_time |\n",
    "+------------+--------+------------+\n",
    "| 2020-11-28 | 1      | 173        |\n",
    "| 2020-11-28 | 2      | 30         |\n",
    "| 2020-12-03 | 1      | 41         |\n",
    "| 2020-12-09 | 2      | 27         |\n",
    "+------------+--------+------------+\n",
    "Explanation: \n",
    "Employee 1 has three events: two on day 2020-11-28 with a total of (32 - 4) + (200 - 55) = 173, and one on day 2020-12-03 with a total of (42 - 1) = 41.\n",
    "Employee 2 has two events: one on day 2020-11-28 with a total of (33 - 3) = 30, and one on day 2020-12-09 with a total of (74 - 47) = 27.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c24144fe-7b34-46d6-9384-913b703420ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+--------+\n|emp_id| event_day|in_time|out_time|\n+------+----------+-------+--------+\n|     1|2020-11-28|      4|      32|\n|     1|2020-11-28|     55|     200|\n|     1|2020-12-03|      1|      42|\n|     2|2020-11-28|      3|      33|\n|     2|2020-12-09|     47|      74|\n+------+----------+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "\n",
    "# Assuming spark session is already created\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Data for employees\n",
    "employees_data = [\n",
    "    (1, '2020-11-28', 4, 32),\n",
    "    (1, '2020-11-28', 55, 200),\n",
    "    (1, '2020-12-03', 1, 42),\n",
    "    (2, '2020-11-28', 3, 33),\n",
    "    (2, '2020-12-09', 47, 74)\n",
    "]\n",
    "\n",
    "employees_schema = StructType([\n",
    "    StructField('emp_id', IntegerType(), True),\n",
    "    StructField('event_day', StringType(), True),\n",
    "    StructField('in_time', IntegerType(), True),\n",
    "    StructField('out_time', IntegerType(), True)\n",
    "])\n",
    "\n",
    "employees_df = spark.createDataFrame(employees_data, schema=employees_schema)\n",
    "\n",
    "# Display the DataFrame\n",
    "employees_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b178bb1-1289-49d7-8f6f-45afb95711a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+\n|emp_id| event_day|total_time|\n+------+----------+----------+\n|     1|2020-11-28|       173|\n|     1|2020-12-03|        41|\n|     2|2020-11-28|        30|\n|     2|2020-12-09|        27|\n+------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to calculate the total time in minutes spent by each employee on each day at the office. Note that within one day, an employee can enter and leave more than once. The time spent in the office for a single entry is out_time - in_time.\n",
    "\n",
    "# Return the result table in any order.\n",
    "\n",
    "# 1. simple group by the results after finding total time result in any order.\n",
    "\n",
    "\n",
    "employees_df.withColumn('total_time', col('out_time') - col('in_time'))\\\n",
    "    .groupBy(col('emp_id'),col('event_day')).agg(sum(col('total_time')).alias('total_time')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56816759-1396-4cf5-8b3a-9aad2d1e5e22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1173. Immediate Food Delivery I\n",
    "```\n",
    "Table: Delivery\n",
    "\n",
    "+-----------------------------+---------+\n",
    "| Column Name                 | Type    |\n",
    "+-----------------------------+---------+\n",
    "| delivery_id                 | int     |\n",
    "| customer_id                 | int     |\n",
    "| order_date                  | date    |\n",
    "| customer_pref_delivery_date | date    |\n",
    "+-----------------------------+---------+\n",
    "delivery_id is the primary key (column with unique values) of this table.\n",
    "The table holds information about food delivery to customers that make orders at some date and specify a preferred delivery date (on the same order date or after it).\n",
    " \n",
    "\n",
    "If the customer's preferred delivery date is the same as the order date, then the order is called immediate; otherwise, it is called scheduled.\n",
    "\n",
    "Write a solution to find the percentage of immediate orders in the table, rounded to 2 decimal places.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Delivery table:\n",
    "+-------------+-------------+------------+-----------------------------+\n",
    "| delivery_id | customer_id | order_date | customer_pref_delivery_date |\n",
    "+-------------+-------------+------------+-----------------------------+\n",
    "| 1           | 1           | 2019-08-01 | 2019-08-02                  |\n",
    "| 2           | 5           | 2019-08-02 | 2019-08-02                  |\n",
    "| 3           | 1           | 2019-08-11 | 2019-08-11                  |\n",
    "| 4           | 3           | 2019-08-24 | 2019-08-26                  |\n",
    "| 5           | 4           | 2019-08-21 | 2019-08-22                  |\n",
    "| 6           | 2           | 2019-08-11 | 2019-08-13                  |\n",
    "+-------------+-------------+------------+-----------------------------+\n",
    "Output: \n",
    "+----------------------+\n",
    "| immediate_percentage |\n",
    "+----------------------+\n",
    "| 33.33                |\n",
    "+----------------------+\n",
    "Explanation: The orders with delivery id 2 and 3 are immediate while the others are scheduled.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be575008-ace7-4c34-9d37-d02a2ea60b9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------+---------------------------+\n|delivery_id|customer_id|order_date|customer_pref_delivery_date|\n+-----------+-----------+----------+---------------------------+\n|          1|          1|2019-08-01|                 2019-08-02|\n|          2|          5|2019-08-02|                 2019-08-02|\n|          3|          1|2019-08-11|                 2019-08-11|\n|          4|          3|2019-08-24|                 2019-08-26|\n|          5|          4|2019-08-21|                 2019-08-22|\n|          6|          2|2019-08-11|                 2019-08-13|\n+-----------+-----------+----------+---------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType\n",
    "\n",
    "# Assuming spark session is already created\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Data for delivery\n",
    "delivery_data = [\n",
    "    (1, 1, '2019-08-01', '2019-08-02'),\n",
    "    (2, 5, '2019-08-02', '2019-08-02'),\n",
    "    (3, 1, '2019-08-11', '2019-08-11'),\n",
    "    (4, 3, '2019-08-24', '2019-08-26'),\n",
    "    (5, 4, '2019-08-21', '2019-08-22'),\n",
    "    (6, 2, '2019-08-11', '2019-08-13')\n",
    "]\n",
    "\n",
    "delivery_schema = StructType([\n",
    "    StructField('delivery_id', IntegerType(), True),\n",
    "    StructField('customer_id', IntegerType(), True),\n",
    "    StructField('order_date', StringType(), True),\n",
    "    StructField('customer_pref_delivery_date', StringType(), True)\n",
    "])\n",
    "\n",
    "delivery_df = spark.createDataFrame(delivery_data, schema=delivery_schema)\n",
    "\n",
    "# Display the DataFrame\n",
    "delivery_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a733af-3a6b-4d6d-b183-d419be3501e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|immediate_percentage|\n+--------------------+\n|               33.33|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# If the customer's preferred delivery date is the same as the order date, then the order is called immediate; otherwise, it is called scheduled.\n",
    "\n",
    "# Write a solution to find the percentage of immediate orders in the table, rounded to 2 decimal places.\n",
    "\n",
    "# 1.withColumnn Use case when order_date == customre_delivery_prep_date then 1 else 0. then apply avg function and round it to 2\n",
    "\n",
    "delivery_df.withColumn('immediteOrders', when(col('order_date') == col('customer_pref_delivery_date'),1)\\\n",
    "    .otherwise(0)).agg(round(avg(col('immediteOrders'))*100,2).alias('immediate_percentage')).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1627094b-d45c-412c-83d1-a614169ba22c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## (Premium) 1445. Apples & Oranges\n",
    "\n",
    "```\n",
    "Table: Sales\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| sale_date     | date    |\n",
    "| fruit         | enum    | \n",
    "| sold_num      | int     | \n",
    "+---------------+---------+\n",
    "(sale_date, fruit) is the primary key (combination of columns with unique values) of this table.\n",
    "This table contains the sales of \"apples\" and \"oranges\" sold each day.\n",
    " \n",
    "\n",
    "Write a solution to report the difference between the number of apples and oranges sold each day.\n",
    "\n",
    "Return the result table ordered by sale_date.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Sales table:\n",
    "+------------+------------+-------------+\n",
    "| sale_date  | fruit      | sold_num    |\n",
    "+------------+------------+-------------+\n",
    "| 2020-05-01 | apples     | 10          |\n",
    "| 2020-05-01 | oranges    | 8           |\n",
    "| 2020-05-02 | apples     | 15          |\n",
    "| 2020-05-02 | oranges    | 15          |\n",
    "| 2020-05-03 | apples     | 20          |\n",
    "| 2020-05-03 | oranges    | 0           |\n",
    "| 2020-05-04 | apples     | 15          |\n",
    "| 2020-05-04 | oranges    | 16          |\n",
    "+------------+------------+-------------+\n",
    "Output: \n",
    "+------------+--------------+\n",
    "| sale_date  | diff         |\n",
    "+------------+--------------+\n",
    "| 2020-05-01 | 2            |\n",
    "| 2020-05-02 | 0            |\n",
    "| 2020-05-03 | 20           |\n",
    "| 2020-05-04 | -1           |\n",
    "+------------+--------------+\n",
    "Explanation: \n",
    "Day 2020-05-01, 10 apples and 8 oranges were sold (Difference  10 - 8 = 2).\n",
    "Day 2020-05-02, 15 apples and 15 oranges were sold (Difference 15 - 15 = 0).\n",
    "Day 2020-05-03, 20 apples and 0 oranges were sold (Difference 20 - 0 = 20).\n",
    "Day 2020-05-04, 15 apples and 16 oranges were sold (Difference 15 - 16 = -1).\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5832bf37-7808-4d2d-98ea-9302c817f921",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------+\n| sale_date|  fruit|sold_num|\n+----------+-------+--------+\n|2020-05-01| apples|      10|\n|2020-05-01|oranges|       8|\n|2020-05-02| apples|      15|\n|2020-05-02|oranges|      15|\n|2020-05-03| apples|      20|\n|2020-05-03|oranges|       0|\n|2020-05-04| apples|      15|\n|2020-05-04|oranges|      16|\n+----------+-------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "# Assuming spark session is already created\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Data for sales\n",
    "sales_data = [\n",
    "    ('2020-05-01', 'apples', 10),\n",
    "    ('2020-05-01', 'oranges', 8),\n",
    "    ('2020-05-02', 'apples', 15),\n",
    "    ('2020-05-02', 'oranges', 15),\n",
    "    ('2020-05-03', 'apples', 20),\n",
    "    ('2020-05-03', 'oranges', 0),\n",
    "    ('2020-05-04', 'apples', 15),\n",
    "    ('2020-05-04', 'oranges', 16)\n",
    "]\n",
    "\n",
    "sales_schema = StructType([\n",
    "    StructField('sale_date', StringType(), True),\n",
    "    StructField('fruit', StringType(), True),\n",
    "    StructField('sold_num', IntegerType(), True)\n",
    "])\n",
    "\n",
    "sales_df = spark.createDataFrame(sales_data, schema=sales_schema)\n",
    "\n",
    "# Display the DataFrame\n",
    "sales_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72fb2574-95c3-478c-a289-5594dbf09507",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+\n| sale_date|diff|\n+----------+----+\n|2020-05-01|   2|\n|2020-05-02|   0|\n|2020-05-03|  20|\n|2020-05-04|  -1|\n+----------+----+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to report the difference between the number of apples and oranges sold each day.\n",
    "# Return the result table ordered by sale_date.\n",
    "\n",
    "# 1. first find the differnce by apples - oranges\n",
    "# 2. Group by sale_date and apply sum on the diff.\n",
    "\n",
    "sales_df.withColumn('applesDiff', when(col('fruit') == 'apples', col('sold_num')).otherwise(0))\\\n",
    "    .withColumn('orangeDiff', when(col('fruit') == 'oranges', col('sold_num')).otherwise(0)).groupBy(col('sale_date'))\\\n",
    "        .agg((sum(col('applesDiff')) - sum(col('orangeDiff'))).alias('diff')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59ba9366-0676-4dea-b140-f799a5014eac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium)1699. Number of Calls Between Two Persons\n",
    "```\n",
    "Table: Calls\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| from_id     | int     |\n",
    "| to_id       | int     |\n",
    "| duration    | int     |\n",
    "+-------------+---------+\n",
    "This table does not have a primary key (column with unique values), it may contain duplicates.\n",
    "This table contains the duration of a phone call between from_id and to_id.\n",
    "from_id != to_id\n",
    " \n",
    "\n",
    "Write a solution to report the number of calls and the total call duration between each pair of distinct persons (person1, person2) where person1 < person2.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Calls table:\n",
    "+---------+-------+----------+\n",
    "| from_id | to_id | duration |\n",
    "+---------+-------+----------+\n",
    "| 1       | 2     | 59       |\n",
    "| 2       | 1     | 11       |\n",
    "| 1       | 3     | 20       |\n",
    "| 3       | 4     | 100      |\n",
    "| 3       | 4     | 200      |\n",
    "| 3       | 4     | 200      |\n",
    "| 4       | 3     | 499      |\n",
    "+---------+-------+----------+\n",
    "Output: \n",
    "+---------+---------+------------+----------------+\n",
    "| person1 | person2 | call_count | total_duration |\n",
    "+---------+---------+------------+----------------+\n",
    "| 1       | 2       | 2          | 70             |\n",
    "| 1       | 3       | 1          | 20             |\n",
    "| 3       | 4       | 4          | 999            |\n",
    "+---------+---------+------------+----------------+\n",
    "Explanation: \n",
    "Users 1 and 2 had 2 calls and the total duration is 70 (59 + 11).\n",
    "Users 1 and 3 had 1 call and the total duration is 20.\n",
    "Users 3 and 4 had 4 calls and the total duration is 999 (100 + 200 + 200 + 499).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80257a38-b9b5-4a62-89f2-f98822eb249a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+\n|from_id|to_id|duration|\n+-------+-----+--------+\n|      1|    2|      59|\n|      2|    1|      11|\n|      1|    3|      20|\n|      3|    4|     100|\n|      3|    4|     200|\n|      3|    4|     200|\n|      4|    3|     499|\n+-------+-----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "# Assuming Spark session is already created\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField('from_id', IntegerType(), True),\n",
    "    StructField('to_id', IntegerType(), True),\n",
    "    StructField('duration', IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, 2, 59),\n",
    "    (2, 1, 11),\n",
    "    (1, 3, 20),\n",
    "    (3, 4, 100),\n",
    "    (3, 4, 200),\n",
    "    (3, 4, 200),\n",
    "    (4, 3, 499)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "calls_df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "calls_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8893093a-8ef4-4f9c-ad25-3100da453bed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+--------------+\n|from_id|to_id|call_count|total_duration|\n+-------+-----+----------+--------------+\n|      1|    2|         2|            70|\n|      1|    3|         1|            20|\n|      3|    4|         4|           999|\n+-------+-----+----------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to report the number of calls and the total call duration between each pair of distinct persons (person1, person2) where person1 < person2.\n",
    "\n",
    "# Return the result table in any order.\n",
    "\n",
    "# 1. Here we want to find between each pair it means the call direction is from both sides. we do union by first select from_id to to_id the next union to_id to from_id this way we can make the columns as single source and group the data.\n",
    "# 2. Then we can get count and sum the duration.\n",
    "\n",
    "df1 = calls_df.select(col('from_id'),col('to_id'),col('duration')).filter(col('from_id') < col('to_id'))\n",
    "df2 = calls_df.select(col('to_id'),col('from_id'),col('duration')).filter(col('from_id') > col('to_id'))\n",
    "df1.union(df2).groupBy(col('from_id'),col('to_id'))\\\n",
    "    .agg(count(col('duration')).alias('call_count'), sum(col('duration')).alias('total_duration')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2679b02-56b9-4ef1-ac9d-0faa0e0b1b9b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Sorting and Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64718893-0141-4241-966b-a939187f8032",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1587. Bank Account Summary II \n",
    "```\n",
    "Table: Users\n",
    "\n",
    "+--------------+---------+\n",
    "| Column Name  | Type    |\n",
    "+--------------+---------+\n",
    "| account      | int     |\n",
    "| name         | varchar |\n",
    "+--------------+---------+\n",
    "account is the primary key (column with unique values) for this table.\n",
    "Each row of this table contains the account number of each user in the bank.\n",
    "There will be no two users having the same name in the table.\n",
    " \n",
    "\n",
    "Table: Transactions\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| trans_id      | int     |\n",
    "| account       | int     |\n",
    "| amount        | int     |\n",
    "| transacted_on | date    |\n",
    "+---------------+---------+\n",
    "trans_id is the primary key (column with unique values) for this table.\n",
    "Each row of this table contains all changes made to all accounts.\n",
    "amount is positive if the user received money and negative if they transferred money.\n",
    "All accounts start with a balance of 0.\n",
    " \n",
    "\n",
    "Write a solution to report the name and balance of users with a balance higher than 10000. The balance of an account is equal to the sum of the amounts of all transactions involving that account.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Users table:\n",
    "+------------+--------------+\n",
    "| account    | name         |\n",
    "+------------+--------------+\n",
    "| 900001     | Alice        |\n",
    "| 900002     | Bob          |\n",
    "| 900003     | Charlie      |\n",
    "+------------+--------------+\n",
    "Transactions table:\n",
    "+------------+------------+------------+---------------+\n",
    "| trans_id   | account    | amount     | transacted_on |\n",
    "+------------+------------+------------+---------------+\n",
    "| 1          | 900001     | 7000       |  2020-08-01   |\n",
    "| 2          | 900001     | 7000       |  2020-09-01   |\n",
    "| 3          | 900001     | -3000      |  2020-09-02   |\n",
    "| 4          | 900002     | 1000       |  2020-09-12   |\n",
    "| 5          | 900003     | 6000       |  2020-08-07   |\n",
    "| 6          | 900003     | 6000       |  2020-09-07   |\n",
    "| 7          | 900003     | -4000      |  2020-09-11   |\n",
    "+------------+------------+------------+---------------+\n",
    "Output: \n",
    "+------------+------------+\n",
    "| name       | balance    |\n",
    "+------------+------------+\n",
    "| Alice      | 11000      |\n",
    "+------------+------------+\n",
    "Explanation: \n",
    "Alice's balance is (7000 + 7000 - 3000) = 11000.\n",
    "Bob's balance is 1000.\n",
    "Charlie's balance is (6000 + 6000 - 4000) = 8000.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c06e7f0-863e-4625-9f43-18e393ee4ecd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n|account|   name|\n+-------+-------+\n| 900001|  Alice|\n| 900002|    Bob|\n| 900003|Charlie|\n+-------+-------+\n\n+--------+-------+------+-------------+\n|trans_id|account|amount|transacted_on|\n+--------+-------+------+-------------+\n|       1| 900001|  7000|   2020-08-01|\n|       2| 900001|  7000|   2020-09-01|\n|       3| 900001| -3000|   2020-09-02|\n|       4| 900002|  1000|   2020-09-12|\n|       5| 900003|  6000|   2020-08-07|\n|       6| 900003|  6000|   2020-09-07|\n|       7| 900003| -4000|   2020-09-11|\n+--------+-------+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "# Assuming Spark session is already created\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define schema for the 'users' DataFrame\n",
    "users_schema = StructType([\n",
    "    StructField('account', IntegerType(), True),\n",
    "    StructField('name', StringType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for the 'transactions' DataFrame\n",
    "transactions_schema = StructType([\n",
    "    StructField('trans_id', IntegerType(), True),\n",
    "    StructField('account', IntegerType(), True),\n",
    "    StructField('amount', IntegerType(), True),\n",
    "    StructField('transacted_on', StringType(), True)\n",
    "])\n",
    "\n",
    "# Sample data for 'users'\n",
    "users_data = [\n",
    "    (900001, 'Alice'),\n",
    "    (900002, 'Bob'),\n",
    "    (900003, 'Charlie')\n",
    "]\n",
    "\n",
    "# Sample data for 'transactions'\n",
    "transactions_data = [\n",
    "    (1, 900001, 7000, '2020-08-01'),\n",
    "    (2, 900001, 7000, '2020-09-01'),\n",
    "    (3, 900001, -3000, '2020-09-02'),\n",
    "    (4, 900002, 1000, '2020-09-12'),\n",
    "    (5, 900003, 6000, '2020-08-07'),\n",
    "    (6, 900003, 6000, '2020-09-07'),\n",
    "    (7, 900003, -4000, '2020-09-11')\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "users_df = spark.createDataFrame(users_data, schema=users_schema)\n",
    "transactions_df = spark.createDataFrame(transactions_data, schema=transactions_schema)\n",
    "\n",
    "# Show the DataFrames\n",
    "users_df.show()\n",
    "transactions_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7da5cc55-6c06-4500-a5da-a3cff8197bba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n| name|totalAmount|\n+-----+-----------+\n|Alice|      11000|\n+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to report the name and balance of users with a balance higher than 10000. The balance of an account is equal to the sum of the amounts of all transactions involving that account.\n",
    "\n",
    "# Return the result table in any order.\n",
    "\n",
    "# 1. Group by account and sum the amount and filter the amount > 1000\n",
    "#Tip while passing a negative value into sum function it will automatically minus the amount.\n",
    "transactions_df.alias('T').join(users_df.alias('U'),col('T.account') == col('U.account'),'left')\\\n",
    "    .groupBy(col('name')).agg(sum(col('amount')).alias('totalAmount')).filter(col('totalAmount') > 10000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df7e11c3-fd5f-4713-801c-ec967762bab5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 182. Duplicate Emails\n",
    "### Leve : Easy\n",
    "\n",
    "```\n",
    "Table: Person\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| id          | int     |\n",
    "| email       | varchar |\n",
    "+-------------+---------+\n",
    "id is the primary key (column with unique values) for this table.\n",
    "Each row of this table contains an email. The emails will not contain uppercase letters.\n",
    " \n",
    "\n",
    "Write a solution to report all the duplicate emails. Note that it's guaranteed that the email field is not NULL.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Person table:\n",
    "+----+---------+\n",
    "| id | email   |\n",
    "+----+---------+\n",
    "| 1  | a@b.com |\n",
    "| 2  | c@d.com |\n",
    "| 3  | a@b.com |\n",
    "+----+---------+\n",
    "Output: \n",
    "+---------+\n",
    "| Email   |\n",
    "+---------+\n",
    "| a@b.com |\n",
    "+---------+\n",
    "Explanation: a@b.com is repeated two times.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c27319a-853e-4e30-88d1-160757d61c01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n| id|  email|\n+---+-------+\n|  1|a@b.com|\n|  2|c@d.com|\n|  3|a@b.com|\n+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataConversion\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(1, 'a@b.com'), (2, 'c@d.com'), (3, 'a@b.com')]\n",
    "columns = ['id', 'email']\n",
    "\n",
    "# Create PySpark DataFrame\n",
    "person_df = spark.createDataFrame(data, columns)\n",
    "person_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "549a34dd-6c37-4e2b-9c1a-6ee75ee86be9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n|  email|\n+-------+\n|a@b.com|\n+-------+\n\n"
     ]
    }
   ],
   "source": [
    "person_df.groupBy(col('email')).agg(count('*').alias('cnt')).filter(col('cnt') > 1 ).select(col('email')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee7cdb46-8b2e-4261-8edd-ed224d524f9d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1050. Actors and Directors Who Cooperated At Least Three Times\n",
    "### Easy\n",
    "\n",
    "```\n",
    "\n",
    "Table: ActorDirector\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| actor_id    | int     |\n",
    "| director_id | int     |\n",
    "| timestamp   | int     |\n",
    "+-------------+---------+\n",
    "timestamp is the primary key (column with unique values) for this table.\n",
    " \n",
    "\n",
    "Write a solution to find all the pairs (actor_id, director_id) where the actor has cooperated with the director at least three times.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "ActorDirector table:\n",
    "+-------------+-------------+-------------+\n",
    "| actor_id    | director_id | timestamp   |\n",
    "+-------------+-------------+-------------+\n",
    "| 1           | 1           | 0           |\n",
    "| 1           | 1           | 1           |\n",
    "| 1           | 1           | 2           |\n",
    "| 1           | 2           | 3           |\n",
    "| 1           | 2           | 4           |\n",
    "| 2           | 1           | 5           |\n",
    "| 2           | 1           | 6           |\n",
    "+-------------+-------------+-------------+\n",
    "Output: \n",
    "+-------------+-------------+\n",
    "| actor_id    | director_id |\n",
    "+-------------+-------------+\n",
    "| 1           | 1           |\n",
    "+-------------+-------------+\n",
    "Explanation: The only pair is (1, 1) where they cooperated exactly 3 times.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3491322a-a24e-4df7-b1b1-5532ce1e5612",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+---------+\n|actor_id|director_id|timestamp|\n+--------+-----------+---------+\n|       1|          1|        0|\n|       1|          1|        1|\n|       1|          1|        2|\n|       1|          2|        3|\n|       1|          2|        4|\n|       2|          1|        5|\n|       2|          1|        6|\n+--------+-----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"DataConversion\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(1, 1, 0), (1, 1, 1), (1, 1, 2), (1, 2, 3), (1, 2, 4), (2, 1, 5), (2, 1, 6)]\n",
    "columns = ['actor_id', 'director_id', 'timestamp']\n",
    "\n",
    "# Create PySpark DataFrame\n",
    "actor_director_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "actor_director_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cf54d99-de2d-4bf9-b85e-1ce2299c81c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n|actor_id|director_id|\n+--------+-----------+\n|       1|          1|\n+--------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to find all the pairs (actor_id, director_id) where the actor has cooperated with the director at least three times.\n",
    "\n",
    "# Return the result table in any order.\n",
    "\n",
    "actor_director_df.groupBy(col('actor_id'),col('director_id')).agg(count('*').alias('Cnt')).filter(col('Cnt')>=3).drop(col('Cnt')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de489138-d7e5-4acc-bd2b-ad11df95536b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium)1511. Customer Order Frequency\n",
    "### Level: Easy\n",
    "```\n",
    "Table: Customers\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| customer_id   | int     |\n",
    "| name          | varchar |\n",
    "| country       | varchar |\n",
    "+---------------+---------+\n",
    "customer_id is the column with unique values for this table.\n",
    "This table contains information about the customers in the company.\n",
    " \n",
    "\n",
    "Table: Product\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| product_id    | int     |\n",
    "| description   | varchar |\n",
    "| price         | int     |\n",
    "+---------------+---------+\n",
    "product_id is the column with unique values for this table.\n",
    "This table contains information on the products in the company.\n",
    "price is the product cost.\n",
    " \n",
    "\n",
    "Table: Orders\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| order_id      | int     |\n",
    "| customer_id   | int     |\n",
    "| product_id    | int     |\n",
    "| order_date    | date    |\n",
    "| quantity      | int     |\n",
    "+---------------+---------+\n",
    "order_id is the column with unique values for this table.\n",
    "This table contains information on customer orders.\n",
    "customer_id is the id of the customer who bought \"quantity\" products with id \"product_id\".\n",
    "Order_date is the date in format ('YYYY-MM-DD') when the order was shipped.\n",
    " \n",
    "\n",
    "Write a solution to report the customer_id and customer_name of customers who have spent at least $100 in each month of June and July 2020.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Customers table:\n",
    "+--------------+-----------+-------------+\n",
    "| customer_id  | name      | country     |\n",
    "+--------------+-----------+-------------+\n",
    "| 1            | Winston   | USA         |\n",
    "| 2            | Jonathan  | Peru        |\n",
    "| 3            | Moustafa  | Egypt       |\n",
    "+--------------+-----------+-------------+\n",
    "Product table:\n",
    "+--------------+-------------+-------------+\n",
    "| product_id   | description | price       |\n",
    "+--------------+-------------+-------------+\n",
    "| 10           | LC Phone    | 300         |\n",
    "| 20           | LC T-Shirt  | 10          |\n",
    "| 30           | LC Book     | 45          |\n",
    "| 40           | LC Keychain | 2           |\n",
    "+--------------+-------------+-------------+\n",
    "Orders table:\n",
    "+--------------+-------------+-------------+-------------+-----------+\n",
    "| order_id     | customer_id | product_id  | order_date  | quantity  |\n",
    "+--------------+-------------+-------------+-------------+-----------+\n",
    "| 1            | 1           | 10          | 2020-06-10  | 1         |\n",
    "| 2            | 1           | 20          | 2020-07-01  | 1         |\n",
    "| 3            | 1           | 30          | 2020-07-08  | 2         |\n",
    "| 4            | 2           | 10          | 2020-06-15  | 2         |\n",
    "| 5            | 2           | 40          | 2020-07-01  | 10        |\n",
    "| 6            | 3           | 20          | 2020-06-24  | 2         |\n",
    "| 7            | 3           | 30          | 2020-06-25  | 2         |\n",
    "| 9            | 3           | 30          | 2020-05-08  | 3         |\n",
    "+--------------+-------------+-------------+-------------+-----------+\n",
    "Output: \n",
    "+--------------+------------+\n",
    "| customer_id  | name       |  \n",
    "+--------------+------------+\n",
    "| 1            | Winston    |\n",
    "+--------------+------------+\n",
    "Explanation: \n",
    "Winston spent $300 (300 * 1) in June and $100 ( 10 * 1 + 45 * 2) in July 2020.\n",
    "Jonathan spent $600 (300 * 2) in June and $20 ( 2 * 10) in July 2020.\n",
    "Moustafa spent $110 (10 * 2 + 45 * 2) in June and $0 in July 2020.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ccc4bc4-2948-4b12-99e6-e17613c4fece",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+\n|customer_id|    name|country|\n+-----------+--------+-------+\n|          1| Winston|    USA|\n|          2|Jonathan|   Peru|\n|          3|Moustafa|  Egypt|\n+-----------+--------+-------+\n\n+----------+-----------+-----+\n|product_id|description|price|\n+----------+-----------+-----+\n|        10|   LC Phone|  300|\n|        20| LC T-Shirt|   10|\n|        30|    LC Book|   45|\n|        40|LC Keychain|    2|\n+----------+-----------+-----+\n\n+--------+-----------+----------+----------+--------+\n|order_id|customer_id|product_id|order_date|quantity|\n+--------+-----------+----------+----------+--------+\n|       1|          1|        10|2020-06-10|       1|\n|       2|          1|        20|2020-07-01|       1|\n|       3|          1|        30|2020-07-08|       2|\n|       4|          2|        10|2020-06-15|       2|\n|       5|          2|        40|2020-07-01|      10|\n|       6|          3|        20|2020-06-24|       2|\n|       7|          3|        30|2020-06-25|       2|\n|       9|          3|        30|2020-05-08|       3|\n+--------+-----------+----------+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define data\n",
    "data_customers = [(1, 'Winston', 'USA'), (2, 'Jonathan', 'Peru'), (3, 'Moustafa', 'Egypt')]\n",
    "data_product = [(10, 'LC Phone', 300), (20, 'LC T-Shirt', 10), (30, 'LC Book', 45), (40, 'LC Keychain', 2)]\n",
    "data_orders = [(1, 1, 10, '2020-06-10', 1), (2, 1, 20, '2020-07-01', 1), (3, 1, 30, '2020-07-08', 2), \n",
    "               (4, 2, 10, '2020-06-15', 2), (5, 2, 40, '2020-07-01', 10), (6, 3, 20, '2020-06-24', 2),\n",
    "               (7, 3, 30, '2020-06-25', 2), (9, 3, 30, '2020-05-08', 3)]\n",
    "\n",
    "# Create DataFrames\n",
    "customers_df = spark.createDataFrame(data_customers, schema=['customer_id', 'name', 'country'])\n",
    "product_df = spark.createDataFrame(data_product, schema=['product_id', 'description', 'price'])\n",
    "orders_df = spark.createDataFrame(data_orders, schema=['order_id', 'customer_id', 'product_id', 'order_date', 'quantity'])\n",
    "\n",
    "# Show DataFrames\n",
    "customers_df.show()\n",
    "product_df.show()\n",
    "orders_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6def7ece-c3db-4e29-b999-7f1289325435",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n|customer_id|   name|\n+-----------+-------+\n|          1|Winston|\n+-----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to report the customer_id and customer_name of customers who have spent at least $100 in each month of June and July 2020.\n",
    "from pyspark.sql.functions import *\n",
    "orders_df.alias('O').filter((col('O.order_date').like('2020-06%') | col('O.order_date').like('2020-07%')))\\\n",
    "    .join(product_df.alias('P'), col('O.product_id') == col('P.product_id'), 'left').join(customers_df.alias('C'), col('C.customer_id') == col('O.customer_id'))\\\n",
    "        .groupBy(col('O.customer_id'), col('C.name'),month(col('order_date'))).agg(sum(col('quantity') * col('price')).alias('total_price')).filter(col('total_price') >= 100)\\\n",
    "            .groupBy(col('customer_id'),col('C.name')).agg(count('*').alias('cnt')).filter(col('cnt')>1).drop(col('cnt')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8060c356-acf2-46c3-a359-796ad8d68a10",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1693. Daily Leads and Partners\n",
    "### Level: Easy\n",
    "```\n",
    "Table: DailySales\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| date_id     | date    |\n",
    "| make_name   | varchar |\n",
    "| lead_id     | int     |\n",
    "| partner_id  | int     |\n",
    "+-------------+---------+\n",
    "There is no primary key (column with unique values) for this table. It may contain duplicates.\n",
    "This table contains the date and the name of the product sold and the IDs of the lead and partner it was sold to.\n",
    "The name consists of only lowercase English letters.\n",
    " \n",
    "\n",
    "For each date_id and make_name, find the number of distinct lead_id's and distinct partner_id's.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "DailySales table:\n",
    "+-----------+-----------+---------+------------+\n",
    "| date_id   | make_name | lead_id | partner_id |\n",
    "+-----------+-----------+---------+------------+\n",
    "| 2020-12-8 | toyota    | 0       | 1          |\n",
    "| 2020-12-8 | toyota    | 1       | 0          |\n",
    "| 2020-12-8 | toyota    | 1       | 2          |\n",
    "| 2020-12-7 | toyota    | 0       | 2          |\n",
    "| 2020-12-7 | toyota    | 0       | 1          |\n",
    "| 2020-12-8 | honda     | 1       | 2          |\n",
    "| 2020-12-8 | honda     | 2       | 1          |\n",
    "| 2020-12-7 | honda     | 0       | 1          |\n",
    "| 2020-12-7 | honda     | 1       | 2          |\n",
    "| 2020-12-7 | honda     | 2       | 1          |\n",
    "+-----------+-----------+---------+------------+\n",
    "Output: \n",
    "+-----------+-----------+--------------+-----------------+\n",
    "| date_id   | make_name | unique_leads | unique_partners |\n",
    "+-----------+-----------+--------------+-----------------+\n",
    "| 2020-12-8 | toyota    | 2            | 3               |\n",
    "| 2020-12-7 | toyota    | 1            | 2               |\n",
    "| 2020-12-8 | honda     | 2            | 2               |\n",
    "| 2020-12-7 | honda     | 3            | 2               |\n",
    "+-----------+-----------+--------------+-----------------+\n",
    "Explanation: \n",
    "For 2020-12-8, toyota gets leads = [0, 1] and partners = [0, 1, 2] while honda gets leads = [1, 2] and partners = [1, 2].\n",
    "For 2020-12-7, toyota gets leads = [0] and partners = [1, 2] while honda gets leads = [0, 1, 2] and partners = [1, 2].\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5ba6419-31cf-4296-a053-003ded9aaa57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------+----------+\n|  date_id|make_name|lead_id|partner_id|\n+---------+---------+-------+----------+\n|2020-12-8|   toyota|      0|         1|\n|2020-12-8|   toyota|      1|         0|\n|2020-12-8|   toyota|      1|         2|\n|2020-12-7|   toyota|      0|         2|\n|2020-12-7|   toyota|      0|         1|\n|2020-12-8|    honda|      1|         2|\n|2020-12-8|    honda|      2|         1|\n|2020-12-7|    honda|      0|         1|\n|2020-12-7|    honda|      1|         2|\n|2020-12-7|    honda|      2|         1|\n+---------+---------+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"daily_sales\").getOrCreate()\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"date_id\", StringType(), True),\n",
    "    StructField(\"make_name\", StringType(), True),\n",
    "    StructField(\"lead_id\", IntegerType(), True),\n",
    "    StructField(\"partner_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    ['2020-12-8', 'toyota', 0, 1],\n",
    "    ['2020-12-8', 'toyota', 1, 0],\n",
    "    ['2020-12-8', 'toyota', 1, 2],\n",
    "    ['2020-12-7', 'toyota', 0, 2],\n",
    "    ['2020-12-7', 'toyota', 0, 1],\n",
    "    ['2020-12-8', 'honda', 1, 2],\n",
    "    ['2020-12-8', 'honda', 2, 1],\n",
    "    ['2020-12-7', 'honda', 0, 1],\n",
    "    ['2020-12-7', 'honda', 1, 2],\n",
    "    ['2020-12-7', 'honda', 2, 1]\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "daily_sales_df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show DataFrame\n",
    "daily_sales_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9996635-4edb-47c8-9ed5-2d6cbf013b6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------+---------------+\n|  date_id|make_name|unique_leads|unique_partners|\n+---------+---------+------------+---------------+\n|2020-12-8|   toyota|           2|              3|\n|2020-12-7|    honda|           3|              2|\n|2020-12-7|   toyota|           1|              2|\n|2020-12-8|    honda|           2|              2|\n+---------+---------+------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# For each date_id and make_name, find the number of distinct lead_id's and distinct partner_id's.\n",
    "\n",
    "# Return the result table in any order.\n",
    "\n",
    "\n",
    "daily_sales_df.groupBy(col('date_id'),col('make_name')).agg(countDistinct(col('lead_id')).alias('unique_leads'),countDistinct(col('partner_id')).alias('unique_partners'))\\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8caf291f-8ae3-46b4-963b-6eb20570cdf1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1495. Friendly Movies Streamed Last Month\n",
    "###Level: Easy\n",
    "\n",
    "```\n",
    "\n",
    "Table: TVProgram\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| program_date  | date    |\n",
    "| content_id    | int     |\n",
    "| channel       | varchar |\n",
    "+---------------+---------+\n",
    "(program_date, content_id) is the primary key (combination of columns with unique values) for this table.\n",
    "This table contains information of the programs on the TV.\n",
    "content_id is the id of the program in some channel on the TV.\n",
    " \n",
    "\n",
    "Table: Content\n",
    "\n",
    "+------------------+---------+\n",
    "| Column Name      | Type    |\n",
    "+------------------+---------+\n",
    "| content_id       | varchar |\n",
    "| title            | varchar |\n",
    "| Kids_content     | enum    |\n",
    "| content_type     | varchar |\n",
    "+------------------+---------+\n",
    "content_id is the primary key (column with unique values) for this table.\n",
    "Kids_content is an ENUM (category) of types ('Y', 'N') where: \n",
    "'Y' means is content for kids otherwise 'N' is not content for kids.\n",
    "content_type is the category of the content as movies, series, etc.\n",
    " \n",
    "\n",
    "Write a solution to report the distinct titles of the kid-friendly movies streamed in June 2020.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "TVProgram table:\n",
    "+--------------------+--------------+-------------+\n",
    "| program_date       | content_id   | channel     |\n",
    "+--------------------+--------------+-------------+\n",
    "| 2020-06-10 08:00   | 1            | LC-Channel  |\n",
    "| 2020-05-11 12:00   | 2            | LC-Channel  |\n",
    "| 2020-05-12 12:00   | 3            | LC-Channel  |\n",
    "| 2020-05-13 14:00   | 4            | Disney Ch   |\n",
    "| 2020-06-18 14:00   | 4            | Disney Ch   |\n",
    "| 2020-07-15 16:00   | 5            | Disney Ch   |\n",
    "+--------------------+--------------+-------------+\n",
    "Content table:\n",
    "+------------+----------------+---------------+---------------+\n",
    "| content_id | title          | Kids_content  | content_type  |\n",
    "+------------+----------------+---------------+---------------+\n",
    "| 1          | Leetcode Movie | N             | Movies        |\n",
    "| 2          | Alg. for Kids  | Y             | Series        |\n",
    "| 3          | Database Sols  | N             | Series        |\n",
    "| 4          | Aladdin        | Y             | Movies        |\n",
    "| 5          | Cinderella     | Y             | Movies        |\n",
    "+------------+----------------+---------------+---------------+\n",
    "Output: \n",
    "+--------------+\n",
    "| title        |\n",
    "+--------------+\n",
    "| Aladdin      |\n",
    "+--------------+\n",
    "Explanation: \n",
    "\"Leetcode Movie\" is not a content for kids.\n",
    "\"Alg. for Kids\" is not a movie.\n",
    "\"Database Sols\" is not a movie\n",
    "\"Alladin\" is a movie, content for kids and was streamed in June 2020.\n",
    "\"Cinderella\" was not streamed in June 2020.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a53689c-d37b-4264-9f1d-aee2cf1dab00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+----------+\n|    program_date|content_id|   channel|\n+----------------+----------+----------+\n|2020-06-10 08:00|         1|LC-Channel|\n|2020-05-11 12:00|         2|LC-Channel|\n|2020-05-12 12:00|         3|LC-Channel|\n|2020-05-13 14:00|         4| Disney Ch|\n|2020-06-18 14:00|         4| Disney Ch|\n|2020-07-15 16:00|         5| Disney Ch|\n+----------------+----------+----------+\n\n+----------+--------------+------------+------------+\n|content_id|         title|Kids_content|content_type|\n+----------+--------------+------------+------------+\n|         1|Leetcode Movie|           N|      Movies|\n|         2| Alg. for Kids|           Y|      Series|\n|         3| Database Sols|           N|      Series|\n|         4|       Aladdin|           Y|      Movies|\n|         5|    Cinderella|           Y|      Movies|\n+----------+--------------+------------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"tv_program_content\").getOrCreate()\n",
    "\n",
    "# Define schema for tv_program\n",
    "tv_program_schema = StructType([\n",
    "    StructField(\"program_date\", StringType(), True),\n",
    "    StructField(\"content_id\", IntegerType(), True),\n",
    "    StructField(\"channel\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Data for tv_program\n",
    "tv_program_data = [\n",
    "    ['2020-06-10 08:00', 1, 'LC-Channel'],\n",
    "    ['2020-05-11 12:00', 2, 'LC-Channel'],\n",
    "    ['2020-05-12 12:00', 3, 'LC-Channel'],\n",
    "    ['2020-05-13 14:00', 4, 'Disney Ch'],\n",
    "    ['2020-06-18 14:00', 4, 'Disney Ch'],\n",
    "    ['2020-07-15 16:00', 5, 'Disney Ch']\n",
    "]\n",
    "\n",
    "# Create DataFrame for tv_program\n",
    "tv_program_df = spark.createDataFrame(tv_program_data, schema=tv_program_schema)\n",
    "\n",
    "# Define schema for content\n",
    "content_schema = StructType([\n",
    "    StructField(\"content_id\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"Kids_content\", StringType(), True),\n",
    "    StructField(\"content_type\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Data for content\n",
    "content_data = [\n",
    "    [1, 'Leetcode Movie', 'N', 'Movies'],\n",
    "    [2, 'Alg. for Kids', 'Y', 'Series'],\n",
    "    [3, 'Database Sols', 'N', 'Series'],\n",
    "    [4, 'Aladdin', 'Y', 'Movies'],\n",
    "    [5, 'Cinderella', 'Y', 'Movies']\n",
    "]\n",
    "\n",
    "# Create DataFrame for content\n",
    "content_df = spark.createDataFrame(content_data, schema=content_schema)\n",
    "\n",
    "# Show DataFrames\n",
    "tv_program_df.show()\n",
    "content_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4b88bfa-ca88-4eae-a91b-5068d88a4335",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n|  title|\n+-------+\n|Aladdin|\n+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "tv_program_df.alias('T').join(content_df.alias('P'), col('T.content_id') == col('P.content_id'),'left')\\\n",
    "    .filter((col('program_date').like('2020-06%')) & (col('kids_content') == 'Y' )).select(col('title')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "024ddd86-fcdc-4f87-8519-95966cc5046e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1501. Countries You Can Safely Invest In\n",
    "### Level: Medium\n",
    "```\n",
    "Table Person:\n",
    "\n",
    "+----------------+---------+\n",
    "| Column Name    | Type    |\n",
    "+----------------+---------+\n",
    "| id             | int     |\n",
    "| name           | varchar |\n",
    "| phone_number   | varchar |\n",
    "+----------------+---------+\n",
    "id is the column of unique values for this table.\n",
    "Each row of this table contains the name of a person and their phone number.\n",
    "Phone number will be in the form 'xxx-yyyyyyy' where xxx is the country code (3 characters) and yyyyyyy is the phone number (7 characters) where x and y are digits. Both can contain leading zeros.\n",
    " \n",
    "\n",
    "Table Country:\n",
    "\n",
    "+----------------+---------+\n",
    "| Column Name    | Type    |\n",
    "+----------------+---------+\n",
    "| name           | varchar |\n",
    "| country_code   | varchar |\n",
    "+----------------+---------+\n",
    "country_code is the column of unique values for this table.\n",
    "Each row of this table contains the country name and its code. country_code will be in the form 'xxx' where x is digits.\n",
    " \n",
    "\n",
    "Table Calls:\n",
    "\n",
    "+-------------+------+\n",
    "| Column Name | Type |\n",
    "+-------------+------+\n",
    "| caller_id   | int  |\n",
    "| callee_id   | int  |\n",
    "| duration    | int  |\n",
    "+-------------+------+\n",
    "This table may contain duplicate rows.\n",
    "Each row of this table contains the caller id, callee id and the duration of the call in minutes. caller_id != callee_id\n",
    " \n",
    "\n",
    "A telecommunications company wants to invest in new countries. The company intends to invest in the countries where the average call duration of the calls in this country is strictly greater than the global average call duration.\n",
    "\n",
    "Write a solution to find the countries where this company can invest.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Person table:\n",
    "+----+----------+--------------+\n",
    "| id | name     | phone_number |\n",
    "+----+----------+--------------+\n",
    "| 3  | Jonathan | 051-1234567  |\n",
    "| 12 | Elvis    | 051-7654321  |\n",
    "| 1  | Moncef   | 212-1234567  |\n",
    "| 2  | Maroua   | 212-6523651  |\n",
    "| 7  | Meir     | 972-1234567  |\n",
    "| 9  | Rachel   | 972-0011100  |\n",
    "+----+----------+--------------+\n",
    "Country table:\n",
    "+----------+--------------+\n",
    "| name     | country_code |\n",
    "+----------+--------------+\n",
    "| Peru     | 051          |\n",
    "| Israel   | 972          |\n",
    "| Morocco  | 212          |\n",
    "| Germany  | 049          |\n",
    "| Ethiopia | 251          |\n",
    "+----------+--------------+\n",
    "Calls table:\n",
    "+-----------+-----------+----------+\n",
    "| caller_id | callee_id | duration |\n",
    "+-----------+-----------+----------+\n",
    "| 1         | 9         | 33       |\n",
    "| 2         | 9         | 4        |\n",
    "| 1         | 2         | 59       |\n",
    "| 3         | 12        | 102      |\n",
    "| 3         | 12        | 330      |\n",
    "| 12        | 3         | 5        |\n",
    "| 7         | 9         | 13       |\n",
    "| 7         | 1         | 3        |\n",
    "| 9         | 7         | 1        |\n",
    "| 1         | 7         | 7        |\n",
    "+-----------+-----------+----------+\n",
    "Output: \n",
    "+----------+\n",
    "| country  |\n",
    "+----------+\n",
    "| Peru     |\n",
    "+----------+\n",
    "Explanation: \n",
    "The average call duration for Peru is (102 + 102 + 330 + 330 + 5 + 5) / 6 = 145.666667\n",
    "The average call duration for Israel is (33 + 4 + 13 + 13 + 3 + 1 + 1 + 7) / 8 = 9.37500\n",
    "The average call duration for Morocco is (33 + 4 + 59 + 59 + 3 + 7) / 6 = 27.5000 \n",
    "Global call duration average = (2 * (33 + 4 + 59 + 102 + 330 + 5 + 13 + 3 + 1 + 7)) / 20 = 55.70000\n",
    "Since Peru is the only country where the average call duration is greater than the global average, it is the only recommended country.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9add414f-9e38-4f87-a1bf-914152366146",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------------+\n| id|    name|phone_number|\n+---+--------+------------+\n|  3|Jonathan| 051-1234567|\n| 12|   Elvis| 051-7654321|\n|  1|  Moncef| 212-1234567|\n|  2|  Maroua| 212-6523651|\n|  7|    Meir| 972-1234567|\n|  9|  Rachel| 972-0011100|\n+---+--------+------------+\n\n+--------+------------+\n|    name|country_code|\n+--------+------------+\n|    Peru|         051|\n|  Israel|         972|\n| Morocco|         212|\n| Germany|         049|\n|Ethiopia|         251|\n+--------+------------+\n\n+---------+---------+--------+\n|caller_id|callee_id|duration|\n+---------+---------+--------+\n|        1|        9|      33|\n|        2|        9|       4|\n|        1|        2|      59|\n|        3|       12|     102|\n|        3|       12|     330|\n|       12|        3|       5|\n|        7|        9|      13|\n|        7|        1|       3|\n|        9|        7|       1|\n|        1|        7|       7|\n+---------+---------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"person_country_calls\").getOrCreate()\n",
    "\n",
    "# Define schema for person\n",
    "person_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"phone_number\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Data for person\n",
    "person_data = [\n",
    "    [3, 'Jonathan', '051-1234567'],\n",
    "    [12, 'Elvis', '051-7654321'],\n",
    "    [1, 'Moncef', '212-1234567'],\n",
    "    [2, 'Maroua', '212-6523651'],\n",
    "    [7, 'Meir', '972-1234567'],\n",
    "    [9, 'Rachel', '972-0011100']\n",
    "]\n",
    "\n",
    "# Create DataFrame for person\n",
    "person_df = spark.createDataFrame(person_data, schema=person_schema)\n",
    "\n",
    "# Define schema for country\n",
    "country_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"country_code\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Data for country\n",
    "country_data = [\n",
    "    ['Peru', '051'],\n",
    "    ['Israel', '972'],\n",
    "    ['Morocco', '212'],\n",
    "    ['Germany', '049'],\n",
    "    ['Ethiopia', '251']\n",
    "]\n",
    "\n",
    "# Create DataFrame for country\n",
    "country_df = spark.createDataFrame(country_data, schema=country_schema)\n",
    "\n",
    "# Define schema for calls\n",
    "calls_schema = StructType([\n",
    "    StructField(\"caller_id\", IntegerType(), True),\n",
    "    StructField(\"callee_id\", IntegerType(), True),\n",
    "    StructField(\"duration\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data for calls\n",
    "calls_data = [\n",
    "    [1, 9, 33],\n",
    "    [2, 9, 4],\n",
    "    [1, 2, 59],\n",
    "    [3, 12, 102],\n",
    "    [3, 12, 330],\n",
    "    [12, 3, 5],\n",
    "    [7, 9, 13],\n",
    "    [7, 1, 3],\n",
    "    [9, 7, 1],\n",
    "    [1, 7, 7]\n",
    "]\n",
    "\n",
    "# Create DataFrame for calls\n",
    "calls_df = spark.createDataFrame(calls_data, schema=calls_schema)\n",
    "\n",
    "# Show DataFrames\n",
    "person_df.show()\n",
    "country_df.show()\n",
    "calls_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27fad39c-d909-416a-bbfa-6f424b1c32ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# A telecommunications company wants to invest in new countries. The company intends to invest in the countries where the average call duration of the calls in this country is strictly greater than the global average call duration.\n",
    "\n",
    "# Write a solution to find the countries where this company can invest.\n",
    "\n",
    "# Return the result table in any order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60fe501c-bef3-4769-adab-429609a1e2d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Advanced Select and Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eee6752b-d2b5-47fd-a0ec-0d68126d01be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 603. Consecutive Available Seats\n",
    "```\n",
    "Table: Cinema\n",
    "\n",
    "+-------------+------+\n",
    "| Column Name | Type |\n",
    "+-------------+------+\n",
    "| seat_id     | int  |\n",
    "| free        | bool |\n",
    "+-------------+------+\n",
    "seat_id is an auto-increment column for this table.\n",
    "Each row of this table indicates whether the ith seat is free or not. 1 means free while 0 means occupied.\n",
    " \n",
    "\n",
    "Find all the consecutive available seats in the cinema.\n",
    "\n",
    "Return the result table ordered by seat_id in ascending order.\n",
    "\n",
    "The test cases are generated so that more than two seats are consecutively available.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Cinema table:\n",
    "+---------+------+\n",
    "| seat_id | free |\n",
    "+---------+------+\n",
    "| 1       | 1    |\n",
    "| 2       | 0    |\n",
    "| 3       | 1    |\n",
    "| 4       | 1    |\n",
    "| 5       | 1    |\n",
    "+---------+------+\n",
    "Output: \n",
    "+---------+\n",
    "| seat_id |\n",
    "+---------+\n",
    "| 3       |\n",
    "| 4       |\n",
    "| 5       |\n",
    "+---------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "705eee37-7c36-408b-b460-1d24f2cdd3e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n|seat_id|free|\n+-------+----+\n|      1|   1|\n|      2|   0|\n|      3|   1|\n|      4|   1|\n|      5|   1|\n+-------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"cinema\").getOrCreate()\n",
    "\n",
    "# Define schema for cinema\n",
    "cinema_schema = StructType([\n",
    "    StructField(\"seat_id\", IntegerType(), True),\n",
    "    StructField(\"free\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data for cinema\n",
    "cinema_data = [\n",
    "    [1, 1],\n",
    "    [2, 0],\n",
    "    [3, 1],\n",
    "    [4, 1],\n",
    "    [5, 1]\n",
    "]\n",
    "\n",
    "# Create DataFrame for cinema\n",
    "cinema_df = spark.createDataFrame(cinema_data, schema=cinema_schema)\n",
    "\n",
    "# Show DataFrame\n",
    "cinema_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da76e20-3721-4959-acef-1d74faf6fe9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "468e5dc8-a203-4c7e-b613-971bc287560a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1795. Rearrange Products Table\n",
    "```\n",
    "Table: Products\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| product_id  | int     |\n",
    "| store1      | int     |\n",
    "| store2      | int     |\n",
    "| store3      | int     |\n",
    "+-------------+---------+\n",
    "product_id is the primary key (column with unique values) for this table.\n",
    "Each row in this table indicates the product's price in 3 different stores: store1, store2, and store3.\n",
    "If the product is not available in a store, the price will be null in that store's column.\n",
    " \n",
    "\n",
    "Write a solution to rearrange the Products table so that each row has (product_id, store, price). If a product is not available in a store, do not include a row with that product_id and store combination in the result table.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Products table:\n",
    "+------------+--------+--------+--------+\n",
    "| product_id | store1 | store2 | store3 |\n",
    "+------------+--------+--------+--------+\n",
    "| 0          | 95     | 100    | 105    |\n",
    "| 1          | 70     | null   | 80     |\n",
    "+------------+--------+--------+--------+\n",
    "Output: \n",
    "+------------+--------+-------+\n",
    "| product_id | store  | price |\n",
    "+------------+--------+-------+\n",
    "| 0          | store1 | 95    |\n",
    "| 0          | store2 | 100   |\n",
    "| 0          | store3 | 105   |\n",
    "| 1          | store1 | 70    |\n",
    "| 1          | store3 | 80    |\n",
    "+------------+--------+-------+\n",
    "Explanation: \n",
    "Product 0 is available in all three stores with prices 95, 100, and 105 respectively.\n",
    "Product 1 is available in store1 with price 70 and store3 with price 80. The product is not available in store2.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdf9fcde-7344-4ad4-9483-8aa5b5aff426",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------+------+\n|product_id|store1|store2|store3|\n+----------+------+------+------+\n|         0|    95|   100|   105|\n|         1|    70|  null|    80|\n+----------+------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "products_schema = StructType([\n",
    "    StructField('product_id', IntegerType(), True),\n",
    "    StructField('store1', IntegerType(), True),\n",
    "    StructField('store2', IntegerType(), True),\n",
    "    StructField('store3', IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sample data for 'products'\n",
    "products_data = [\n",
    "    (0, 95, 100, 105),\n",
    "    (1, 70, None, 80)  # None represents null values\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "products_df = spark.createDataFrame(products_data, schema=products_schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "products_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca52a070-3e9f-4a48-bc71-43f3ec2b7440",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n|product_id| store|price|\n+----------+------+-----+\n|         0|store1|   95|\n|         0|store2|  100|\n|         0|store3|  105|\n|         1|store1|   70|\n|         1|store3|   80|\n+----------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to rearrange the Products table so that each row has (product_id, store, price). If a product is not available in a store, do not include a row with that product_id and store combination in the result table.\n",
    "\n",
    "# Return the result table in any order.\n",
    "#1. This need to be unpivoted\n",
    "from pyspark.sql.functions import expr\n",
    "unpivotExpr = \"stack(3,'store1', store1, 'store2', store2, 'store3', store3) as (store, price)\"\n",
    "unpivotDf = products_df.select(col('product_id'),expr(unpivotExpr)).where(\"price is not null\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f00ba661-37fb-4def-b479-0c6c2ef5c7d8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 613. Shortest Distance in a Line\n",
    "```\n",
    "Table: Point\n",
    "\n",
    "+-------------+------+\n",
    "| Column Name | Type |\n",
    "+-------------+------+\n",
    "| x           | int  |\n",
    "+-------------+------+\n",
    "In SQL, x is the primary key column for this table.\n",
    "Each row of this table indicates the position of a point on the X-axis.\n",
    " \n",
    "\n",
    "Find the shortest distance between any two points from the Point table.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Point table:\n",
    "+----+\n",
    "| x  |\n",
    "+----+\n",
    "| -1 |\n",
    "| 0  |\n",
    "| 2  |\n",
    "+----+\n",
    "Output: \n",
    "+----------+\n",
    "| shortest |\n",
    "+----------+\n",
    "| 1        |\n",
    "+----------+\n",
    "Explanation: The shortest distance is between points -1 and 0 which is |(-1) - 0| = 1.\n",
    " \n",
    "\n",
    "Follow up: How could you optimize your solution if the Point table is ordered in ascending order?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "704087f6-0f6b-46a4-aed6-f7b6da7ebe8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n|  x|\n+---+\n| -1|\n|  0|\n|  2|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Sample data\n",
    "data = [[-1], [0], [2]]\n",
    "columns = ['x']\n",
    "\n",
    "# Create PySpark DataFrame\n",
    "point_df = spark.createDataFrame(data, columns).withColumn(\"x\", col(\"x\").cast(\"int\"))\n",
    "\n",
    "# Show the DataFrame\n",
    "point_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4df62b66-b8f8-4fb9-a28b-4c8a64177442",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|shortest|\n+--------+\n|       1|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "windowOptions = Window.orderBy(col('x'))\n",
    "point_df.withColumn('shortest',lag(col('x')).over(windowOptions)).select(min(col('x') - col('shortest')).alias('shortest')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62bc1191-7553-4142-8536-967abb42297d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1965. Employees With Missing Information\n",
    "```\n",
    "Table: Employees\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| employee_id | int     |\n",
    "| name        | varchar |\n",
    "+-------------+---------+\n",
    "employee_id is the column with unique values for this table.\n",
    "Each row of this table indicates the name of the employee whose ID is employee_id.\n",
    " \n",
    "\n",
    "Table: Salaries\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| employee_id | int     |\n",
    "| salary      | int     |\n",
    "+-------------+---------+\n",
    "employee_id is the column with unique values for this table.\n",
    "Each row of this table indicates the salary of the employee whose ID is employee_id.\n",
    " \n",
    "\n",
    "Write a solution to report the IDs of all the employees with missing information. The information of an employee is missing if:\n",
    "\n",
    "The employee's name is missing, or\n",
    "The employee's salary is missing.\n",
    "Return the result table ordered by employee_id in ascending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Employees table:\n",
    "+-------------+----------+\n",
    "| employee_id | name     |\n",
    "+-------------+----------+\n",
    "| 2           | Crew     |\n",
    "| 4           | Haven    |\n",
    "| 5           | Kristian |\n",
    "+-------------+----------+\n",
    "Salaries table:\n",
    "+-------------+--------+\n",
    "| employee_id | salary |\n",
    "+-------------+--------+\n",
    "| 5           | 76071  |\n",
    "| 1           | 22517  |\n",
    "| 4           | 63539  |\n",
    "+-------------+--------+\n",
    "Output: \n",
    "+-------------+\n",
    "| employee_id |\n",
    "+-------------+\n",
    "| 1           |\n",
    "| 2           |\n",
    "+-------------+\n",
    "Explanation: \n",
    "Employees 1, 2, 4, and 5 are working at this company.\n",
    "The name of employee 1 is missing.\n",
    "The salary of employee 2 is missing.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11bcad45-221c-4cc2-ae07-96b918e1f239",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n|employee_id|    name|\n+-----------+--------+\n|          2|    Crew|\n|          4|   Haven|\n|          5|Kristian|\n+-----------+--------+\n\n+-----------+------+\n|employee_id|salary|\n+-----------+------+\n|          5| 76071|\n|          1| 22517|\n|          4| 63539|\n+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Sample data for employees\n",
    "employees_data = [[2, 'Crew'], [4, 'Haven'], [5, 'Kristian']]\n",
    "employees_columns = ['employee_id', 'name']\n",
    "\n",
    "# Sample data for salaries\n",
    "salaries_data = [[5, 76071], [1, 22517], [4, 63539]]\n",
    "salaries_columns = ['employee_id', 'salary']\n",
    "\n",
    "# Create PySpark DataFrames\n",
    "employees_df = spark.createDataFrame(employees_data, employees_columns).withColumn(\"employee_id\", col(\"employee_id\").cast(\"int\"))\n",
    "salaries_df = spark.createDataFrame(salaries_data, salaries_columns).withColumn(\"employee_id\", col(\"employee_id\").cast(\"int\"))\n",
    "\n",
    "# Show the DataFrames\n",
    "employees_df.show()\n",
    "salaries_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "309aaf65-c8bd-4cae-9a21-18410ca3c9c7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|employee_id|\n+-----------+\n|          1|\n|          2|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to report the IDs of all the employees with missing information. The information of an employee is missing if:\n",
    "\n",
    "# The employee's name is missing, or\n",
    "# The employee's salary is missing.\n",
    "# Return the result table ordered by employee_id in ascending order.\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "df1 = employees_df.alias('E').join(salaries_df.alias('S'), col('E.employee_id') == col('S.employee_id'),'full')\\\n",
    "        .filter((col('E.name').isNull() | (col('S.salary').isNull()))).select(col('E.employee_id'))\n",
    "\n",
    "df2 = employees_df.alias('E').join(salaries_df.alias('S'), col('E.employee_id') == col('S.employee_id'),'full')\\\n",
    "        .filter((col('E.name').isNull() | (col('S.salary').isNull()))).select(col('S.employee_id'))\n",
    "df1.union(df2).filter(~col('employee_id').isNull()).distinct().orderBy(col('employee_id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f08d81c1-cd35-4a58-9280-0b020e6fdadb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1264. Page Recommendations\n",
    "```\n",
    "Table: Friendship\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| user1_id      | int     |\n",
    "| user2_id      | int     |\n",
    "+---------------+---------+\n",
    "(user1_id, user2_id) is the primary key (combination of columns with unique values) for this table.\n",
    "Each row of this table indicates that there is a friendship relation between user1_id and user2_id.\n",
    " \n",
    "\n",
    "Table: Likes\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| user_id     | int     |\n",
    "| page_id     | int     |\n",
    "+-------------+---------+\n",
    "(user_id, page_id) is the primary key (combination of columns with unique values) for this table.\n",
    "Each row of this table indicates that user_id likes page_id.\n",
    " \n",
    "\n",
    "Write a solution to recommend pages to the user with user_id = 1 using the pages that your friends liked. It should not recommend pages you already liked.\n",
    "\n",
    "Return result table in any order without duplicates.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Friendship table:\n",
    "+----------+----------+\n",
    "| user1_id | user2_id |\n",
    "+----------+----------+\n",
    "| 1        | 2        |\n",
    "| 1        | 3        |\n",
    "| 1        | 4        |\n",
    "| 2        | 3        |\n",
    "| 2        | 4        |\n",
    "| 2        | 5        |\n",
    "| 6        | 1        |\n",
    "+----------+----------+\n",
    "Likes table:\n",
    "+---------+---------+\n",
    "| user_id | page_id |\n",
    "+---------+---------+\n",
    "| 1       | 88      |\n",
    "| 2       | 23      |\n",
    "| 3       | 24      |\n",
    "| 4       | 56      |\n",
    "| 5       | 11      |\n",
    "| 6       | 33      |\n",
    "| 2       | 77      |\n",
    "| 3       | 77      |\n",
    "| 6       | 88      |\n",
    "+---------+---------+\n",
    "Output: \n",
    "+------------------+\n",
    "| recommended_page |\n",
    "+------------------+\n",
    "| 23               |\n",
    "| 24               |\n",
    "| 56               |\n",
    "| 33               |\n",
    "| 77               |\n",
    "+------------------+\n",
    "Explanation: \n",
    "User one is friend with users 2, 3, 4 and 6.\n",
    "Suggested pages are 23 from user 2, 24 from user 3, 56 from user 3 and 33 from user 6.\n",
    "Page 77 is suggested from both user 2 and user 3.\n",
    "Page 88 is not suggested because user 1 already likes it.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d6c0588-f389-48db-a93c-d7e0b5c3153e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n|user1_id|user2_id|\n+--------+--------+\n|       1|       2|\n|       1|       3|\n|       1|       4|\n|       2|       3|\n|       2|       4|\n|       2|       5|\n|       6|       1|\n+--------+--------+\n\n+-------+-------+\n|user_id|page_id|\n+-------+-------+\n|      1|     88|\n|      2|     23|\n|      3|     24|\n|      4|     56|\n|      5|     11|\n|      6|     33|\n|      2|     77|\n|      3|     77|\n|      6|     88|\n+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "# Assuming the Spark session is already created as `spark`\n",
    "\n",
    "# Creating the schema for the friendship DataFrame\n",
    "friendship_schema = StructType([\n",
    "    StructField(\"user1_id\", IntegerType(), True),\n",
    "    StructField(\"user2_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Creating the friendship DataFrame\n",
    "friendship_data = [\n",
    "    (1, 2), (1, 3), (1, 4),\n",
    "    (2, 3), (2, 4), (2, 5),\n",
    "    (6, 1)\n",
    "]\n",
    "friendship_df = spark.createDataFrame(friendship_data, schema=friendship_schema)\n",
    "friendship_df.show()\n",
    "\n",
    "# Creating the schema for the likes DataFrame\n",
    "likes_schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"page_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Creating the likes DataFrame\n",
    "likes_data = [\n",
    "    (1, 88), (2, 23), (3, 24),\n",
    "    (4, 56), (5, 11), (6, 33),\n",
    "    (2, 77), (3, 77), (6, 88)\n",
    "]\n",
    "likes_df = spark.createDataFrame(likes_data, schema=likes_schema)\n",
    "likes_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ca03a36-8437-410b-b746-e46ebfa79155",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n|page_id|\n+-------+\n|     23|\n|     77|\n|     24|\n|     56|\n|     88|\n|     33|\n+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "cte_df = friendship_df.select(col(\"user1_id\"), col(\"user2_id\")) \\\n",
    "    .unionAll(friendship_df.select(col(\"user2_id\").alias(\"user1_id\"), col(\"user1_id\").alias(\"user2_id\")))\n",
    "\n",
    "result_df = cte_df.join(likes_df, cte_df.user2_id == likes_df.user_id, how='left') \\\n",
    "    .select(cte_df.user2_id, likes_df.user_id, likes_df.page_id) \\\n",
    "    .where(cte_df.user1_id == 1).select(col('page_id'))\n",
    "\n",
    "result_df.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "277b1e14-adc6-4404-93eb-793b7f427fcb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 608. Tree Node\n",
    "### Level: Medium\n",
    "```\n",
    "Table: Tree\n",
    "\n",
    "+-------------+------+\n",
    "| Column Name | Type |\n",
    "+-------------+------+\n",
    "| id          | int  |\n",
    "| p_id        | int  |\n",
    "+-------------+------+\n",
    "id is the column with unique values for this table.\n",
    "Each row of this table contains information about the id of a node and the id of its parent node in a tree.\n",
    "The given structure is always a valid tree.\n",
    " \n",
    "\n",
    "Each node in the tree can be one of three types:\n",
    "\n",
    "\"Leaf\": if the node is a leaf node.\n",
    "\"Root\": if the node is the root of the tree.\n",
    "\"Inner\": If the node is neither a leaf node nor a root node.\n",
    "Write a solution to report the type of each node in the tree.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c4dde6a-5898-45a8-b772-ef2cb3aef74b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n| id|p_id|\n+---+----+\n|  1|null|\n|  2|   1|\n|  3|   1|\n|  4|   2|\n|  5|   2|\n+---+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"tree\").getOrCreate()\n",
    "\n",
    "# Define schema for tree\n",
    "tree_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"p_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data for tree\n",
    "tree_data = [\n",
    "    [1, None],\n",
    "    [2, 1],\n",
    "    [3, 1],\n",
    "    [4, 2],\n",
    "    [5, 2]\n",
    "]\n",
    "\n",
    "# Create DataFrame for tree\n",
    "tree_df = spark.createDataFrame(tree_data, schema=tree_schema)\n",
    "\n",
    "# Show DataFrame\n",
    "tree_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55d9be0a-b376-4bd7-bcb9-bdba79534f5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n| id| Type|\n+---+-----+\n|  1| Root|\n|  2|Inner|\n|  3| Leaf|\n|  4| Leaf|\n|  5| Leaf|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "distinct_p_ids = [row['p_id'] for row in tree_df.select(col('p_id')).distinct().collect()]\n",
    "tree_df.withColumn('Type', when(col('p_id').isNull(),'Root')\\\n",
    "                .when(col('id').isin(distinct_p_ids) & (col('p_id').isNotNull()) ,'Inner')\\\n",
    "                .otherwise('Leaf')).drop(col('P_id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a54b8e0-38a8-4288-9530-bafba4248cbe",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 534. Game Play Analysis III\n",
    "### Level: Medium\n",
    "```\n",
    "Table: Activity\n",
    "\n",
    "+--------------+---------+\n",
    "| Column Name  | Type    |\n",
    "+--------------+---------+\n",
    "| player_id    | int     |\n",
    "| device_id    | int     |\n",
    "| event_date   | date    |\n",
    "| games_played | int     |\n",
    "+--------------+---------+\n",
    "(player_id, event_date) is the primary key (column with unique values) of this table.\n",
    "This table shows the activity of players of some games.\n",
    "Each row is a record of a player who logged in and played a number of games (possibly 0) before logging out on someday using some device.\n",
    " \n",
    "\n",
    "Write a solution to report for each player and date, how many games played so far by the player. That is, the total number of games played by the player until that date. Check the example for clarity.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Activity table:\n",
    "+-----------+-----------+------------+--------------+\n",
    "| player_id | device_id | event_date | games_played |\n",
    "+-----------+-----------+------------+--------------+\n",
    "| 1         | 2         | 2016-03-01 | 5            |\n",
    "| 1         | 2         | 2016-05-02 | 6            |\n",
    "| 1         | 3         | 2017-06-25 | 1            |\n",
    "| 3         | 1         | 2016-03-02 | 0            |\n",
    "| 3         | 4         | 2018-07-03 | 5            |\n",
    "+-----------+-----------+------------+--------------+\n",
    "Output: \n",
    "+-----------+------------+---------------------+\n",
    "| player_id | event_date | games_played_so_far |\n",
    "+-----------+------------+---------------------+\n",
    "| 1         | 2016-03-01 | 5                   |\n",
    "| 1         | 2016-05-02 | 11                  |\n",
    "| 1         | 2017-06-25 | 12                  |\n",
    "| 3         | 2016-03-02 | 0                   |\n",
    "| 3         | 2018-07-03 | 5                   |\n",
    "+-----------+------------+---------------------+\n",
    "Explanation: \n",
    "For the player with id 1, 5 + 6 = 11 games played by 2016-05-02, and 5 + 6 + 1 = 12 games played by 2017-06-25.\n",
    "For the player with id 3, 0 + 5 = 5 games played by 2018-07-03.\n",
    "Note that for each player we only care about the days when the player logged in.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8147b258-d35d-473a-b899-688cdd72a2bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+------------+\n|player_id|device_id|event_date|games_played|\n+---------+---------+----------+------------+\n|        1|        2|2016-03-01|           5|\n|        1|        2|2016-05-02|           6|\n|        1|        3|2017-06-25|           1|\n|        3|        1|2016-03-02|           0|\n|        3|        4|2018-07-03|           5|\n+---------+---------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType\n",
    "\n",
    "# Assuming you have an active Spark session\n",
    "# spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"player_id\", IntegerType(), True),\n",
    "    StructField(\"device_id\", IntegerType(), True),\n",
    "    StructField(\"event_date\", StringType(), True),\n",
    "    StructField(\"games_played\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    (1, 2, '2016-03-01', 5),\n",
    "    (1, 2, '2016-05-02', 6),\n",
    "    (1, 3, '2017-06-25', 1),\n",
    "    (3, 1, '2016-03-02', 0),\n",
    "    (3, 4, '2018-07-03', 5)\n",
    "]\n",
    "\n",
    "# Create the DataFrame\n",
    "activity_df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "activity_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a10fcbfc-95df-4be4-8943-f91b3675e2cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+------------+-------------------+\n|player_id|device_id|event_date|games_played|games_played_so_far|\n+---------+---------+----------+------------+-------------------+\n|        1|        2|2016-03-01|           5|                  5|\n|        1|        2|2016-05-02|           6|                 11|\n|        1|        3|2017-06-25|           1|                 12|\n|        3|        1|2016-03-02|           0|                  0|\n|        3|        4|2018-07-03|           5|                  5|\n+---------+---------+----------+------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Write a solution to report for each player and date, how many games played so far by the player.\n",
    "#That is, the total number of games played by the player until that date. Check the example for clarity.\n",
    "\n",
    "#Return the result table in any order.\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "windowOptions = Window.partitionBy(col('player_id')).orderBy(col('event_date'))\n",
    "activity_df.withColumn('games_played_so_far', sum(col('games_played')).over(windowOptions)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c05fcd0-6d58-419a-9b2f-a1adccb2728f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1783. Grand Slam Titles\n",
    "### Level: Medium\n",
    "\n",
    "```\n",
    "Table: Players\n",
    "\n",
    "+----------------+---------+\n",
    "| Column Name    | Type    |\n",
    "+----------------+---------+\n",
    "| player_id      | int     |\n",
    "| player_name    | varchar |\n",
    "+----------------+---------+\n",
    "player_id is the primary key (column with unique values) for this table.\n",
    "Each row in this table contains the name and the ID of a tennis player.\n",
    " \n",
    "\n",
    "Table: Championships\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| year          | int     |\n",
    "| Wimbledon     | int     |\n",
    "| Fr_open       | int     |\n",
    "| US_open       | int     |\n",
    "| Au_open       | int     |\n",
    "+---------------+---------+\n",
    "year is the primary key (column with unique values) for this table.\n",
    "Each row of this table contains the IDs of the players who won one each tennis tournament of the grand slam.\n",
    " \n",
    "\n",
    "Write a solution to report the number of grand slam tournaments won by each player. Do not include the players who did not win any tournament.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Players table:\n",
    "+-----------+-------------+\n",
    "| player_id | player_name |\n",
    "+-----------+-------------+\n",
    "| 1         | Nadal       |\n",
    "| 2         | Federer     |\n",
    "| 3         | Novak       |\n",
    "+-----------+-------------+\n",
    "Championships table:\n",
    "+------+-----------+---------+---------+---------+\n",
    "| year | Wimbledon | Fr_open | US_open | Au_open |\n",
    "+------+-----------+---------+---------+---------+\n",
    "| 2018 | 1         | 1       | 1       | 1       |\n",
    "| 2019 | 1         | 1       | 2       | 2       |\n",
    "| 2020 | 2         | 1       | 2       | 2       |\n",
    "+------+-----------+---------+---------+---------+\n",
    "Output: \n",
    "+-----------+-------------+-------------------+\n",
    "| player_id | player_name | grand_slams_count |\n",
    "+-----------+-------------+-------------------+\n",
    "| 2         | Federer     | 5                 |\n",
    "| 1         | Nadal       | 7                 |\n",
    "+-----------+-------------+-------------------+\n",
    "Explanation: \n",
    "Player 1 (Nadal) won 7 titles: Wimbledon (2018, 2019), Fr_open (2018, 2019, 2020), US_open (2018), and Au_open (2018).\n",
    "Player 2 (Federer) won 5 titles: Wimbledon (2020), US_open (2019, 2020), and Au_open (2019, 2020).\n",
    "Player 3 (Novak) did not win anything, we did not include them in the result table.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a49f69d4-bc7b-498a-a4bb-01965aa65e42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n|player_id|player_name|\n+---------+-----------+\n|        1|      Nadal|\n|        2|    Federer|\n|        3|      Novak|\n+---------+-----------+\n\n+----+---------+-------+-------+-------+\n|year|Wimbledon|Fr_open|US_open|Au_open|\n+----+---------+-------+-------+-------+\n|2018|        1|      1|      1|      1|\n|2019|        1|      1|      2|      2|\n|2020|        2|      1|      2|      2|\n+----+---------+-------+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define schema and create DataFrame for players\n",
    "players_schema = StructType([\n",
    "    StructField(\"player_id\", IntegerType(), True),\n",
    "    StructField(\"player_name\", StringType(), True)\n",
    "])\n",
    "players_data = [(1, 'Nadal'), (2, 'Federer'), (3, 'Novak')]\n",
    "players_df = spark.createDataFrame(players_data, schema=players_schema)\n",
    "\n",
    "# Define schema and create DataFrame for championships\n",
    "championships_schema = StructType([\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"Wimbledon\", IntegerType(), True),\n",
    "    StructField(\"Fr_open\", IntegerType(), True),\n",
    "    StructField(\"US_open\", IntegerType(), True),\n",
    "    StructField(\"Au_open\", IntegerType(), True)\n",
    "])\n",
    "championships_data = [(2018, 1, 1, 1, 1), (2019, 1, 1, 2, 2), (2020, 2, 1, 2, 2)]\n",
    "championships_df = spark.createDataFrame(championships_data, schema=championships_schema)\n",
    "players_df.show()\n",
    "championships_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1caeee42-c312-4c0d-874a-3fe707860fae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+\n|player_id|player_name|count(wins)|\n+---------+-----------+-----------+\n|        1|      Nadal|          7|\n|        2|    Federer|          5|\n+---------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "unpivoted_df = championships_df.selectExpr(\n",
    "    \"year\",\n",
    "    \"stack(4, 'Wimbledon', Wimbledon, 'Fr_open', Fr_open, 'US_open', US_open, 'Au_open', Au_open) as (championship, wins)\"\n",
    ")\n",
    "\n",
    "unpivoted_df.alias('U').join(players_df.alias('P'),col('U.wins') == col('P.player_id'),'left')\\\n",
    ".groupBy(col('player_id'),col('player_name')).agg(count(col('wins'))).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "731f8c87-0999-43e5-b3a3-1892ed30b5be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1747. Leetflex Banned Accounts\n",
    "### Level: Medium\n",
    "\n",
    "```\n",
    "\n",
    "Table: LogInfo\n",
    "\n",
    "+-------------+----------+\n",
    "| Column Name | Type     |\n",
    "+-------------+----------+\n",
    "| account_id  | int      |\n",
    "| ip_address  | int      |\n",
    "| login       | datetime |\n",
    "| logout      | datetime |\n",
    "+-------------+----------+\n",
    "This table may contain duplicate rows.\n",
    "The table contains information about the login and logout dates of Leetflex accounts. It also contains the IP address from which the account was logged in and out.\n",
    "It is guaranteed that the logout time is after the login time.\n",
    " \n",
    "\n",
    "Write a solution to find the account_id of the accounts that should be banned from Leetflex. An account should be banned if it was logged in at some moment from two different IP addresses.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "LogInfo table:\n",
    "+------------+------------+---------------------+---------------------+\n",
    "| account_id | ip_address | login               | logout              |\n",
    "+------------+------------+---------------------+---------------------+\n",
    "| 1          | 1          | 2021-02-01 09:00:00 | 2021-02-01 09:30:00 |\n",
    "| 1          | 2          | 2021-02-01 08:00:00 | 2021-02-01 11:30:00 |\n",
    "| 2          | 6          | 2021-02-01 20:30:00 | 2021-02-01 22:00:00 |\n",
    "| 2          | 7          | 2021-02-02 20:30:00 | 2021-02-02 22:00:00 |\n",
    "| 3          | 9          | 2021-02-01 16:00:00 | 2021-02-01 16:59:59 |\n",
    "| 3          | 13         | 2021-02-01 17:00:00 | 2021-02-01 17:59:59 |\n",
    "| 4          | 10         | 2021-02-01 16:00:00 | 2021-02-01 17:00:00 |\n",
    "| 4          | 11         | 2021-02-01 17:00:00 | 2021-02-01 17:59:59 |\n",
    "+------------+------------+---------------------+---------------------+\n",
    "Output: \n",
    "+------------+\n",
    "| account_id |\n",
    "+------------+\n",
    "| 1          |\n",
    "| 4          |\n",
    "+------------+\n",
    "Explanation: \n",
    "Account ID 1 --> The account was active from \"2021-02-01 09:00:00\" to \"2021-02-01 09:30:00\" with two different IP addresses (1 and 2). It should be banned.\n",
    "Account ID 2 --> The account was active from two different addresses (6, 7) but in two different times.\n",
    "Account ID 3 --> The account was active from two different addresses (9, 13) on the same day but they do not intersect at any moment.\n",
    "Account ID 4 --> The account was active from \"2021-02-01 17:00:00\" to \"2021-02-01 17:00:00\" with two different IP addresses (10 and 11). It should be banned.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64bb87ad-7c9c-4abd-b593-7bb2688e9479",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+-------------------+\n|account_id|ip_address|              login|             logout|\n+----------+----------+-------------------+-------------------+\n|         1|         1|2021-02-01 09:00:00|2021-02-01 09:30:00|\n|         1|         2|2021-02-01 08:00:00|2021-02-01 11:30:00|\n|         2|         6|2021-02-01 20:30:00|2021-02-01 22:00:00|\n|         2|         7|2021-02-02 20:30:00|2021-02-02 22:00:00|\n|         3|         9|2021-02-01 16:00:00|2021-02-01 16:59:59|\n|         3|        13|2021-02-01 17:00:00|2021-02-01 17:59:59|\n|         4|        10|2021-02-01 16:00:00|2021-02-01 17:00:00|\n|         4|        11|2021-02-01 17:00:00|2021-02-01 17:59:59|\n+----------+----------+-------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, 1, '2021-02-01 09:00:00', '2021-02-01 09:30:00'),\n",
    "    (1, 2, '2021-02-01 08:00:00', '2021-02-01 11:30:00'),\n",
    "    (2, 6, '2021-02-01 20:30:00', '2021-02-01 22:00:00'),\n",
    "    (2, 7, '2021-02-02 20:30:00', '2021-02-02 22:00:00'),\n",
    "    (3, 9, '2021-02-01 16:00:00', '2021-02-01 16:59:59'),\n",
    "    (3, 13, '2021-02-01 17:00:00', '2021-02-01 17:59:59'),\n",
    "    (4, 10, '2021-02-01 16:00:00', '2021-02-01 17:00:00'),\n",
    "    (4, 11, '2021-02-01 17:00:00', '2021-02-01 17:59:59')\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "schema = ['account_id', 'ip_address', 'login', 'logout']\n",
    "\n",
    "# Create DataFrame\n",
    "log_info_df = spark.createDataFrame(data, schema=schema)\n",
    "log_info_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22af1e4d-0c70-475e-9136-547f81413845",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|account_id|\n+----------+\n|         1|\n|         4|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Write a solution to find the account_id of the accounts that should be banned from Leetflex.\n",
    "#An account should be banned if it was logged in at some moment from two different IP addresses.\n",
    "\n",
    "#Return the result table in any order.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "cross_join_df = log_info_df.alias('l1').crossJoin(log_info_df.alias('l2'))\n",
    "\n",
    "filtered_df = cross_join_df.filter(\n",
    "    (col('l1.account_id') == col('l2.account_id')) &\n",
    "    (col('l1.ip_address') != col('l2.ip_address')) &\n",
    "    (col('l1.login') <= col('l2.logout')) &\n",
    "    (col('l2.login') <= col('l1.logout'))\n",
    ")\n",
    "\n",
    "result_df = filtered_df.select(col('l1.account_id')).distinct()\n",
    "\n",
    "result_df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e7d7970-813b-4334-ba84-f56fd5b7071a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#Subqueries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5f57ef0-7bde-497c-a635-48334e1c2154",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1350. Students With Invalid Departments\n",
    "```\n",
    "Table: Departments\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| id            | int     |\n",
    "| name          | varchar |\n",
    "+---------------+---------+\n",
    "In SQL, id is the primary key of this table.\n",
    "The table has information about the id of each department of a university.\n",
    " \n",
    "\n",
    "Table: Students\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| id            | int     |\n",
    "| name          | varchar |\n",
    "| department_id | int     |\n",
    "+---------------+---------+\n",
    "In SQL, id is the primary key of this table.\n",
    "The table has information about the id of each student at a university and the id of the department he/she studies at.\n",
    " \n",
    "\n",
    "Find the id and the name of all students who are enrolled in departments that no longer exist.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Departments table:\n",
    "+------+--------------------------+\n",
    "| id   | name                     |\n",
    "+------+--------------------------+\n",
    "| 1    | Electrical Engineering   |\n",
    "| 7    | Computer Engineering     |\n",
    "| 13   | Bussiness Administration |\n",
    "+------+--------------------------+\n",
    "Students table:\n",
    "+------+----------+---------------+\n",
    "| id   | name     | department_id |\n",
    "+------+----------+---------------+\n",
    "| 23   | Alice    | 1             |\n",
    "| 1    | Bob      | 7             |\n",
    "| 5    | Jennifer | 13            |\n",
    "| 2    | John     | 14            |\n",
    "| 4    | Jasmine  | 77            |\n",
    "| 3    | Steve    | 74            |\n",
    "| 6    | Luis     | 1             |\n",
    "| 8    | Jonathan | 7             |\n",
    "| 7    | Daiana   | 33            |\n",
    "| 11   | Madelynn | 1             |\n",
    "+------+----------+---------------+\n",
    "Output: \n",
    "+------+----------+\n",
    "| id   | name     |\n",
    "+------+----------+\n",
    "| 2    | John     |\n",
    "| 7    | Daiana   |\n",
    "| 4    | Jasmine  |\n",
    "| 3    | Steve    |\n",
    "+------+----------+\n",
    "Explanation: \n",
    "John, Daiana, Steve, and Jasmine are enrolled in departments 14, 33, 74, and 77 respectively. department 14, 33, 74, and 77 do not exist in the Departments table.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d2d8c5c-9c0c-4c5d-98d6-9983f325b257",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n| id|                name|\n+---+--------------------+\n|  1|Electrical Engine...|\n|  7|Computer Engineering|\n| 13|Bussiness Adminis...|\n+---+--------------------+\n\n+---+--------+-------------+\n| id|    name|department_id|\n+---+--------+-------------+\n| 23|   Alice|            1|\n|  1|     Bob|            7|\n|  5|Jennifer|           13|\n|  2|    John|           14|\n|  4| Jasmine|           77|\n|  3|   Steve|           74|\n|  6|    Luis|            1|\n|  8|Jonathan|            7|\n|  7|  Daiana|           33|\n| 11|Madelynn|            1|\n+---+--------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Define schemas\n",
    "departments_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "students_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"department_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create data\n",
    "departments_data = [\n",
    "    (1, 'Electrical Engineering'),\n",
    "    (7, 'Computer Engineering'),\n",
    "    (13, 'Bussiness Administration')\n",
    "]\n",
    "\n",
    "students_data = [\n",
    "    (23, 'Alice', 1),\n",
    "    (1, 'Bob', 7),\n",
    "    (5, 'Jennifer', 13),\n",
    "    (2, 'John', 14),\n",
    "    (4, 'Jasmine', 77),\n",
    "    (3, 'Steve', 74),\n",
    "    (6, 'Luis', 1),\n",
    "    (8, 'Jonathan', 7),\n",
    "    (7, 'Daiana', 33),\n",
    "    (11, 'Madelynn', 1)\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "departments_df = spark.createDataFrame(departments_data, schema=departments_schema)\n",
    "students_df = spark.createDataFrame(students_data, schema=students_schema)\n",
    "\n",
    "# Show DataFrames\n",
    "departments_df.show()\n",
    "students_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36283a33-6f4a-46c1-a211-dba30c4ab6dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n| id|   name|\n+---+-------+\n|  2|   John|\n|  4|Jasmine|\n|  3|  Steve|\n|  7| Daiana|\n+---+-------+\n\n"
     ]
    }
   ],
   "source": [
    "students_df.alias('S')\\\n",
    "    .join(departments_df.alias('D'),col('S.department_id') == col('D.id'),'left').filter(col('D.name').isNull())\\\n",
    "        .select(col('S.id'),col('S.name')).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9e54397-9b71-41c0-a6dc-780d6805906a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1303. Find the Team Size\n",
    "```\n",
    "Table: Employee\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| employee_id   | int     |\n",
    "| team_id       | int     |\n",
    "+---------------+---------+\n",
    "employee_id is the primary key (column with unique values) for this table.\n",
    "Each row of this table contains the ID of each employee and their respective team.\n",
    " \n",
    "\n",
    "Write a solution to find the team size of each of the employees.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Employee Table:\n",
    "+-------------+------------+\n",
    "| employee_id | team_id    |\n",
    "+-------------+------------+\n",
    "|     1       |     8      |\n",
    "|     2       |     8      |\n",
    "|     3       |     8      |\n",
    "|     4       |     7      |\n",
    "|     5       |     9      |\n",
    "|     6       |     9      |\n",
    "+-------------+------------+\n",
    "Output: \n",
    "+-------------+------------+\n",
    "| employee_id | team_size  |\n",
    "+-------------+------------+\n",
    "|     1       |     3      |\n",
    "|     2       |     3      |\n",
    "|     3       |     3      |\n",
    "|     4       |     1      |\n",
    "|     5       |     2      |\n",
    "|     6       |     2      |\n",
    "+-------------+------------+\n",
    "Explanation: \n",
    "Employees with Id 1,2,3 are part of a team with team_id = 8.\n",
    "Employee with Id 4 is part of a team with team_id = 7.\n",
    "Employees with Id 5,6 are part of a team with team_id = 9.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f75f6f1-70ea-4833-8a24-5372d2c26829",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n|employee_id|team_id|\n+-----------+-------+\n|          1|      8|\n|          2|      8|\n|          3|      8|\n|          4|      7|\n|          5|      9|\n|          6|      9|\n+-----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "# Assuming you have a Spark session named 'spark'\n",
    "data = [[1, 8], [2, 8], [3, 8], [4, 7], [5, 9], [6, 9]]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField('employee_id', IntegerType(), True),\n",
    "    StructField('team_id', IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "employee_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show DataFrame\n",
    "employee_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dae63db-80a1-4ab8-be12-5e3ff15effe5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n|employee_id|team_size|\n+-----------+---------+\n|          1|        3|\n|          2|        3|\n|          3|        3|\n|          4|        1|\n|          5|        2|\n|          6|        2|\n+-----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "employee_df.alias('E1').join(employee_df.alias('E2'), col('E1.team_id') == col('E2.team_id'))\\\n",
    "    .groupBy(col('E1.employee_id')).agg(count('*').alias('team_size')).orderBy(col('employee_id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b3b2d2e-1c2a-493d-84b6-7205018bce27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 512. Game Play Analysis II\n",
    "```\n",
    "Table: Activity\n",
    "\n",
    "+--------------+---------+\n",
    "| Column Name  | Type    |\n",
    "+--------------+---------+\n",
    "| player_id    | int     |\n",
    "| device_id    | int     |\n",
    "| event_date   | date    |\n",
    "| games_played | int     |\n",
    "+--------------+---------+\n",
    "(player_id, event_date) is the primary key (combination of columns with unique values) of this table.\n",
    "This table shows the activity of players of some games.\n",
    "Each row is a record of a player who logged in and played a number of games (possibly 0) before logging out on someday using some device.\n",
    " \n",
    "\n",
    "Write a solution to report the device that is first logged in for each player.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Activity table:\n",
    "+-----------+-----------+------------+--------------+\n",
    "| player_id | device_id | event_date | games_played |\n",
    "+-----------+-----------+------------+--------------+\n",
    "| 1         | 2         | 2016-03-01 | 5            |\n",
    "| 1         | 2         | 2016-05-02 | 6            |\n",
    "| 2         | 3         | 2017-06-25 | 1            |\n",
    "| 3         | 1         | 2016-03-02 | 0            |\n",
    "| 3         | 4         | 2018-07-03 | 5            |\n",
    "+-----------+-----------+------------+--------------+\n",
    "Output: \n",
    "+-----------+-----------+\n",
    "| player_id | device_id |\n",
    "+-----------+-----------+\n",
    "| 1         | 2         |\n",
    "| 2         | 3         |\n",
    "| 3         | 1         |\n",
    "+-----------+-----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7be7930e-1c94-4c31-b650-a7d9c502b468",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----------+------------+\n|player_id|device_id|event_date|games_played|\n+---------+---------+----------+------------+\n|        1|        2|2016-03-01|           5|\n|        1|        2|2016-05-02|           6|\n|        2|        3|2017-06-25|           1|\n|        3|        1|2016-03-02|           0|\n|        3|        4|2018-07-03|           5|\n+---------+---------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType, StringType\n",
    "\n",
    "# Assuming you have a Spark session named 'spark'\n",
    "data = [[1, 2, '2016-03-01', 5], [1, 2, '2016-05-02', 6], [2, 3, '2017-06-25', 1], [3, 1, '2016-03-02', 0], [3, 4, '2018-07-03', 5]]\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField('player_id', IntegerType(), True),\n",
    "    StructField('device_id', IntegerType(), True),\n",
    "    StructField('event_date', StringType(), True),\n",
    "    StructField('games_played', IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "activity_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show DataFrame\n",
    "activity_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "806ae450-0bd7-4e44-978d-b9c65d4a0fde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n|player_id|device_id|\n+---------+---------+\n|        1|        2|\n|        2|        3|\n|        3|        1|\n+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "windowOptions = Window.partitionBy(col('player_id')).orderBy(col('event_date'))\n",
    "activity_df.withColumn('first_device',row_number().over(windowOptions)).filter(col('first_device') == 1)\\\n",
    "    .select(col('player_id'),col('device_id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b733442-205e-4ac5-b3b9-dda1ac975357",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 184. Department Highest Salary\n",
    "```\n",
    "Table: Employee\n",
    "\n",
    "+--------------+---------+\n",
    "| Column Name  | Type    |\n",
    "+--------------+---------+\n",
    "| id           | int     |\n",
    "| name         | varchar |\n",
    "| salary       | int     |\n",
    "| departmentId | int     |\n",
    "+--------------+---------+\n",
    "id is the primary key (column with unique values) for this table.\n",
    "departmentId is a foreign key (reference columns) of the ID from the Department table.\n",
    "Each row of this table indicates the ID, name, and salary of an employee. It also contains the ID of their department.\n",
    " \n",
    "\n",
    "Table: Department\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| id          | int     |\n",
    "| name        | varchar |\n",
    "+-------------+---------+\n",
    "id is the primary key (column with unique values) for this table. It is guaranteed that department name is not NULL.\n",
    "Each row of this table indicates the ID of a department and its name.\n",
    " \n",
    "\n",
    "Write a solution to find employees who have the highest salary in each of the departments.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Employee table:\n",
    "+----+-------+--------+--------------+\n",
    "| id | name  | salary | departmentId |\n",
    "+----+-------+--------+--------------+\n",
    "| 1  | Joe   | 70000  | 1            |\n",
    "| 2  | Jim   | 90000  | 1            |\n",
    "| 3  | Henry | 80000  | 2            |\n",
    "| 4  | Sam   | 60000  | 2            |\n",
    "| 5  | Max   | 90000  | 1            |\n",
    "+----+-------+--------+--------------+\n",
    "Department table:\n",
    "+----+-------+\n",
    "| id | name  |\n",
    "+----+-------+\n",
    "| 1  | IT    |\n",
    "| 2  | Sales |\n",
    "+----+-------+\n",
    "Output: \n",
    "+------------+----------+--------+\n",
    "| Department | Employee | Salary |\n",
    "+------------+----------+--------+\n",
    "| IT         | Jim      | 90000  |\n",
    "| Sales      | Henry    | 80000  |\n",
    "| IT         | Max      | 90000  |\n",
    "+------------+----------+--------+\n",
    "Explanation: Max and Jim both have the highest salary in the IT department and Henry has the highest salary in the Sales department.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e882f56-7a94-47fc-b765-3f63c04335d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+------------+\n| id| name|salary|departmentId|\n+---+-----+------+------------+\n|  1|  Joe| 70000|           1|\n|  2|  Jim| 90000|           1|\n|  3|Henry| 80000|           2|\n|  4|  Sam| 60000|           2|\n|  5|  Max| 90000|           1|\n+---+-----+------+------------+\n\n+---+-----+\n| id| name|\n+---+-----+\n|  1|   IT|\n|  2|Sales|\n+---+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Assuming you already have a Spark session\n",
    "# spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Define schemas for the DataFrames\n",
    "employee_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"departmentId\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "department_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create data for the DataFrames\n",
    "employee_data = [\n",
    "    (1, 'Joe', 70000, 1),\n",
    "    (2, 'Jim', 90000, 1),\n",
    "    (3, 'Henry', 80000, 2),\n",
    "    (4, 'Sam', 60000, 2),\n",
    "    (5, 'Max', 90000, 1)\n",
    "]\n",
    "\n",
    "department_data = [\n",
    "    (1, 'IT'),\n",
    "    (2, 'Sales')\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "employee_df = spark.createDataFrame(employee_data, schema=employee_schema)\n",
    "department_df = spark.createDataFrame(department_data, schema=department_schema)\n",
    "\n",
    "# Show the DataFrames\n",
    "employee_df.show()\n",
    "department_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a327d1c-0c9d-4d3c-8733-36d8eaad7d65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------+\n|Department|Employee|salary|\n+----------+--------+------+\n|        IT|     Jim| 90000|\n|        IT|     Max| 90000|\n|     Sales|   Henry| 80000|\n+----------+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "#Write a solution to find employees who have the highest salary in each of the departments.\n",
    "#Return the result table in any order.\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "windowOptions  = Window.partitionBy(col('departmentId')).orderBy(col('salary').desc())\n",
    "employee_df.alias('E').join(department_df.alias('D'), col('E.departmentId') == col('D.id'),'inner')\\\n",
    "    .withColumn('Rank',rank().over(windowOptions)).filter(col('Rank') == 1)\\\n",
    "        .select(col('D.name').alias('Department'),col('E.name').alias('Employee'),col('salary')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d2293d2-c4ee-4dd2-b9e7-fda351f00255",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1549. The Most Recent Orders for Each Product\n",
    "```\n",
    "Table: Customers\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| customer_id   | int     |\n",
    "| name          | varchar |\n",
    "+---------------+---------+\n",
    "customer_id is the column with unique values for this table.\n",
    "This table contains information about the customers.\n",
    " \n",
    "\n",
    "Table: Orders\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| order_id      | int     |\n",
    "| order_date    | date    |\n",
    "| customer_id   | int     |\n",
    "| product_id    | int     |\n",
    "+---------------+---------+\n",
    "order_id is the column with unique values for this table.\n",
    "This table contains information about the orders made by customer_id.\n",
    "There will be no product ordered by the same user more than once in one day.\n",
    " \n",
    "\n",
    "Table: Products\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| product_id    | int     |\n",
    "| product_name  | varchar |\n",
    "| price         | int     |\n",
    "+---------------+---------+\n",
    "product_id is the column with unique values for this table.\n",
    "This table contains information about the Products.\n",
    " \n",
    "\n",
    "Write a solution to find the most recent order(s) of each product.\n",
    "\n",
    "Return the result table ordered by product_name in ascending order and in case of a tie by the product_id in ascending order. If there still a tie, order them by order_id in ascending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Customers table:\n",
    "+-------------+-----------+\n",
    "| customer_id | name      |\n",
    "+-------------+-----------+\n",
    "| 1           | Winston   |\n",
    "| 2           | Jonathan  |\n",
    "| 3           | Annabelle |\n",
    "| 4           | Marwan    |\n",
    "| 5           | Khaled    |\n",
    "+-------------+-----------+\n",
    "Orders table:\n",
    "+----------+------------+-------------+------------+\n",
    "| order_id | order_date | customer_id | product_id |\n",
    "+----------+------------+-------------+------------+\n",
    "| 1        | 2020-07-31 | 1           | 1          |\n",
    "| 2        | 2020-07-30 | 2           | 2          |\n",
    "| 3        | 2020-08-29 | 3           | 3          |\n",
    "| 4        | 2020-07-29 | 4           | 1          |\n",
    "| 5        | 2020-06-10 | 1           | 2          |\n",
    "| 6        | 2020-08-01 | 2           | 1          |\n",
    "| 7        | 2020-08-01 | 3           | 1          |\n",
    "| 8        | 2020-08-03 | 1           | 2          |\n",
    "| 9        | 2020-08-07 | 2           | 3          |\n",
    "| 10       | 2020-07-15 | 1           | 2          |\n",
    "+----------+------------+-------------+------------+\n",
    "Products table:\n",
    "+------------+--------------+-------+\n",
    "| product_id | product_name | price |\n",
    "+------------+--------------+-------+\n",
    "| 1          | keyboard     | 120   |\n",
    "| 2          | mouse        | 80    |\n",
    "| 3          | screen       | 600   |\n",
    "| 4          | hard disk    | 450   |\n",
    "+------------+--------------+-------+\n",
    "Output: \n",
    "+--------------+------------+----------+------------+\n",
    "| product_name | product_id | order_id | order_date |\n",
    "+--------------+------------+----------+------------+\n",
    "| keyboard     | 1          | 6        | 2020-08-01 |\n",
    "| keyboard     | 1          | 7        | 2020-08-01 |\n",
    "| mouse        | 2          | 8        | 2020-08-03 |\n",
    "| screen       | 3          | 3        | 2020-08-29 |\n",
    "+--------------+------------+----------+------------+\n",
    "Explanation: \n",
    "keyboard's most recent order is in 2020-08-01, it was ordered two times this day.\n",
    "mouse's most recent order is in 2020-08-03, it was ordered only once this day.\n",
    "screen's most recent order is in 2020-08-29, it was ordered only once this day.\n",
    "The hard disk was never ordered and we do not include it in the result table.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36009cdf-55ef-4abe-ac8d-6ca1d287b4f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n|customer_id|     name|\n+-----------+---------+\n|          1|  Winston|\n|          2| Jonathan|\n|          3|Annabelle|\n|          4|   Marwan|\n|          5|   Khaled|\n+-----------+---------+\n\n+--------+----------+-----------+----------+\n|order_id|order_date|customer_id|product_id|\n+--------+----------+-----------+----------+\n|       1|2020-07-31|          1|         1|\n|       2|2020-07-30|          2|         2|\n|       3|2020-08-29|          3|         3|\n|       4|2020-07-29|          4|         1|\n|       5|2020-06-10|          1|         2|\n|       6|2020-08-01|          2|         1|\n|       7|2020-08-01|          3|         1|\n|       8|2020-08-03|          1|         2|\n|       9|2020-08-07|          2|         3|\n|      10|2020-07-15|          1|         2|\n+--------+----------+-----------+----------+\n\n+----------+------------+-----+\n|product_id|product_name|price|\n+----------+------------+-----+\n|         1|    keyboard|  120|\n|         2|       mouse|   80|\n|         3|      screen|  600|\n|         4|   hard disk|  450|\n+----------+------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "# Define schemas for the DataFrames\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "products_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"price\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create data for the DataFrames\n",
    "customers_data = [\n",
    "    (1, 'Winston'),\n",
    "    (2, 'Jonathan'),\n",
    "    (3, 'Annabelle'),\n",
    "    (4, 'Marwan'),\n",
    "    (5, 'Khaled')\n",
    "]\n",
    "\n",
    "orders_data = [\n",
    "    (1, '2020-07-31', 1, 1),\n",
    "    (2, '2020-07-30', 2, 2),\n",
    "    (3, '2020-08-29', 3, 3),\n",
    "    (4, '2020-07-29', 4, 1),\n",
    "    (5, '2020-06-10', 1, 2),\n",
    "    (6, '2020-08-01', 2, 1),\n",
    "    (7, '2020-08-01', 3, 1),\n",
    "    (8, '2020-08-03', 1, 2),\n",
    "    (9, '2020-08-07', 2, 3),\n",
    "    (10, '2020-07-15', 1, 2)\n",
    "]\n",
    "\n",
    "products_data = [\n",
    "    (1, 'keyboard', 120),\n",
    "    (2, 'mouse', 80),\n",
    "    (3, 'screen', 600),\n",
    "    (4, 'hard disk', 450)\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "customers_df = spark.createDataFrame(customers_data, schema=customers_schema)\n",
    "orders_df = spark.createDataFrame(orders_data, schema=orders_schema)\n",
    "products_df = spark.createDataFrame(products_data, schema=products_schema)\n",
    "\n",
    "# Show the DataFrames\n",
    "customers_df.show()\n",
    "orders_df.show()\n",
    "products_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89b0cc64-a6c8-43d7-86e7-45aa736513f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------+----------+\n|product_name|product_id|order_id|order_date|\n+------------+----------+--------+----------+\n|    keyboard|         1|       6|2020-08-01|\n|    keyboard|         1|       7|2020-08-01|\n|       mouse|         2|       8|2020-08-03|\n|      screen|         3|       3|2020-08-29|\n+------------+----------+--------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to find the most recent order(s) of each product.\n",
    "\n",
    "# Return the result table ordered by product_name in ascending order and in case of a tie by the product_id in ascending order. If there still a tie, order them by order_id in ascending order.\n",
    "windowOptions = Window.partitionBy(col('O.product_id')).orderBy(col('order_date').desc())\n",
    "orders_df.alias('O').join(products_df.alias('P'), col('O.product_id') == col('P.product_id'),'left')\\\n",
    "    .withColumn('recentOrder',rank().over(windowOptions)).filter(col('recentOrder')==1)\\\n",
    "        .select(col('product_name'),col('P.product_id'),col('O.order_id'),col('O.order_date')).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb542c90-6d30-41a9-8ec8-5e9743fde611",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1532. The Most Recent Three Orders\n",
    "```\n",
    "Table: Customers\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| customer_id   | int     |\n",
    "| name          | varchar |\n",
    "+---------------+---------+\n",
    "customer_id is the column with unique values for this table.\n",
    "This table contains information about customers.\n",
    " \n",
    "\n",
    "Table: Orders\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| order_id      | int     |\n",
    "| order_date    | date    |\n",
    "| customer_id   | int     |\n",
    "| cost          | int     |\n",
    "+---------------+---------+\n",
    "order_id is the column with unique values for this table.\n",
    "This table contains information about the orders made by customer_id.\n",
    "Each customer has one order per day.\n",
    " \n",
    "\n",
    "Write a solution to find the most recent three orders of each user. If a user ordered less than three orders, return all of their orders.\n",
    "\n",
    "Return the result table ordered by customer_name in ascending order and in case of a tie by the customer_id in ascending order. If there is still a tie, order them by order_date in descending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Customers table:\n",
    "+-------------+-----------+\n",
    "| customer_id | name      |\n",
    "+-------------+-----------+\n",
    "| 1           | Winston   |\n",
    "| 2           | Jonathan  |\n",
    "| 3           | Annabelle |\n",
    "| 4           | Marwan    |\n",
    "| 5           | Khaled    |\n",
    "+-------------+-----------+\n",
    "Orders table:\n",
    "+----------+------------+-------------+------+\n",
    "| order_id | order_date | customer_id | cost |\n",
    "+----------+------------+-------------+------+\n",
    "| 1        | 2020-07-31 | 1           | 30   |\n",
    "| 2        | 2020-07-30 | 2           | 40   |\n",
    "| 3        | 2020-07-31 | 3           | 70   |\n",
    "| 4        | 2020-07-29 | 4           | 100  |\n",
    "| 5        | 2020-06-10 | 1           | 1010 |\n",
    "| 6        | 2020-08-01 | 2           | 102  |\n",
    "| 7        | 2020-08-01 | 3           | 111  |\n",
    "| 8        | 2020-08-03 | 1           | 99   |\n",
    "| 9        | 2020-08-07 | 2           | 32   |\n",
    "| 10       | 2020-07-15 | 1           | 2    |\n",
    "+----------+------------+-------------+------+\n",
    "Output: \n",
    "+---------------+-------------+----------+------------+\n",
    "| customer_name | customer_id | order_id | order_date |\n",
    "+---------------+-------------+----------+------------+\n",
    "| Annabelle     | 3           | 7        | 2020-08-01 |\n",
    "| Annabelle     | 3           | 3        | 2020-07-31 |\n",
    "| Jonathan      | 2           | 9        | 2020-08-07 |\n",
    "| Jonathan      | 2           | 6        | 2020-08-01 |\n",
    "| Jonathan      | 2           | 2        | 2020-07-30 |\n",
    "| Marwan        | 4           | 4        | 2020-07-29 |\n",
    "| Winston       | 1           | 8        | 2020-08-03 |\n",
    "| Winston       | 1           | 1        | 2020-07-31 |\n",
    "| Winston       | 1           | 10       | 2020-07-15 |\n",
    "+---------------+-------------+----------+------------+\n",
    "Explanation: \n",
    "Winston has 4 orders, we discard the order of \"2020-06-10\" because it is the oldest order.\n",
    "Annabelle has only 2 orders, we return them.\n",
    "Jonathan has exactly 3 orders.\n",
    "Marwan ordered only one time.\n",
    "We sort the result table by customer_name in ascending order, by customer_id in ascending order, and by order_date in descending order in case of a tie.\n",
    " \n",
    "\n",
    "Follow up: Could you write a general solution for the most recent n orders?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a992d7e3-216e-4002-984e-c3b61a965401",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n|customer_id|     name|\n+-----------+---------+\n|          1|  Winston|\n|          2| Jonathan|\n|          3|Annabelle|\n|          4|   Marwan|\n|          5|   Khaled|\n+-----------+---------+\n\n+--------+----------+-----------+----+\n|order_id|order_date|customer_id|cost|\n+--------+----------+-----------+----+\n|       1|2020-07-31|          1|  30|\n|       2|2020-07-30|          2|  40|\n|       3|2020-07-31|          3|  70|\n|       4|2020-07-29|          4| 100|\n|       5|2020-06-10|          1|1010|\n|       6|2020-08-01|          2| 102|\n|       7|2020-08-01|          3| 111|\n|       8|2020-08-03|          1|  99|\n|       9|2020-08-07|          2|  32|\n|      10|2020-07-15|          1|   2|\n+--------+----------+-----------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "# Define schemas for the DataFrames\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"cost\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create data for the DataFrames\n",
    "customers_data = [\n",
    "    (1, 'Winston'),\n",
    "    (2, 'Jonathan'),\n",
    "    (3, 'Annabelle'),\n",
    "    (4, 'Marwan'),\n",
    "    (5, 'Khaled')\n",
    "]\n",
    "\n",
    "orders_data = [\n",
    "    (1, '2020-07-31', 1, 30),\n",
    "    (2, '2020-07-30', 2, 40),\n",
    "    (3, '2020-07-31', 3, 70),\n",
    "    (4, '2020-07-29', 4, 100),\n",
    "    (5, '2020-06-10', 1, 1010),\n",
    "    (6, '2020-08-01', 2, 102),\n",
    "    (7, '2020-08-01', 3, 111),\n",
    "    (8, '2020-08-03', 1, 99),\n",
    "    (9, '2020-08-07', 2, 32),\n",
    "    (10, '2020-07-15', 1, 2)\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "customers_df = spark.createDataFrame(customers_data, schema=customers_schema)\n",
    "orders_df = spark.createDataFrame(orders_data, schema=orders_schema)\n",
    "\n",
    "# Show the DataFrames\n",
    "customers_df.show()\n",
    "orders_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17319196-3350-4e04-a629-823332e60e3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+--------+----------+\n|customer_name|customer_id|order_id|order_date|\n+-------------+-----------+--------+----------+\n|    Annabelle|          3|       7|2020-08-01|\n|    Annabelle|          3|       3|2020-07-31|\n|     Jonathan|          2|       9|2020-08-07|\n|     Jonathan|          2|       6|2020-08-01|\n|     Jonathan|          2|       2|2020-07-30|\n|       Marwan|          4|       4|2020-07-29|\n|      Winston|          1|       8|2020-08-03|\n|      Winston|          1|       1|2020-07-31|\n|      Winston|          1|      10|2020-07-15|\n+-------------+-----------+--------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to find the most recent three orders of each user. If a user ordered less than three orders, return all of their orders.\n",
    "\n",
    "# Return the result table ordered by customer_name in ascending order and in case of a tie by the customer_id in ascending order. If there is still a tie, order them by order_date in descending order.\n",
    "\n",
    "windowOptions = Window.partitionBy(col('C.customer_id')).orderBy(col('order_date').desc())\n",
    "orders_df.alias('O').join(customers_df.alias('C'), col('O.customer_id') == col('C.customer_id'),'left')\\\n",
    "    .withColumn('Recent3Orders',row_number().over(windowOptions)).filter(col('Recent3Orders').isin(1,2,3))\\\n",
    "        .select(col('name').alias('customer_name'),col('C.customer_id'),col('order_id'),col('order_date'))\\\n",
    "            .orderBy(col('customer_name'),col('order_date').desc()).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ac520d7-e156-4cd8-8ee0-83f1ba77472e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1831. Maximum Transaction Each Day\n",
    "```\n",
    "Table: Transactions\n",
    "\n",
    "+----------------+----------+\n",
    "| Column Name    | Type     |\n",
    "+----------------+----------+\n",
    "| transaction_id | int      |\n",
    "| day            | datetime |\n",
    "| amount         | int      |\n",
    "+----------------+----------+\n",
    "transaction_id is the column with unique values for this table.\n",
    "Each row contains information about one transaction.\n",
    " \n",
    "\n",
    "Write a solution to report the IDs of the transactions with the maximum amount on their respective day. If in one day there are multiple such transactions, return all of them.\n",
    "\n",
    "Return the result table ordered by transaction_id in ascending order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Transactions table:\n",
    "+----------------+--------------------+--------+\n",
    "| transaction_id | day                | amount |\n",
    "+----------------+--------------------+--------+\n",
    "| 8              | 2021-4-3 15:57:28  | 57     |\n",
    "| 9              | 2021-4-28 08:47:25 | 21     |\n",
    "| 1              | 2021-4-29 13:28:30 | 58     |\n",
    "| 5              | 2021-4-28 16:39:59 | 40     |\n",
    "| 6              | 2021-4-29 23:39:28 | 58     |\n",
    "+----------------+--------------------+--------+\n",
    "Output: \n",
    "+----------------+\n",
    "| transaction_id |\n",
    "+----------------+\n",
    "| 1              |\n",
    "| 5              |\n",
    "| 6              |\n",
    "| 8              |\n",
    "+----------------+\n",
    "Explanation: \n",
    "\"2021-4-3\"  --> We have one transaction with ID 8, so we add 8 to the result table.\n",
    "\"2021-4-28\" --> We have two transactions with IDs 5 and 9. The transaction with ID 5 has an amount of 40, while the transaction with ID 9 has an amount of 21. We only include the transaction with ID 5 as it has the maximum amount this day.\n",
    "\"2021-4-29\" --> We have two transactions with IDs 1 and 6. Both transactions have the same amount of 58, so we include both in the result table.\n",
    "We order the result table by transaction_id after collecting these IDs.\n",
    " \n",
    "\n",
    "Follow up: Could you solve it without using the MAX() function?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2f1ad50-a0e5-4a2b-9702-33b978f7f28d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+------+\n|transaction_id|                day|amount|\n+--------------+-------------------+------+\n|             8|2021-04-03 15:57:28|    57|\n|             9|2021-04-28 08:47:25|    21|\n|             1|2021-04-29 13:28:30|    58|\n|             5|2021-04-28 16:39:59|    40|\n|             6|2021-04-29 23:39:28|    58|\n+--------------+-------------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, StringType\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "transactions_schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), True),\n",
    "    StructField(\"day\", StringType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create data for the DataFrame\n",
    "transactions_data = [\n",
    "    (8, '2021-04-03 15:57:28', 57),\n",
    "    (9, '2021-04-28 08:47:25', 21),\n",
    "    (1, '2021-04-29 13:28:30', 58),\n",
    "    (5, '2021-04-28 16:39:59', 40),\n",
    "    (6, '2021-04-29 23:39:28', 58)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "transactions_df = spark.createDataFrame(transactions_data, schema=transactions_schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "transactions_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c61737f0-7a95-4ee0-8d57-a24889ed68a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n|transaction_id|\n+--------------+\n|             1|\n|             5|\n|             6|\n|             8|\n+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to report the IDs of the transactions with the maximum amount on their respective day. If in one day there are multiple such transactions, return all of them.\n",
    "\n",
    "# Return the result table ordered by transaction_id in ascending order.\n",
    "df_with_date = transactions_df.withColumn('date', to_date('day', 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "windowOptions = Window.partitionBy(col('date')).orderBy(col('amount').desc())\n",
    "df_with_date.withColumn('MaxTransactionOfTheDay',rank().over(windowOptions))\\\n",
    "    .filter(col('MaxTransactionOfTheDay') == 1).select(col('transaction_id'))\\\n",
    "        .orderBy(col('transaction_id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f41cda0f-d1bf-4867-ba76-ad18084444e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Advanced Topics: Window Function and CTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c003a500-bea6-4892-bba8-9a215add406a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1077. Project Employees III\n",
    "```\n",
    "Table: Project\n",
    "\n",
    "+-------------+---------+\n",
    "| Column Name | Type    |\n",
    "+-------------+---------+\n",
    "| project_id  | int     |\n",
    "| employee_id | int     |\n",
    "+-------------+---------+\n",
    "(project_id, employee_id) is the primary key (combination of columns with unique values) of this table.\n",
    "employee_id is a foreign key (reference column) to Employee table.\n",
    "Each row of this table indicates that the employee with employee_id is working on the project with project_id.\n",
    " \n",
    "\n",
    "Table: Employee\n",
    "\n",
    "+------------------+---------+\n",
    "| Column Name      | Type    |\n",
    "+------------------+---------+\n",
    "| employee_id      | int     |\n",
    "| name             | varchar |\n",
    "| experience_years | int     |\n",
    "+------------------+---------+\n",
    "employee_id is the primary key (column with unique values) of this table.\n",
    "Each row of this table contains information about one employee.\n",
    " \n",
    "\n",
    "Write a solution to report the most experienced employees in each project. In case of a tie, report all employees with the maximum number of experience years.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Project table:\n",
    "+-------------+-------------+\n",
    "| project_id  | employee_id |\n",
    "+-------------+-------------+\n",
    "| 1           | 1           |\n",
    "| 1           | 2           |\n",
    "| 1           | 3           |\n",
    "| 2           | 1           |\n",
    "| 2           | 4           |\n",
    "+-------------+-------------+\n",
    "Employee table:\n",
    "+-------------+--------+------------------+\n",
    "| employee_id | name   | experience_years |\n",
    "+-------------+--------+------------------+\n",
    "| 1           | Khaled | 3                |\n",
    "| 2           | Ali    | 2                |\n",
    "| 3           | John   | 3                |\n",
    "| 4           | Doe    | 2                |\n",
    "+-------------+--------+------------------+\n",
    "Output: \n",
    "+-------------+---------------+\n",
    "| project_id  | employee_id   |\n",
    "+-------------+---------------+\n",
    "| 1           | 1             |\n",
    "| 1           | 3             |\n",
    "| 2           | 1             |\n",
    "+-------------+---------------+\n",
    "Explanation: Both employees with id 1 and 3 have the most experience among the employees of the first project. For the second project, the employee with id 1 has the most experience.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f3978d0-2b34-4a7f-b2e1-34c98f32a27c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n|project_id|employee_id|\n+----------+-----------+\n|         1|          1|\n|         1|          2|\n|         1|          3|\n|         2|          1|\n|         2|          4|\n+----------+-----------+\n\n+-----------+------+----------------+\n|employee_id|  name|experience_years|\n+-----------+------+----------------+\n|          1|Khaled|               3|\n|          2|   Ali|               2|\n|          3|  John|               3|\n|          4|   Doe|               2|\n+-----------+------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Assuming you already have a Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Data for the project DataFrame\n",
    "project_data = [(1, 1), (1, 2), (1, 3), (2, 1), (2, 4)]\n",
    "project_schema = StructType([\n",
    "    StructField('project_id', IntegerType(), False),\n",
    "    StructField('employee_id', IntegerType(), False)\n",
    "])\n",
    "project_df = spark.createDataFrame(project_data, schema=project_schema)\n",
    "\n",
    "# Data for the employee DataFrame\n",
    "employee_data = [(1, 'Khaled', 3), (2, 'Ali', 2), (3, 'John', 3), (4, 'Doe', 2)]\n",
    "employee_schema = StructType([\n",
    "    StructField('employee_id', IntegerType(), False),\n",
    "    StructField('name', StringType(), False),\n",
    "    StructField('experience_years', IntegerType(), False)\n",
    "])\n",
    "employee_df = spark.createDataFrame(employee_data, schema=employee_schema)\n",
    "\n",
    "# Show the DataFrames\n",
    "project_df.show()\n",
    "employee_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99da5ca7-cf1a-4db4-8384-f0cd1ca895ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n|project_id|employee_id|\n+----------+-----------+\n|         1|          1|\n|         1|          3|\n|         2|          1|\n+----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to report the most experienced employees in each project. In case of a tie, report all employees with the maximum number of experience years.\n",
    "\n",
    "# Return the result table in any order.\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "windowOptions = Window.partitionBy(col('P.project_id')).orderBy(col('experience_years').desc())\n",
    "employee_df.alias('E').join(project_df.alias('P'),col('E.employee_id') == col('P.employee_id'),'left')\\\n",
    "    .withColumn('maxExperience',rank().over(windowOptions)).filter(col('maxExperience') == 1)\\\n",
    "        .select(col('project_id'),col('E.employee_id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f59cd40-f72f-4a01-8105-4983c6294aba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## (Premium) 1285. Find the Start and End Number of Continuous Ranges\n",
    "```\n",
    "Table: Logs\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| log_id        | int     |\n",
    "+---------------+---------+\n",
    "log_id is the column of unique values for this table.\n",
    "Each row of this table contains the ID in a log Table.\n",
    " \n",
    "\n",
    "Write a solution to find the start and end number of continuous ranges in the table Logs.\n",
    "\n",
    "Return the result table ordered by start_id.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Logs table:\n",
    "+------------+\n",
    "| log_id     |\n",
    "+------------+\n",
    "| 1          |\n",
    "| 2          |\n",
    "| 3          |\n",
    "| 7          |\n",
    "| 8          |\n",
    "| 10         |\n",
    "+------------+\n",
    "Output: \n",
    "+------------+--------------+\n",
    "| start_id   | end_id       |\n",
    "+------------+--------------+\n",
    "| 1          | 3            |\n",
    "| 7          | 8            |\n",
    "| 10         | 10           |\n",
    "+------------+--------------+\n",
    "Explanation: \n",
    "The result table should contain all ranges in table Logs.\n",
    "From 1 to 3 is contained in the table.\n",
    "From 4 to 6 is missing in the table\n",
    "From 7 to 8 is contained in the table.\n",
    "Number 9 is missing from the table.\n",
    "Number 10 is contained in the table.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4778ffa0-32e4-4cda-961f-a2b80284dd71",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n|log_id|\n+------+\n|     1|\n|     2|\n|     3|\n|     7|\n|     8|\n|    10|\n+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "# Assuming you already have a Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Data for the logs DataFrame\n",
    "logs_data = [(1,), (2,), (3,), (7,), (8,), (10,)]\n",
    "logs_schema = StructType([\n",
    "    StructField('log_id', IntegerType(), False)\n",
    "])\n",
    "logs_df = spark.createDataFrame(logs_data, schema=logs_schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "logs_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6d86df4-f142-4668-9aae-77e4e08cdb38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n|start_id|end_id|\n+--------+------+\n|       1|     3|\n|       7|     8|\n|      10|    10|\n+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to find the start and end number of continuous ranges in the table Logs.\n",
    "# Return the result table ordered by start_id.\n",
    "\n",
    "windowOptions = Window.orderBy(col('log_id'))\n",
    "logs_df.withColumn('RN', (col('log_id') - row_number().over(windowOptions))).groupBy(col('RN'))\\\n",
    "    .agg(min(col('log_id')).alias('start_id'),max(col('log_id')).alias('end_id')).drop(col('RN')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d010c51-380f-4cf7-8aa5-c55aa1320f7c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1596. The Most Frequently Ordered Products for Each Customer\n",
    "```\n",
    "Table: Customers\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| customer_id   | int     |\n",
    "| name          | varchar |\n",
    "+---------------+---------+\n",
    "customer_id is the column with unique values for this table.\n",
    "This table contains information about the customers.\n",
    " \n",
    "\n",
    "Table: Orders\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| order_id      | int     |\n",
    "| order_date    | date    |\n",
    "| customer_id   | int     |\n",
    "| product_id    | int     |\n",
    "+---------------+---------+\n",
    "order_id is the column with unique values for this table.\n",
    "This table contains information about the orders made by customer_id.\n",
    "No customer will order the same product more than once in a single day.\n",
    " \n",
    "\n",
    "Table: Products\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| product_id    | int     |\n",
    "| product_name  | varchar |\n",
    "| price         | int     |\n",
    "+---------------+---------+\n",
    "product_id is the column with unique values for this table.\n",
    "This table contains information about the products.\n",
    " \n",
    "\n",
    "Write a solution to find the most frequently ordered product(s) for each customer.\n",
    "\n",
    "The result table should have the product_id and product_name for each customer_id who ordered at least one order.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Customers table:\n",
    "+-------------+-------+\n",
    "| customer_id | name  |\n",
    "+-------------+-------+\n",
    "| 1           | Alice |\n",
    "| 2           | Bob   |\n",
    "| 3           | Tom   |\n",
    "| 4           | Jerry |\n",
    "| 5           | John  |\n",
    "+-------------+-------+\n",
    "Orders table:\n",
    "+----------+------------+-------------+------------+\n",
    "| order_id | order_date | customer_id | product_id |\n",
    "+----------+------------+-------------+------------+\n",
    "| 1        | 2020-07-31 | 1           | 1          |\n",
    "| 2        | 2020-07-30 | 2           | 2          |\n",
    "| 3        | 2020-08-29 | 3           | 3          |\n",
    "| 4        | 2020-07-29 | 4           | 1          |\n",
    "| 5        | 2020-06-10 | 1           | 2          |\n",
    "| 6        | 2020-08-01 | 2           | 1          |\n",
    "| 7        | 2020-08-01 | 3           | 3          |\n",
    "| 8        | 2020-08-03 | 1           | 2          |\n",
    "| 9        | 2020-08-07 | 2           | 3          |\n",
    "| 10       | 2020-07-15 | 1           | 2          |\n",
    "+----------+------------+-------------+------------+\n",
    "Products table:\n",
    "+------------+--------------+-------+\n",
    "| product_id | product_name | price |\n",
    "+------------+--------------+-------+\n",
    "| 1          | keyboard     | 120   |\n",
    "| 2          | mouse        | 80    |\n",
    "| 3          | screen       | 600   |\n",
    "| 4          | hard disk    | 450   |\n",
    "+------------+--------------+-------+\n",
    "Output: \n",
    "+-------------+------------+--------------+\n",
    "| customer_id | product_id | product_name |\n",
    "+-------------+------------+--------------+\n",
    "| 1           | 2          | mouse        |\n",
    "| 2           | 1          | keyboard     |\n",
    "| 2           | 2          | mouse        |\n",
    "| 2           | 3          | screen       |\n",
    "| 3           | 3          | screen       |\n",
    "| 4           | 1          | keyboard     |\n",
    "+-------------+------------+--------------+\n",
    "Explanation: \n",
    "Alice (customer 1) ordered the mouse three times and the keyboard one time, so the mouse is the most frequently ordered product for them.\n",
    "Bob (customer 2) ordered the keyboard, the mouse, and the screen one time, so those are the most frequently ordered products for them.\n",
    "Tom (customer 3) only ordered the screen (two times), so that is the most frequently ordered product for them.\n",
    "Jerry (customer 4) only ordered the keyboard (one time), so that is the most frequently ordered product for them.\n",
    "John (customer 5) did not order anything, so we do not include them in the result table.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8da7da28-5315-4d64-8115-c3bf057b7784",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n|customer_id| name|\n+-----------+-----+\n|          1|Alice|\n|          2|  Bob|\n|          3|  Tom|\n|          4|Jerry|\n|          5| John|\n+-----------+-----+\n\n+--------+----------+-----------+----------+\n|order_id|order_date|customer_id|product_id|\n+--------+----------+-----------+----------+\n|       1|2020-07-31|          1|         1|\n|       2|2020-07-30|          2|         2|\n|       3|2020-08-29|          3|         3|\n|       4|2020-07-29|          4|         1|\n|       5|2020-06-10|          1|         2|\n|       6|2020-08-01|          2|         1|\n|       7|2020-08-01|          3|         3|\n|       8|2020-08-03|          1|         2|\n|       9|2020-08-07|          2|         3|\n|      10|2020-07-15|          1|         2|\n+--------+----------+-----------+----------+\n\n+----------+------------+-----+\n|product_id|product_name|price|\n+----------+------------+-----+\n|         1|    keyboard|  120|\n|         2|       mouse|   80|\n|         3|      screen|  600|\n|         4|   hard disk|  450|\n+----------+------------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Assuming you already have a Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Data for the customers DataFrame\n",
    "customers_data = [(1, 'Alice'), (2, 'Bob'), (3, 'Tom'), (4, 'Jerry'), (5, 'John')]\n",
    "customers_schema = StructType([\n",
    "    StructField('customer_id', IntegerType(), False),\n",
    "    StructField('name', StringType(), False)\n",
    "])\n",
    "customers_df = spark.createDataFrame(customers_data, schema=customers_schema)\n",
    "\n",
    "# Data for the orders DataFrame\n",
    "orders_data = [\n",
    "    (1, '2020-07-31', 1, 1), (2, '2020-07-30', 2, 2), (3, '2020-08-29', 3, 3),\n",
    "    (4, '2020-07-29', 4, 1), (5, '2020-06-10', 1, 2), (6, '2020-08-01', 2, 1),\n",
    "    (7, '2020-08-01', 3, 3), (8, '2020-08-03', 1, 2), (9, '2020-08-07', 2, 3),\n",
    "    (10, '2020-07-15', 1, 2)\n",
    "]\n",
    "orders_schema = StructType([\n",
    "    StructField('order_id', IntegerType(), False),\n",
    "    StructField('order_date', StringType(), False),\n",
    "    StructField('customer_id', IntegerType(), False),\n",
    "    StructField('product_id', IntegerType(), False)\n",
    "])\n",
    "orders_df = spark.createDataFrame(orders_data, schema=orders_schema)\n",
    "\n",
    "# Data for the products DataFrame\n",
    "products_data = [\n",
    "    (1, 'keyboard', 120), (2, 'mouse', 80),\n",
    "    (3, 'screen', 600), (4, 'hard disk', 450)\n",
    "]\n",
    "products_schema = StructType([\n",
    "    StructField('product_id', IntegerType(), False),\n",
    "    StructField('product_name', StringType(), False),\n",
    "    StructField('price', IntegerType(), False)\n",
    "])\n",
    "products_df = spark.createDataFrame(products_data, schema=products_schema)\n",
    "\n",
    "# Show the DataFrames\n",
    "customers_df.show()\n",
    "orders_df.show()\n",
    "products_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "477105ea-c449-4386-b88c-8fab0b4c1248",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+------------+\n|customer_id|product_id|product_name|\n+-----------+----------+------------+\n|          1|         2|       mouse|\n|          2|         1|    keyboard|\n|          2|         2|       mouse|\n|          2|         3|      screen|\n|          3|         3|      screen|\n|          4|         1|    keyboard|\n+-----------+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "windowOptions = Window.partitionBy(col('customer_id')).orderBy(col('frequency').desc())\n",
    "orders_df.alias('O').join(products_df.alias('P'),col('O.product_id') == col('P.product_id'))\\\n",
    "    .groupBy(col('customer_id'),col('O.product_id'),col('P.product_name')).agg(count('*')\\\n",
    "        .alias('frequency')).withColumn('RN',rank().over(windowOptions)).filter(col('RN') == 1).drop(col('RN'),col('frequency')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a27bde-9ba4-4bd6-8f9d-2ab75b6491b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## (Premium) 1709. Biggest Window Between Visits\n",
    "```\n",
    "Table: UserVisits\n",
    "\n",
    "+-------------+------+\n",
    "| Column Name | Type |\n",
    "+-------------+------+\n",
    "| user_id     | int  |\n",
    "| visit_date  | date |\n",
    "+-------------+------+\n",
    "This table does not have a primary key, it might contain duplicate rows.\n",
    "This table contains logs of the dates that users visited a certain retailer.\n",
    " \n",
    "\n",
    "Assume today's date is '2021-1-1'.\n",
    "\n",
    "Write a solution that will, for each user_id, find out the largest window of days between each visit and the one right after it (or today if you are considering the last visit).\n",
    "\n",
    "Return the result table ordered by user_id.\n",
    "\n",
    "The query result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "UserVisits table:\n",
    "+---------+------------+\n",
    "| user_id | visit_date |\n",
    "+---------+------------+\n",
    "| 1       | 2020-11-28 |\n",
    "| 1       | 2020-10-20 |\n",
    "| 1       | 2020-12-3  |\n",
    "| 2       | 2020-10-5  |\n",
    "| 2       | 2020-12-9  |\n",
    "| 3       | 2020-11-11 |\n",
    "+---------+------------+\n",
    "Output: \n",
    "+---------+---------------+\n",
    "| user_id | biggest_window|\n",
    "+---------+---------------+\n",
    "| 1       | 39            |\n",
    "| 2       | 65            |\n",
    "| 3       | 51            |\n",
    "+---------+---------------+\n",
    "Explanation: \n",
    "For the first user, the windows in question are between dates:\n",
    "    - 2020-10-20 and 2020-11-28 with a total of 39 days. \n",
    "    - 2020-11-28 and 2020-12-3 with a total of 5 days. \n",
    "    - 2020-12-3 and 2021-1-1 with a total of 29 days.\n",
    "Making the biggest window the one with 39 days.\n",
    "For the second user, the windows in question are between dates:\n",
    "    - 2020-10-5 and 2020-12-9 with a total of 65 days.\n",
    "    - 2020-12-9 and 2021-1-1 with a total of 23 days.\n",
    "Making the biggest window the one with 65 days.\n",
    "For the third user, the only window in question is between dates 2020-11-11 and 2021-1-1 with a total of 51 days.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23fb3a74-e440-47c1-820f-5becd2122597",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n|user_id|visit_date|\n+-------+----------+\n|      1|2020-11-28|\n|      1|2020-10-20|\n|      1|2020-12-03|\n|      2|2020-10-05|\n|      2|2020-12-09|\n|      3|2020-11-11|\n+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Assuming you already have a Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Data for the user_visits DataFrame\n",
    "user_visits_data = [\n",
    "    (1, '2020-11-28'), (1, '2020-10-20'), (1, '2020-12-03'),\n",
    "    (2, '2020-10-05'), (2, '2020-12-09'), (3, '2020-11-11')\n",
    "]\n",
    "\n",
    "# Convert date strings to actual DateType\n",
    "\n",
    "user_visits_schema = StructType([\n",
    "    StructField('user_id', IntegerType(), False),\n",
    "    StructField('visit_date', StringType(), False)\n",
    "])\n",
    "\n",
    "# Create the DataFrame\n",
    "user_visits_df = spark.createDataFrame(user_visits_data, schema=user_visits_schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "user_visits_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26c0d346-a3a7-4d83-9a86-cff66a58f568",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n|user_id|biggest_window|\n+-------+--------------+\n|      1|            39|\n|      2|            65|\n|      3|            51|\n+-------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Assume today's date is '2021-1-1'.\n",
    "\n",
    "# Write a solution that will, for each user_id, find out the largest window of days between each visit and the one right after it (or today if you are considering the last visit).\n",
    "\n",
    "# Return the result table ordered by user_id.\n",
    "\n",
    "windowOptions = Window.partitionBy(col('user_id')).orderBy(col('visit_date'))\n",
    "user_visits_df.withColumn('visit_date2', lead(col('visit_date'),1,'2021-01-01').over(windowOptions))\\\n",
    "    .withColumn('frequency',abs(datediff(col('visit_date'),col('visit_date2')))).groupBy(col('user_id'))\\\n",
    "        .agg(max(col('frequency')).alias('biggest_window'))\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8f9928d-efb0-48c4-a4e5-1de4cbcf4ff7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1270. All People Report to the Given Manager\n",
    "```\n",
    "Table: Employees\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| employee_id   | int     |\n",
    "| employee_name | varchar |\n",
    "| manager_id    | int     |\n",
    "+---------------+---------+\n",
    "employee_id is the column of unique values for this table.\n",
    "Each row of this table indicates that the employee with ID employee_id and name employee_name reports his work to his/her direct manager with manager_id\n",
    "The head of the company is the employee with employee_id = 1.\n",
    " \n",
    "\n",
    "Write a solution to find employee_id of all employees that directly or indirectly report their work to the head of the company.\n",
    "\n",
    "The indirect relation between managers will not exceed three managers as the company is small.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Employees table:\n",
    "+-------------+---------------+------------+\n",
    "| employee_id | employee_name | manager_id |\n",
    "+-------------+---------------+------------+\n",
    "| 1           | Boss          | 1          |\n",
    "| 3           | Alice         | 3          |\n",
    "| 2           | Bob           | 1          |\n",
    "| 4           | Daniel        | 2          |\n",
    "| 7           | Luis          | 4          |\n",
    "| 8           | Jhon          | 3          |\n",
    "| 9           | Angela        | 8          |\n",
    "| 77          | Robert        | 1          |\n",
    "+-------------+---------------+------------+\n",
    "Output: \n",
    "+-------------+\n",
    "| employee_id |\n",
    "+-------------+\n",
    "| 2           |\n",
    "| 77          |\n",
    "| 4           |\n",
    "| 7           |\n",
    "+-------------+\n",
    "Explanation: \n",
    "The head of the company is the employee with employee_id 1.\n",
    "The employees with employee_id 2 and 77 report their work directly to the head of the company.\n",
    "The employee with employee_id 4 reports their work indirectly to the head of the company 4 --> 2 --> 1. \n",
    "The employee with employee_id 7 reports their work indirectly to the head of the company 7 --> 4 --> 2 --> 1.\n",
    "The employees with employee_id 3, 8, and 9 do not report their work to the head of the company directly or indirectly. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f83737be-fc0a-436e-b910-8fcff3d0d4e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----------+\n|employee_id|employee_name|manager_id|\n+-----------+-------------+----------+\n|          1|         Boss|         1|\n|          3|        Alice|         3|\n|          2|          Bob|         1|\n|          4|       Daniel|         2|\n|          7|         Luis|         4|\n|          8|         John|         3|\n|          9|       Angela|         8|\n|         77|       Robert|         1|\n+-----------+-------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"employee_name\", StringType(), True),\n",
    "    StructField(\"manager_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create the DataFrame\n",
    "data = [\n",
    "    (1, 'Boss', 1), \n",
    "    (3, 'Alice', 3), \n",
    "    (2, 'Bob', 1), \n",
    "    (4, 'Daniel', 2), \n",
    "    (7, 'Luis', 4), \n",
    "    (8, 'John', 3), \n",
    "    (9, 'Angela', 8), \n",
    "    (77, 'Robert', 1)\n",
    "]\n",
    "\n",
    "employees_df = spark.createDataFrame(data, schema)\n",
    "employees_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f712585-0d4d-4bb4-8844-da33d3d18c18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|employee_id|\n+-----------+\n|          4|\n|          7|\n|         77|\n|          2|\n+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Write a solution to find employee_id of all employees that directly or indirectly report their work to the head of the company.\n",
    "\n",
    "# The indirect relation between managers will not exceed three managers as the company is small.\n",
    "\n",
    "# Return the result table in any order.\n",
    "\n",
    "\n",
    "df = employees_df.alias('T1').join(employees_df.alias('T2'), col('T1.employee_id') == col('T2.employee_id'),'inner')\\\n",
    "    .filter((col('T1.manager_id') == 1) & (col('T1.employee_id') != 1)).select(col('T1.employee_id'))\n",
    "\n",
    "df2 = df.alias('T3').join(employees_df.alias('T4'), col('T3.employee_id') == col('T4.manager_id'),'inner' )\\\n",
    "    .select(col('T4.employee_id')).union(df)\n",
    "\n",
    "df2.alias('T5').join(employees_df.alias('T6'), col('T5.employee_id') == col('T6.manager_id'),'inner').select(col('T6.employee_id'))\\\n",
    "    .union(df2).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01696237-cc79-44af-8ad0-1b9d27d8e5b8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1412. Find the Quiet Students in All Exams\n",
    "```\n",
    "Table: Student\n",
    "\n",
    "+---------------------+---------+\n",
    "| Column Name         | Type    |\n",
    "+---------------------+---------+\n",
    "| student_id          | int     |\n",
    "| student_name        | varchar |\n",
    "+---------------------+---------+\n",
    "student_id is the primary key (column with unique values) for this table.\n",
    "student_name is the name of the student.\n",
    " \n",
    "\n",
    "Table: Exam\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| exam_id       | int     |\n",
    "| student_id    | int     |\n",
    "| score         | int     |\n",
    "+---------------+---------+\n",
    "(exam_id, student_id) is the primary key (combination of columns with unique values) for this table.\n",
    "Each row of this table indicates that the student with student_id had a score points in the exam with id exam_id.\n",
    " \n",
    "\n",
    "A quiet student is the one who took at least one exam and did not score the highest or the lowest score.\n",
    "\n",
    "Write a solution to report the students (student_id, student_name) being quiet in all exams. Do not return the student who has never taken any exam.\n",
    "\n",
    "Return the result table ordered by student_id.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Student table:\n",
    "+-------------+---------------+\n",
    "| student_id  | student_name  |\n",
    "+-------------+---------------+\n",
    "| 1           | Daniel        |\n",
    "| 2           | Jade          |\n",
    "| 3           | Stella        |\n",
    "| 4           | Jonathan      |\n",
    "| 5           | Will          |\n",
    "+-------------+---------------+\n",
    "Exam table:\n",
    "+------------+--------------+-----------+\n",
    "| exam_id    | student_id   | score     |\n",
    "+------------+--------------+-----------+\n",
    "| 10         |     1        |    70     |\n",
    "| 10         |     2        |    80     |\n",
    "| 10         |     3        |    90     |\n",
    "| 20         |     1        |    80     |\n",
    "| 30         |     1        |    70     |\n",
    "| 30         |     3        |    80     |\n",
    "| 30         |     4        |    90     |\n",
    "| 40         |     1        |    60     |\n",
    "| 40         |     2        |    70     |\n",
    "| 40         |     4        |    80     |\n",
    "+------------+--------------+-----------+\n",
    "Output: \n",
    "+-------------+---------------+\n",
    "| student_id  | student_name  |\n",
    "+-------------+---------------+\n",
    "| 2           | Jade          |\n",
    "+-------------+---------------+\n",
    "Explanation: \n",
    "For exam 1: Student 1 and 3 hold the lowest and high scores respectively.\n",
    "For exam 2: Student 1 hold both highest and lowest score.\n",
    "For exam 3 and 4: Studnet 1 and 4 hold the lowest and high scores respectively.\n",
    "Student 2 and 5 have never got the highest or lowest in any of the exams.\n",
    "Since student 5 is not taking any exam, he is excluded from the result.\n",
    "So, we only return the information of Student 2.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3364826-fa68-47f5-8e00-c964635bafd4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n|student_id|student_name|\n+----------+------------+\n|         1|      Daniel|\n|         2|        Jade|\n|         3|      Stella|\n|         4|    Jonathan|\n|         5|        Will|\n+----------+------------+\n\n+-------+----------+-----+\n|exam_id|student_id|score|\n+-------+----------+-----+\n|     10|         1|   70|\n|     10|         2|   80|\n|     10|         3|   90|\n|     20|         1|   80|\n|     30|         1|   70|\n|     30|         3|   80|\n|     30|         4|   90|\n|     40|         1|   60|\n|     40|         2|   70|\n|     40|         4|   80|\n+-------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Create student DataFrame\n",
    "student_data = [(1, 'Daniel'), (2, 'Jade'), (3, 'Stella'), (4, 'Jonathan'), (5, 'Will')]\n",
    "student_schema = [\"student_id\", \"student_name\"]\n",
    "student_df = spark.createDataFrame(student_data, schema=student_schema)\n",
    "\n",
    "# Create exam DataFrame\n",
    "exam_data = [(10, 1, 70), (10, 2, 80), (10, 3, 90), (20, 1, 80), (30, 1, 70), (30, 3, 80), (30, 4, 90), (40, 1, 60), (40, 2, 70), (40, 4, 80)]\n",
    "exam_schema = [\"exam_id\", \"student_id\", \"score\"]\n",
    "exam_df = spark.createDataFrame(exam_data, schema=exam_schema)\n",
    "\n",
    "# Show the DataFrames\n",
    "student_df.show()\n",
    "exam_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a127893-b669-4cbf-9b8a-c87fa14e4805",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n|student_id|student_name|\n+----------+------------+\n|         2|        Jade|\n+----------+------------+\n\n"
     ]
    }
   ],
   "source": [
    "# A quiet student is the one who took at least one exam and did not score the highest or the lowest score.\n",
    "\n",
    "# Write a solution to report the students (student_id, student_name) being quiet in all exams. Do not return the student who has never taken any exam.\n",
    "\n",
    "# Return the result table ordered by student_id.\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "windowOptions1 = Window.partitionBy(col('exam_id')).orderBy(col('score').asc())\n",
    "windowOptions2 = Window.partitionBy(col('exam_id')).orderBy(col('score').desc())\n",
    "\n",
    "df2 = exam_df.alias('E').join(student_df.alias('S'), col('E.student_id') == col('S.student_id'),'left')\\\n",
    "    .withColumn('R1',rank().over(windowOptions1)).withColumn('R2',rank().over(windowOptions2)).drop(col('s.student_id'))\n",
    "\n",
    "df3 = df2.filter( (col('R1') ==1) | (col('R2') == 1)).select(col('student_id'))\n",
    "\n",
    "filtered_student_ids = df3.select('student_id').rdd.flatMap(lambda x: x).collect()\n",
    "df2.filter(~col('student_id').isin(filtered_student_ids)).select(col('student_id'),col('student_name')).distinct().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68174310-2c08-44fb-a43e-445974aa3a20",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1767. Find the Subtasks That Did Not Execute\n",
    "```\n",
    "Table: Tasks\n",
    "\n",
    "+----------------+---------+\n",
    "| Column Name    | Type    |\n",
    "+----------------+---------+\n",
    "| task_id        | int     |\n",
    "| subtasks_count | int     |\n",
    "+----------------+---------+\n",
    "task_id is the column with unique values for this table.\n",
    "Each row in this table indicates that task_id was divided into subtasks_count subtasks labeled from 1 to subtasks_count.\n",
    "It is guaranteed that 2 <= subtasks_count <= 20.\n",
    " \n",
    "\n",
    "Table: Executed\n",
    "\n",
    "+---------------+---------+\n",
    "| Column Name   | Type    |\n",
    "+---------------+---------+\n",
    "| task_id       | int     |\n",
    "| subtask_id    | int     |\n",
    "+---------------+---------+\n",
    "(task_id, subtask_id) is the combination of columns with unique values for this table.\n",
    "Each row in this table indicates that for the task task_id, the subtask with ID subtask_id was executed successfully.\n",
    "It is guaranteed that subtask_id <= subtasks_count for each task_id.\n",
    " \n",
    "\n",
    "Write a solution to report the IDs of the missing subtasks for each task_id.\n",
    "\n",
    "Return the result table in any order.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Tasks table:\n",
    "+---------+----------------+\n",
    "| task_id | subtasks_count |\n",
    "+---------+----------------+\n",
    "| 1       | 3              |\n",
    "| 2       | 2              |\n",
    "| 3       | 4              |\n",
    "+---------+----------------+\n",
    "Executed table:\n",
    "+---------+------------+\n",
    "| task_id | subtask_id |\n",
    "+---------+------------+\n",
    "| 1       | 2          |\n",
    "| 3       | 1          |\n",
    "| 3       | 2          |\n",
    "| 3       | 3          |\n",
    "| 3       | 4          |\n",
    "+---------+------------+\n",
    "Output: \n",
    "+---------+------------+\n",
    "| task_id | subtask_id |\n",
    "+---------+------------+\n",
    "| 1       | 1          |\n",
    "| 1       | 3          |\n",
    "| 2       | 1          |\n",
    "| 2       | 2          |\n",
    "+---------+------------+\n",
    "Explanation: \n",
    "Task 1 was divided into 3 subtasks (1, 2, 3). Only subtask 2 was executed successfully, so we include (1, 1) and (1, 3) in the answer.\n",
    "Task 2 was divided into 2 subtasks (1, 2). No subtask was executed successfully, so we include (2, 1) and (2, 2) in the answer.\n",
    "Task 3 was divided into 4 subtasks (1, 2, 3, 4). All of the subtasks were executed successfully.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "778c5548-527d-404a-a38e-037622706e35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n|task_id|subtasks_count|\n+-------+--------------+\n|      1|             3|\n|      2|             2|\n|      3|             4|\n+-------+--------------+\n\n+-------+----------+\n|task_id|subtask_id|\n+-------+----------+\n|      1|         2|\n|      3|         1|\n|      3|         2|\n|      3|         3|\n|      3|         4|\n+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, lit\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"tasks_example\").getOrCreate()\n",
    "\n",
    "# Create tasks DataFrame\n",
    "tasks_data = [(1, 3), (2, 2), (3, 4)]\n",
    "tasks_schema = [\"task_id\", \"subtasks_count\"]\n",
    "tasks_df = spark.createDataFrame(tasks_data, schema=tasks_schema)\n",
    "\n",
    "# Create executed DataFrame\n",
    "executed_data = [(1, 2), (3, 1), (3, 2), (3, 3), (3, 4)]\n",
    "executed_schema = [\"task_id\", \"subtask_id\"]\n",
    "executed_df = spark.createDataFrame(executed_data, schema=executed_schema)\n",
    "\n",
    "tasks_df.show()\n",
    "executed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85fdee66-0fed-492f-8841-5711789522b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n|task_id|subtask_id|\n+-------+----------+\n|      1|         1|\n|      1|         3|\n|      2|         1|\n|      2|         2|\n+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "tasks_with_subtasks_df = tasks_df.withColumn(\"subtask_id\", explode(sequence(lit(1), col(\"subtasks_count\"))))\n",
    "\n",
    "tasks_with_subtasks_df.alias('S').join(executed_df.alias('E'), (col('S.task_id') == col('E.task_id')) & \\\n",
    "     (col('S.subtask_id') == col('E.subtask_id')),'left').filter(col('E.subtask_id').isNull()) \\\n",
    "    .select(col('S.task_id'), col('S.subtask_id')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "602b3597-1963-4a8e-835d-cb75849037e6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## (Premium) 1225. Report Contiguous Dates\n",
    "```\n",
    "Table: Failed\n",
    "\n",
    "+--------------+---------+\n",
    "| Column Name  | Type    |\n",
    "+--------------+---------+\n",
    "| fail_date    | date    |\n",
    "+--------------+---------+\n",
    "fail_date is the primary key (column with unique values) for this table.\n",
    "This table contains the days of failed tasks.\n",
    " \n",
    "\n",
    "Table: Succeeded\n",
    "\n",
    "+--------------+---------+\n",
    "| Column Name  | Type    |\n",
    "+--------------+---------+\n",
    "| success_date | date    |\n",
    "+--------------+---------+\n",
    "success_date is the primary key (column with unique values) for this table.\n",
    "This table contains the days of succeeded tasks.\n",
    " \n",
    "\n",
    "A system is running one task every day. Every task is independent of the previous tasks. The tasks can fail or succeed.\n",
    "\n",
    "Write a solution to report the period_state for each continuous interval of days in the period from 2019-01-01 to 2019-12-31.\n",
    "\n",
    "period_state is 'failed' if tasks in this interval failed or 'succeeded' if tasks in this interval succeeded. Interval of days are retrieved as start_date and end_date.\n",
    "\n",
    "Return the result table ordered by start_date.\n",
    "\n",
    "The result format is in the following example.\n",
    "\n",
    " \n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: \n",
    "Failed table:\n",
    "+-------------------+\n",
    "| fail_date         |\n",
    "+-------------------+\n",
    "| 2018-12-28        |\n",
    "| 2018-12-29        |\n",
    "| 2019-01-04        |\n",
    "| 2019-01-05        |\n",
    "+-------------------+\n",
    "Succeeded table:\n",
    "+-------------------+\n",
    "| success_date      |\n",
    "+-------------------+\n",
    "| 2018-12-30        |\n",
    "| 2018-12-31        |\n",
    "| 2019-01-01        |\n",
    "| 2019-01-02        |\n",
    "| 2019-01-03        |\n",
    "| 2019-01-06        |\n",
    "+-------------------+\n",
    "Output: \n",
    "+--------------+--------------+--------------+\n",
    "| period_state | start_date   | end_date     |\n",
    "+--------------+--------------+--------------+\n",
    "| succeeded    | 2019-01-01   | 2019-01-03   |\n",
    "| failed       | 2019-01-04   | 2019-01-05   |\n",
    "| succeeded    | 2019-01-06   | 2019-01-06   |\n",
    "+--------------+--------------+--------------+\n",
    "Explanation: \n",
    "The report ignored the system state in 2018 as we care about the system in the period 2019-01-01 to 2019-12-31.\n",
    "From 2019-01-01 to 2019-01-03 all tasks succeeded and the system state was \"succeeded\".\n",
    "From 2019-01-04 to 2019-01-05 all tasks failed and the system state was \"failed\".\n",
    "From 2019-01-06 to 2019-01-06 all tasks succeeded and the system state was \"succeeded\".\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e8a3a36-f7ec-4fe9-a7b0-3d8acbb27868",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n| fail_date|\n+----------+\n|2018-12-28|\n|2018-12-29|\n|2019-01-04|\n|2019-01-05|\n+----------+\n\n+------------+\n|success_date|\n+------------+\n|  2018-12-30|\n|  2018-12-31|\n|  2019-01-01|\n|  2019-01-02|\n|  2019-01-03|\n|  2019-01-06|\n+------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DateType, StringType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"FailuresAndSuccesses\").getOrCreate()\n",
    "\n",
    "# Define schemas\n",
    "failed_schema = StructType([StructField(\"fail_date\", StringType(), True)])\n",
    "succeeded_schema = StructType([StructField(\"success_date\", StringType(), True)])\n",
    "\n",
    "# Create data\n",
    "failed_data = [['2018-12-28'], ['2018-12-29'], ['2019-01-04'], ['2019-01-05']]\n",
    "succeeded_data = [['2018-12-30'], ['2018-12-31'], ['2019-01-01'], ['2019-01-02'], ['2019-01-03'], ['2019-01-06']]\n",
    "\n",
    "# Create PySpark DataFrames\n",
    "failed_spark_df = spark.createDataFrame(failed_data, schema=failed_schema)\n",
    "succeeded_spark_df = spark.createDataFrame(succeeded_data, schema=succeeded_schema)\n",
    "\n",
    "# Show the PySpark DataFrames\n",
    "failed_spark_df.show()\n",
    "succeeded_spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54caf95e-9c93-4240-a1b3-d2115460bd62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+\n|period_state|start_date|  end_date|\n+------------+----------+----------+\n|   Succeeded|2019-01-01|2019-01-03|\n|      failed|2019-01-04|2019-01-05|\n|   Succeeded|2019-01-06|2019-01-06|\n+------------+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# A system is running one task every day. Every task is independent of the previous tasks. The tasks can fail or succeed.\n",
    "\n",
    "# Write a solution to report the period_state for each continuous interval of days in the period from 2019-01-01 to 2019-12-31.\n",
    "\n",
    "# period_state is 'failed' if tasks in this interval failed or 'succeeded' if tasks in this interval succeeded. Interval of days are retrieved as start_date and end_date.\n",
    "\n",
    "# Return the result table ordered by start_date.\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "df1 = succeeded_spark_df.withColumn('success_date', col('success_date').cast('date'))\\\n",
    "    .select(col('success_date'),lit('Succeeded').alias('Succeeded'))\n",
    "df2 = failed_spark_df.withColumn('fail_date', col('fail_date').cast('date'))\\\n",
    "    .select(col('fail_date'),lit('failed').alias('Failed'))\n",
    "\n",
    "df2 = df1.union(df2).filter(col('success_date').between('2019-01-01','2019-12-31')).orderBy(col('success_date'))\n",
    "\n",
    "windowOptions = Window.partitionBy(col('Succeeded')).orderBy(col('success_date'))\n",
    "df2.withColumn('RN',row_number().over(windowOptions)) \\\n",
    "    .withColumn('dateGroup', date_sub(col('success_date'),col('RN'))).groupBy(col('dateGroup'))\\\n",
    "        .agg(max(col('Succeeded')).alias('period_state'), min(col('success_date')).alias('start_date'), \\\n",
    "            max(col('success_date')).alias('end_date')).drop(col('dateGroup')).orderBy(col('start_date')).show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "50 Advanced Pyspark Dataframe Problems",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
